<!DOCTYPE html>
<html lang="en">
<head>
    <title>Mastering the game of Go with deep neural networks and tree search | Nature</title>
    
        
<link rel="preload" href=/static/fonts/Lora-Regular.8861b0072d.woff2 as="font" type="font/woff2" crossorigin>


<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes"/>


<script>
    (function(e){var t=e.documentElement,n=e.implementation;t.className='js';if(n&&n.hasFeature('http://www.w3.org/TR/SVG11/feature#Image','1.1')){t.className+=' svg'}})(document)
</script>
<script>window.abTestSharedArticleRenderer = false;</script>




    <link rel="apple-touch-icon" sizes="114x114" href="/static/images/favicons/nature/favicon-114x114.1431047068.png"/>
    <link rel="apple-touch-icon" sizes="144x144" href="/static/images/favicons/nature/favicon-144x144.3e61d1f755.png"/>
    <link rel="apple-touch-icon" sizes="180x180" href="/static/images/favicons/nature/favicon-180x180.5789768d66.png"/>
    <link rel="icon" type="image/png" href="/static/images/favicons/nature/favicon-16x16.bfd0e25c74.png" sizes="16x16"/>
    <link rel="icon" type="image/png" href="/static/images/favicons/nature/favicon-32x32.1c4a03347a.png" sizes="32x32"/>
    <link rel="icon" type="image/png" href="/static/images/favicons/nature/favicon-96x96.14ff9c7f48.png" sizes="96x96"/>
    <link rel="icon" type="image/png" href="/static/images/favicons/nature/favicon-194x194.5789768d66.png" sizes="194x194"/>
    <link rel="shortcut icon" href="/static/images/favicons/nature/favicon.14515a9f85.ico"/>
    <meta name="msapplication-TileColor" content="#940720"/>
    <meta name="msapplication-TileImage" content="/static/images/favicons/nature/favicon-144x144.3e61d1f755.png">
    <meta name="theme-color" content="#940720">


<link href="/static/css/mosaic-grade-c.c4d038d112.css" rel="stylesheet"/>




<link id="mustard" data-test="journal-mosaic-css" href="/static/css/journal-mosaic.4fb975b1e8.css" rel="stylesheet"  media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)" />






    
    <style>
         .background-brand-primary {
              background-color: #920B24;
              color: #fff;
         }

         .background-brand-secondary {
              background-color: #a12a3f;
              color: #fff;
         }

         .background-brand-secondary-pdf {
              background-color: #036599;
              background-size: 22px auto;
              background-position: 80% 50%;
              color: #fff;
         }

         .background-brand-secondary-pdf a {
              color: #fff;
         }

         .background-brand-tertiary {
              background-color: #d7c8c8;
              color: #222;
         }

         .background-gradient-brand-primary {
              background-image: linear-gradient(to right, #920B24, #920B24 50%, transparent 50%);
         }

         .background-gradient-brand-secondary-tertiary {
              background-image: linear-gradient(to right, #a12a3f, #a12a3f 50%, #d7c8c8 50%);
         }

         .background-gradient-brand-gray {
              background-image: linear-gradient(to right, #e0e0e0, #e0e0e0 50%, transparent 50%);
         }

         .background-gradient-brand-gray-light {
              background-image: linear-gradient(to right, #eee, #eee 50%, transparent 50%);
         }

         
              .background-brand-primary a {
                  color: #fff;
              }
         

         
              .background-brand-secondary a {
                  color: #fff;
              }
         

         

         .nav-border {
              border-bottom: 4px solid #a12a3f;
         }

         /* less than or equal to 875px */
         @media only screen and (max-width: 54.688em) {
              .banner {
                  background-color: #920B24;
              }

              .nav-border .menu-cell li {
                  border-bottom: 1px solid #a12a3f;
              }
         }

         .sticky-header .inner-banner,
         .header-primary-color {
              background-color: #920B24;
         }

         
             .menu-button,
             .menu-button-clone,
             .header-submit-button {
                 border: 1px solid #fff;
                 border: 1px solid rgba(255, 255, 255, 0.75);
                 color: #fff;
             }
             .header-top-bar {
                 border-bottom: 1px solid #fff;
                 border-bottom: 1px solid rgba(255, 255, 255, 0.75);
             }

             .icon-rotate.tools-menu-button-icon:after,
             .menu-button-icon:after {
                  background-image: url("/static/images/icons/icon-arrow-down-12x7-white.e10aedb54f.svg"), none;
             }

             .nature-research-logo {
                  background-image: url("/static/images/product-logos/nature-research-logo-white.a9028dbd21.svg"), none;
             }
             .header-search-button {
                  padding-right: 25px;
                  background-image: url("/static/images/icons/icon-search-24x24-white.7b3ba09d04.svg"), none;
             }
         
     </style>
     


<!-- SpringerLink's event tracker, as per: https://github.com/springernature/springerlink-event-tracker -->

<script type="text/javascript">
document.addEventListener('accessdetailsloaded', function(e) {
    var dL, doi, pageType, eventObject, i, script, loginEventData;

    for (i = 0; i < window.dataLayer.length; i++) {
        if (window.dataLayer[i]["content"] && window.dataLayer[i]["page"]) {
            dL = window.dataLayer[i];
        }
    }

    if (!dL
        || !dL.content.article
        || !(doi = dL.content.article.doi) && doi.length > 0
        || (pageType = dL.page.category.pageType.toLowerCase()) !== "article"
    ) {return false;}

    pageType = pageType.charAt(0).toUpperCase() + pageType.substring(1);

    eventObject = {'content_type': pageType, 'doi': doi};

    loginEventData = e.detail;
    if (loginEventData && loginEventData.business_partner_id) {
        eventObject['business_partner_ids'] = loginEventData.business_partner_id;
    }

    script = document.createElement('script');
    script.id = 'springerlink-event-tracker';
    script.src = 'https://event-tracker.springernature.com/dist/eventTracker.js';
    script.onload = function() {
        new EventTracker({'platform': 'Nature'}).sendEvent('display', eventObject);
    };
    document.head.appendChild(script);
});
</script>
<script>
    dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"computational-science;computer-science;reward","webtrendsContentCategory":null,"webtrendsContentCollection":"The multidisciplinary nature of machine intelligence","webtrendsContentGroup":"Nature","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/nature16961"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["David Silver","Aja Huang","Chris J. Maddison","Arthur Guez","Laurent Sifre","George van den Driessche","Julian Schrittwieser","Ioannis Antonoglou","Veda Panneershelvam","Marc Lanctot","Sander Dieleman","Dominik Grewe","John Nham","Nal Kalchbrenner","Ilya Sutskever","Timothy Lillicrap","Madeleine Leach","Koray Kavukcuoglu","Thore Graepel","Demis Hassabis"],"publishedAt":1453852800,"publishedAtString":"2016-01-27","title":"Mastering the game of Go with deep neural networks and tree search","legacy":null,"publishedAtTime":null,"documentType":"npg"},"journal":{"pcode":"nature","title":"nature","volume":"529","issue":"7587"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":{"id":"csgqqsrfxh"}},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"ab_test_nature_rebrand_150","active":false},{"name":"ab_test_subscribe_button","active":true},{"name":"ab_test_briefing_new_signup_box","active":true},{"name":"ab_test_best_available_version","active":false},{"name":"ab_test_magazine_article_native_ads","active":false}]},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null}];
</script>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NWDMT9Q');</script>
<!-- End Google Tag Manager -->


    <meta name="description" content="A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence."/>

<meta name="robots" content="noarchive">
<meta name="access" content="Yes">
<meta name="WT.cg_s" content="Article"/>
<meta name="WT.z_bandiera_abtest" content="a"/>
<meta name="WT.page_categorisation" content="Article_HTML"/>

    <meta name="WT.template" content="oscar"/>
    <meta name="WT.z_cg_type" content="Nature Research Journals"/>
    <meta name="WT.cg_n" content="Nature"/>
    <meta name="dc.rights" content="©2019 Macmillan Publishers Limited. All Rights Reserved."/>
    <meta name="prism.issn" content="1476-4687"/>


<link rel="search" href="http://www.nature.com/search">
<link rel="search" href="http://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="http://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">


    
    

    <meta name="journal_id" content="nature"/>

    <meta name="dc.title" content="Mastering the game of Go with deep neural networks and tree search"/>

    <meta name="dc.source" content="Nature 2016 529:7587"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Nature Publishing Group"/>

    <meta name="dc.date" content="2016-01-27"/>

    <meta name="dc.type" content="Research"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Nature Publishing Group"/>

    <meta name="dc.rightsAgent" content="permissions@nature.com"/>

    <meta name="dc.description" content="A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence."/>

    <meta name="prism.issn" content="1476-4687"/>

    <meta name="prism.publicationName" content="Nature"/>

    <meta name="prism.publicationDate" content="2016-01-27"/>

    <meta name="prism.volume" content="529"/>

    <meta name="prism.number" content="7587"/>

    <meta name="prism.section" content="Research"/>

    <meta name="prism.startingPage" content="484"/>

    <meta name="prism.endingPage" content="489"/>

    <meta name="prism.copyright" content="2016 Nature Publishing Group"/>

    <meta name="prism.rightsAgent" content="permissions@nature.com"/>

    <meta name="prism.url" content="https://www.nature.com/articles/nature16961"/>

    <meta name="prism.doi" content="doi:10.1038/nature16961"/>

    <meta name="citation_pdf_url" content="https://www.nature.com/articles/nature16961.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/nature16961"/>

    <meta name="citation_journal_title" content="Nature"/>

    <meta name="citation_journal_abbrev" content="nature"/>

    <meta name="citation_publisher" content="Nature Publishing Group"/>

    <meta name="citation_issn" content="1476-4687"/>

    <meta name="citation_title" content="Mastering the game of Go with deep neural networks and tree search"/>

    <meta name="citation_volume" content="529"/>

    <meta name="citation_issue" content="7587"/>

    <meta name="citation_publication_date" content="2016/01"/>

    <meta name="citation_online_date" content="2016/01/27"/>

    <meta name="citation_firstpage" content="484"/>

    <meta name="citation_lastpage" content="489"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1038/nature16961"/>

    <meta name="DOI" content="10.1038/nature16961"/>

    <meta name="citation_doi" content="10.1038/nature16961"/>

    <meta name="description" content="A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence."/>

    <meta name="dc.creator" content="David Silver"/>

    <meta name="dc.creator" content="Aja Huang"/>

    <meta name="dc.creator" content="Chris J. Maddison"/>

    <meta name="dc.creator" content="Arthur Guez"/>

    <meta name="dc.creator" content="Laurent Sifre"/>

    <meta name="dc.creator" content="George van den Driessche"/>

    <meta name="dc.creator" content="Julian Schrittwieser"/>

    <meta name="dc.creator" content="Ioannis Antonoglou"/>

    <meta name="dc.creator" content="Veda Panneershelvam"/>

    <meta name="dc.creator" content="Marc Lanctot"/>

    <meta name="dc.creator" content="Sander Dieleman"/>

    <meta name="dc.creator" content="Dominik Grewe"/>

    <meta name="dc.creator" content="John Nham"/>

    <meta name="dc.creator" content="Nal Kalchbrenner"/>

    <meta name="dc.creator" content="Ilya Sutskever"/>

    <meta name="dc.creator" content="Timothy Lillicrap"/>

    <meta name="dc.creator" content="Madeleine Leach"/>

    <meta name="dc.creator" content="Koray Kavukcuoglu"/>

    <meta name="dc.creator" content="Thore Graepel"/>

    <meta name="dc.creator" content="Demis Hassabis"/>

    <meta name="dc.subject" content="Computational science"/>

    <meta name="dc.subject" content="Computer science"/>

    <meta name="dc.subject" content="Reward"/>

    <meta name="citation_author" content="David Silver"/>

    <meta name="citation_author" content="Aja Huang"/>

    <meta name="citation_author" content="Chris J. Maddison"/>

    <meta name="citation_author" content="Arthur Guez"/>

    <meta name="citation_author" content="Laurent Sifre"/>

    <meta name="citation_author" content="George van den Driessche"/>

    <meta name="citation_author" content="Julian Schrittwieser"/>

    <meta name="citation_author" content="Ioannis Antonoglou"/>

    <meta name="citation_author" content="Veda Panneershelvam"/>

    <meta name="citation_author" content="Marc Lanctot"/>

    <meta name="citation_author" content="Sander Dieleman"/>

    <meta name="citation_author" content="Dominik Grewe"/>

    <meta name="citation_author" content="John Nham"/>

    <meta name="citation_author" content="Nal Kalchbrenner"/>

    <meta name="citation_author" content="Ilya Sutskever"/>

    <meta name="citation_author" content="Timothy Lillicrap"/>

    <meta name="citation_author" content="Madeleine Leach"/>

    <meta name="citation_author" content="Koray Kavukcuoglu"/>

    <meta name="citation_author" content="Thore Graepel"/>

    <meta name="citation_author" content="Demis Hassabis"/>

    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Mastering the game of Go with deep neural networks and tree search"/>

    <meta name="twitter:site" content="@naturenews"/>

    <meta name="twitter:description" content="A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence."/>

    <meta name="twitter:image" content="https://media.springernature.com/full/nature-static/assets/v1/image-assets/nature16961-f1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="WT.z_primary_atype" content="Research"/>

    <meta name="WT.z_subject_term" content="Computational science;Computer science;Reward"/>

    <meta name="WT.z_subject_term_id" content="computational-science;computer-science;reward"/>


    
        <meta property="og:url" content="https://www.nature.com/articles/nature16961"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Nature"/>
        <meta property="og:title" content="Mastering the game of Go with deep neural networks and tree search"/>
        <meta property="og:description" content="A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence."/>
        <meta property="og:image" content="https://media.springernature.com/m685/nature-static/assets/v1/image-assets/nature16961-f1.jpg"/>
    
</head>
<!--[if IE 9]><body class="ie9 article-page"><![endif]-->
<!--[if gt IE 9]><!--><body class="article-page"><!--<![endif]-->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NWDMT9Q"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<div role="banner" class="position-relative cleared z-index-50" data-test="top-containers">
    <div class="hide-print container background-gray-dark">
    <div class="content mq1200-padded">
        <a href="#content" id="skiplink" class="skiplink contrast-text js-no-scroll" data-track="click"
           data-track-action="skip to content" data-track-category="header" data-track-label="link">Skip to main content</a>
    </div>
</div>



<div class="container cleared container-type-banner-advert inline-group" data-container-type="banner-advert">
    <div class="hide-print pt10 pb10 mq875-pa0 position-relative z-index-100 clear-float cleared leaderboard-or-billboard-container">
        <div class="leaderboard-or-billboard-inner ad-with-label">
            
                
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-gpt-unitpath="/285/nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=nature16961;doi=10.1038/nature16961;subjmeta=1042,117,1788,378,631,639,705;kwrd=Computational science,Computer science,Reward">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/nature.com/article&amp;sz=728x90&amp;c=-501702319&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnature16961%26doi%3D10.1038/nature16961%26subjmeta%3D1042,117,1788,378,631,639,705%26kwrd%3DComputational science,Computer science,Reward">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/nature.com/article&amp;sz=728x90&amp;c=-501702319&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnature16961%26doi%3D10.1038/nature16961%26subjmeta%3D1042,117,1788,378,631,639,705%26kwrd%3DComputational science,Computer science,Reward"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

            
        </div>
    </div>
</div>

<div class="hide-print text-orange content grade-c-show"><p>Thank you for visiting nature.com. You are using a browser version with
    limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off
    compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site
    without styles and JavaScript.</p></div>

    
    
    <div class="hide-print container background-bluegray contrast-text text14 strong lower mq640-hide position-relative z-index-100"
         data-test="header-breadcrumbs">
        <div class="content cleared mq1200-padded">
            <div class="breadcrumbs pin-left">
                    <ol class="ma0 cleared clean-list inline-list"><li id="breadcrumb0" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"
                                     itemref="breadcrumb1"><a href="/" itemprop="url" class="icon icon-right icon-arrow-right-6x10-white pr15"
                                                                                                                 data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="title">nature</span></a></li><li id="breadcrumb1" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"
                                     itemref="breadcrumb2" class="ml6"><a href="/nature/articles?type&#x3D;article" itemprop="url" class="icon icon-right icon-arrow-right-6x10-white pr15"
                                                                                                                 data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="title">articles</span></a></li><li id="breadcrumb2" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"><a href="/articles/nature16961" itemprop="url" data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:article" class="ml6"><span itemprop="title">article</span></a></li></ol>
            </div>
            
                <p class="pin-right box-sizing header-primary-color nature-research-logo ma0 pa6 hide-text grade-c-hide">A Nature Research Journal</p>
            
        </div>
    </div>


    
    
        <div class="hide-print container js-header-container mb20 position-relative z-index-50" data-ui="header-container"
             data-container-type="header">
            <div class="cleared clear hide-print position-relative background-gradient-brand-primary js-header default-header z-index-100 composite-layer tighten-line-height">
                <div class="banner js-banner content mq1200-padded position-relative">
                    <div class="inner-banner cleared">
                        <div class="main-column small-header-main pin-left">
                            <div class="header-logo-container header-primary-color">
                                <a href="#menu" id="menu-button" class="js-header-menu-button menu-button js-no-scroll"
                                   data-test="menu-button">
                                            <span class="menu-button-icon icon-rotate">
                                                <span class="menu-button-label">Menu</span>
                                            </span>
                                </a>
                                    <h1 class="inline-block"><a href="/nature" class="header-logo inline-block"
                                        data-track="click" data-track-action="home"
                                        data-track-category="header" data-track-label="image"><img alt="Nature"
                                                               src="//media.springernature.com/full/nature-cms/uploads/product/nature/header-9a912abb419d552ac49f29390d1ad819.svg"
                                                               class="grade-c-invisible header-logo-primary js-svg"
                                                               data-png="//media.springernature.com/full/nature-cms/uploads/product/nature/header-7ba808809b424669810535098d74fd18.png"><img
                                            alt="Nature" src="//media.springernature.com/full/nature-cms/uploads/product/nature/header-845e39177b477cf5e943d89f42d3442c.svg"
                                            class="grade-c-invisible header-logo-secondary js-svg"
                                            data-png="//media.springernature.com/full/nature-cms/uploads/product/nature/header-7f0f25a8bece4e5a955988c0c521db53.png"></a></h1>
                            </div>
                        </div>
                        <div class="position-absolute position-right small-header-side">
                            <div class="pin-right">
                              
                                <a href="#search-menu" data-component="tray-button"
                                   class="pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray text11 contrast-text mr1"
                                   data-test="search-link" data-track="click" data-track-category="header"
                                   data-track-action="open tray" data-track-label="button">
                                            <span
                                                class="icon icon-above icon-search-25x25-white icon-search-25x25-gray block text-center">
                                                <span class="block mt6 small-header-hide">Search</span>
                                            </span>
                                </a>
                                
                                  <a href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;1"
                                     class="mq640-hide pin-left inline-group-top pa10 small-header-icons background-bluegray text11 contrast-text mr1"
                                     data-test="ealert-link" data-track="click" data-track-action="ealert"
                                     data-track-category="header" data-track-label="link">
                                            <span
                                                class="icon icon-above icon-ealert-25x25-white icon-ealert-25x25-gray block text-center">
                                                <span class="block mt6 small-header-hide">E-alert</span>
                                            </span>
                                  </a>
                                
                                
                                  <a href="http://mts-nature.nature.com/"
                                     class="mq640-hide inline-block pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray text11 contrast-text mr1"
                                     data-test="submit-link" data-track="click" data-track-action="manuscript submission"
                                     data-track-category="header" data-track-label="link">
                                            <span
                                                class="icon icon-above icon-submit-25x25-white icon-submit-25x25-gray block text-center">
                                                <span class="block mt6 small-header-hide">Submit</span>
                                            </span>
                                  </a>
                                
                                
<a href="/nams/svc/myaccount"
   id="my-account"
   class="placeholder inline-block pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray text11 contrast-text"
   data-test="login-link" data-track="click" data-track-action="my account" data-track-category="header" data-track-label="link">
            <span class="icon icon-above icon-login-25x25-white icon-login-25x25-gray block text-center">
                <span class="block mt6 small-header-hide">My Account</span>
            </span>
</a>
<a href="https://idp.nature.com/authorize/natureuser?client_id&#x3D;grover&amp;redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fnature16961"
   id="login-button"
   style="display: none;"
   class="placeholder inline-block pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray text11 contrast-text"
   data-test="login-link" data-track="click" data-track-action="login" data-track-category="header" data-track-label="link">
            <span class="icon icon-above icon-login-25x25-white icon-login-25x25-gray block text-center">
                <span class="block mt6 small-header-hide">Login</span>
            </span>
</a>




                              
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    

    
    

</div>



<div id="content" class="position-relative z-index-1">
    <div class="container cleared container-type-article" data-container-type="article">
        <div class="content position-relative cleared clear mq1200-padded" data-component="article-container">
            <article itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle">
                <div class="full-width-print main-column pin-left js-main-column highlighter" role="main">
                    <header>
                        <div class="grid grid-12 last">
                            <p class="text13 text-gray-light tiny-space-below" data-test="article-identifier">
                                Article
                                
                                <span class="pl6 pr6 text-gray-extra-light"> | </span>
                                <a href=#article-info>Published: <time datetime=2016-01-27 itemprop=datePublished>27 January 2016</time></a>
                            </p>
                            
                            <h1 class="tighten-line-height small-space-below" data-article-title="" itemprop="name headline">Mastering the game of Go with deep neural networks and tree search</h1>
                            <ul class="grid grid-12 last clean-list inline-list js-list-authors text14 small-space-below" data-etal=25 data-etal-small=3><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator></span><a href=#auth-1 class="icon icon-right-top icon-mail-12x9-blue pr15" data-corresp-id=c1 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>David Silver</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><sup class=js-hide> <a href=#n1>n1</a></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-2 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Aja Huang</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><sup class=js-hide> <a href=#n1>n1</a></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-3 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Chris J. Maddison</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-4 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Arthur Guez</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-5 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Laurent Sifre</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-6 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>George van den Driessche</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-7 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Julian Schrittwieser</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-8 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Ioannis Antonoglou</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-9 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Veda Panneershelvam</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-10 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Marc Lanctot</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-11 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Sander Dieleman</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-12 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Dominik Grewe</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-13 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>John Nham</span></a><sup class=js-hide><a href=#a2>2</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google, 1600 Amphitheatre Parkway, Mountain View"><meta itemprop=address content="Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-14 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Nal Kalchbrenner</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-15 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Ilya Sutskever</span></a><sup class=js-hide><a href=#a2>2</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google, 1600 Amphitheatre Parkway, Mountain View"><meta itemprop=address content="Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-16 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Timothy Lillicrap</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-17 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Madeleine Leach</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-18 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Koray Kavukcuoglu</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>, </span><a href=#auth-19 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Thore Graepel</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup><li itemprop=author itemscope itemtype=http://schema.org/Person><span class=js-separator>&nbsp;&amp; </span><a href=#auth-20 class="icon icon-right-top icon-mail-12x9-blue pr15" data-corresp-id=c2 data-track=click data-track-category="article body" data-track-action="open author" data-track-label=link><span itemprop=name>Demis Hassabis</span></a><sup class=js-hide><a href=#a1>1</a><span itemprop=affiliation itemscope itemtype=http://schema.org/Organization class=hide><meta itemprop=name content="Google DeepMind"><meta itemprop=address content="Google DeepMind, 5 New Street Square, London EC4A 3TW, UK"></span></sup></ul>
                            

                            <div class="grid-12 cleared" data-container-section=info><p class="text14 standard-space-below"><i>Nature</i> <b><span class=visually-hidden>volume </span>529</b>, <span class=visually-hidden>pages </span><span itemprop=pageStart>484</span>&ndash;<span itemprop=pageEnd>489</span> (28 January 2016)<span class="pl6 pr6 text-gray-extra-light"> | </span><a class="pr15 icon icon-right icon-download" data-track=click data-track-action="download article citation" data-track-category="article body" data-track-label=link href=/articles/nature16961.ris>Download Citation</a></p></div>
                            
                        </div>
                    </header>
                    
    <div class="js-hide" data-component="article-subject-links">
        
            <h3 class="h3 strong mb4">Subjects</h3>
            <ul class="mb0 pa0 tiny-space-above inline-list text14">
                <li><a href="/subjects/computational-science" class="subject-tag-link-steelblue mr2 mb2" data-track="click" data-track-action="view subject" data-track-category="article body" data-track-label="link">Computational science</a></li><li><a href="/subjects/computer-science" class="subject-tag-link-steelblue mr2 mb2" data-track="click" data-track-action="view subject" data-track-category="article body" data-track-label="link">Computer science</a></li><li><a href="/subjects/reward" class="subject-tag-link-steelblue mr2 mb2" data-track="click" data-track-action="view subject" data-track-category="article body" data-track-label="link">Reward</a></li>
            </ul>
        
    </div>

                    
    

                    

                    <div data-article-body="true" data-track-component="article body" class="article-body clear">
                        <section aria-labelledby=abstract><div class="serif article-section js-article-section cleared clear" id=abstract-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left" id=abstract>Abstract</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=abstract-content itemprop=description><p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.</p></div></div></section>

                        
                            
                                <div class="distractionFree border-top-1 border-bottom-1 border-gray-medium mb20 pt6 pl20 mq875-pl0 visually-hidden"></div>
                            
                        

                        
                            <section aria-labelledby="main"><div class="serif article-section js-article-section cleared clear" id="main-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="main">Main</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="main-content"><p>All games of perfect information have an optimal value function, <i>v</i><sup>*</sup>(<i>s</i>), which determines the outcome of the game, from every board position or state <i>s</i>, under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately <i>b</i><sup><i>d</i></sup> possible sequences of moves, where <i>b</i> is the game’s breadth (number of legal moves per position) and <i>d</i> is its depth (game length). In large games, such as chess (<i>b</i> ≈ 35, <i>d</i> ≈ 80)<sup><a id="ref-link-section-1" title="Allis, L. V. Searching for Solutions in Games and Artificial Intelligence. PhD thesis, Univ. Limburg, Maastricht, The Netherlands (1994)" href="/articles/nature16961#ref1" aria-label="Reference 1" data-track="click" data-track-action="reference anchor" data-track-label="link">1</a></sup> and especially Go (<i>b</i> ≈ 250, <i>d</i> ≈ 150)<sup><a id="ref-link-section-2" title="Allis, L. V. Searching for Solutions in Games and Artificial Intelligence. PhD thesis, Univ. Limburg, Maastricht, The Netherlands (1994)" href="/articles/nature16961#ref1" aria-label="Reference 1" data-track="click" data-track-action="reference anchor" data-track-label="link">1</a></sup>, exhaustive search is infeasible<sup><a id="ref-link-section-3" title="van den Herik, H., Uiterwijk, J. W. &amp; van Rijswijck, J. Games solved: now and in the future. Artif. Intell. 134, 277–311 (2002)" href="/articles/nature16961#ref2" aria-label="Reference 2" data-track="click" data-track-action="reference anchor" data-track-label="link">2</a>,<a id="ref-link-section-4" title="Schaeffer, J. The games computers (and people) play. Advances in Computers 52, 189–266 (2000)" href="/articles/nature16961#ref3" aria-label="Reference 3" data-track="click" data-track-action="reference anchor" data-track-label="link">3</a></sup>, but the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search tree at state <i>s</i> and replacing the subtree below <i>s</i> by an approximate value function <i>v</i>(<i>s</i>) ≈ <i>v</i><sup>*</sup>(<i>s</i>) that predicts the outcome from state <i>s</i>. This approach has led to superhuman performance in chess<sup><a id="ref-link-section-5" title="Campbell, M., Hoane, A. &amp; Hsu, F. Deep Blue. Artif. Intell. 134, 57–83 (2002)" href="/articles/nature16961#ref4" aria-label="Reference 4" data-track="click" data-track-action="reference anchor" data-track-label="link">4</a></sup>, checkers<sup><a id="ref-link-section-6" title="Schaeffer, J. et al. A world championship caliber checkers program. Artif. Intell. 53, 273–289 (1992)" href="/articles/nature16961#ref5" aria-label="Reference 5" data-track="click" data-track-action="reference anchor" data-track-label="link">5</a></sup> and othello<sup><a id="ref-link-section-7" title="Buro, M. From simple features to sophisticated evaluation functions. In 1st International Conference on Computers and Games, 126–145 (1999)" href="/articles/nature16961#ref6" aria-label="Reference 6" data-track="click" data-track-action="reference anchor" data-track-label="link">6</a></sup>, but it was believed to be intractable in Go due to the complexity of the game<sup><a id="ref-link-section-8" title="Müller, M. Computer Go. Artif. Intell. 134, 145–179 (2002)" href="/articles/nature16961#ref7" aria-label="Reference 7" data-track="click" data-track-action="reference anchor" data-track-label="link">7</a></sup>. Second, the breadth of the search may be reduced by sampling actions from a policy <i>p</i>(<i>a</i>|<i>s</i>) that is a probability distribution over possible moves <i>a</i> in position <i>s</i>. For example, Monte Carlo rollouts<sup><a id="ref-link-section-9" title="Tesauro, G. &amp; Galperin, G. On-line policy improvement using Monte-Carlo search. In Advances in Neural Information Processing, 1068–1074 (1996)" href="/articles/nature16961#ref8" aria-label="Reference 8" data-track="click" data-track-action="reference anchor" data-track-label="link">8</a></sup> search to maximum depth without branching at all, by sampling long sequences of actions for both players from a policy <i>p</i>. Averaging over such rollouts can provide an effective position evaluation, achieving superhuman performance in backgammon<sup><a id="ref-link-section-10" title="Tesauro, G. &amp; Galperin, G. On-line policy improvement using Monte-Carlo search. In Advances in Neural Information Processing, 1068–1074 (1996)" href="/articles/nature16961#ref8" aria-label="Reference 8" data-track="click" data-track-action="reference anchor" data-track-label="link">8</a></sup> and Scrabble<sup><a id="ref-link-section-11" title="Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241–275 (2002)" href="/articles/nature16961#ref9" aria-label="Reference 9" data-track="click" data-track-action="reference anchor" data-track-label="link">9</a></sup>, and weak amateur level play in Go<sup><a id="ref-link-section-12" title="Bouzy, B. &amp; Helmstetter, B. Monte-Carlo Go developments. In 10th International Conference on Advances in Computer Games, 159–174 (2003)" href="/articles/nature16961#ref10" aria-label="Reference 10" data-track="click" data-track-action="reference anchor" data-track-label="link">10</a></sup>.</p><p>Monte Carlo tree search (MCTS)<sup><a id="ref-link-section-13" title="Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In 5th International Conference on Computers and Games, 72–83 (2006)" href="/articles/nature16961#ref11" aria-label="Reference 11" data-track="click" data-track-action="reference anchor" data-track-label="link">11</a>,<a id="ref-link-section-14" title="Kocsis, L. &amp; Szepesvári, C. Bandit based Monte-Carlo planning. In 15th European Conference on Machine Learning, 282–293 (2006)" href="/articles/nature16961#ref12" aria-label="Reference 12" data-track="click" data-track-action="reference anchor" data-track-label="link">12</a></sup> uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function<sup><a id="ref-link-section-15" title="Kocsis, L. &amp; Szepesvári, C. Bandit based Monte-Carlo planning. In 15th European Conference on Machine Learning, 282–293 (2006)" href="/articles/nature16961#ref12" aria-label="Reference 12" data-track="click" data-track-action="reference anchor" data-track-label="link">12</a></sup>. The strongest current Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves<sup><a id="ref-link-section-16" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a></sup>. These policies are used to narrow the search to a beam of high-probability actions, and to sample actions during rollouts. This approach has achieved strong amateur play<sup><a id="ref-link-section-17" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a>,<a id="ref-link-section-18" title="Baudiš, P. &amp; Gailly, J.-L. Pachi: State of the art open source Go program. In Advances in Computer Games, 24–38 (Springer, 2012)" href="/articles/nature16961#ref14" aria-label="Reference 14" data-track="click" data-track-action="reference anchor" data-track-label="link">14</a>,<a id="ref-link-section-19" title="Müller, M., Enzenberger, M., Arneson, B. &amp; Segal, R. Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259–270 (2010)" href="/articles/nature16961#ref15" aria-label="Reference 15" data-track="click" data-track-action="reference anchor" data-track-label="link">15</a></sup>. However, prior work has been limited to shallow policies<sup><a id="ref-link-section-20" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a>,<a id="ref-link-section-21" title="Baudiš, P. &amp; Gailly, J.-L. Pachi: State of the art open source Go program. In Advances in Computer Games, 24–38 (Springer, 2012)" href="/articles/nature16961#ref14" aria-label="Reference 14" data-track="click" data-track-action="reference anchor" data-track-label="link">14</a>,<a id="ref-link-section-22" title="Müller, M., Enzenberger, M., Arneson, B. &amp; Segal, R. Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259–270 (2010)" href="/articles/nature16961#ref15" aria-label="Reference 15" data-track="click" data-track-action="reference anchor" data-track-label="link">15</a></sup> or value functions<sup><a id="ref-link-section-23" title="Gelly, S. &amp; Silver, D. Combining online and offline learning in UCT. In 17th International Conference on Machine Learning, 273–280 (2007)" href="/articles/nature16961#ref16" aria-label="Reference 16" data-track="click" data-track-action="reference anchor" data-track-label="link">16</a></sup> based on a linear combination of input features.</p><p>Recently, deep convolutional neural networks have achieved unprecedented performance in visual domains: for example, image classification<sup><a id="ref-link-section-24" title="Krizhevsky, A., Sutskever, I. &amp; Hinton, G. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 1097–1105 (2012)" href="/articles/nature16961#ref17" aria-label="Reference 17" data-track="click" data-track-action="reference anchor" data-track-label="link">17</a></sup>, face recognition<sup><a id="ref-link-section-25" title="Lawrence, S., Giles, C. L., Tsoi, A. C. &amp; Back, A. D. Face recognition: a convolutional neural-network approach. IEEE Trans. Neural Netw. 8, 98–113 (1997)" href="/articles/nature16961#ref18" aria-label="Reference 18" data-track="click" data-track-action="reference anchor" data-track-label="link">18</a></sup>, and playing Atari games<sup><a id="ref-link-section-26" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)" href="/articles/nature16961#ref19" aria-label="Reference 19" data-track="click" data-track-action="reference anchor" data-track-label="link">19</a></sup>. They use many layers of neurons, each arranged in overlapping tiles, to construct increasingly abstract, localized representations of an image<sup><a id="ref-link-section-27" title="LeCun, Y., Bengio, Y. &amp; Hinton, G. Deep learning. Nature 521, 436–444 (2015)" href="/articles/nature16961#ref20" aria-label="Reference 20" data-track="click" data-track-action="reference anchor" data-track-label="link">20</a></sup>. We employ a similar architecture for the game of Go. We pass in the board position as a 19 × 19 image and use convolutional layers to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.</p><p>We train the neural networks using a pipeline consisting of several stages of machine learning (<a href="#f1" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 1</a>). We begin by training a supervised learning (SL) policy network <i>p</i><sub><i>σ</i></sub> directly from expert human moves. This provides fast, efficient learning updates with immediate feedback and high-quality gradients. Similar to prior work<sup><a id="ref-link-section-28" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a>,<a id="ref-link-section-29" title="Müller, M., Enzenberger, M., Arneson, B. &amp; Segal, R. Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259–270 (2010)" href="/articles/nature16961#ref15" aria-label="Reference 15" data-track="click" data-track-action="reference anchor" data-track-label="link">15</a></sup>, we also train a fast policy <i>p</i><sub><i>π</i></sub> that can rapidly sample actions during rollouts. Next, we train a reinforcement learning (RL) policy network <i>p</i><sub><i>ρ</i></sub> that improves the SL policy network by optimizing the final outcome of games of self-play. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy. Finally, we train a value network <i>v</i><sub><i>θ</i></sub> that predicts the winner of games played by the RL policy network against itself. Our program AlphaGo efficiently combines the policy and value networks with MCTS.</p><div class="border-gray-medium border-all-5 standard-space-below pl10 pr10 pt20 pb20 clear" data-container-section="figure"><figure><figcaption><b id="f1" class="block tiny-space-below">Figure 1: Neural network training pipeline and architecture.</b></figcaption><div class="small-space-below"><a href="/articles/nature16961/figures/1" class="block small-space-below" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="image" rel="nofollow"><img src="https://media.nature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f1.jpg" alt="Figure 1" class="block max-width" itemprop="image" aria-describedby="f1-desc"></a><div class="suppress-bottom-margin add-top-margin text14 sans-serif" id="f1-desc"><p><b>a</b>, A fast rollout policy <i>p</i><sub><i>π</i></sub> and supervised learning (SL) policy network <i>p</i><sub><i>σ</i></sub> are trained to predict human expert moves in a data set of positions. A reinforcement learning (RL) policy network <i>p</i><sub><i>ρ</i></sub> is initialized to the SL policy network, and is then improved by policy gradient learning to maximize the outcome (that is, winning more games) against previous versions of the policy network. A new data set is generated by playing games of self-play with the RL policy network. Finally, a value network <i>v</i><sub><i>θ</i></sub> is trained by regression to predict the expected outcome (that is, whether the current player wins) in positions from the self-play data set. <b>b</b>, Schematic representation of the neural network architecture used in AlphaGo. The policy network takes a representation of the board position <i>s</i> as its input, passes it through many convolutional layers with parameters <i>σ</i> (SL policy network) or <i>ρ</i> (RL policy network), and outputs a probability distribution <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m52.gif" alt="" class="inline"> or <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m53.gif" alt="" class="inline"> over legal moves <i>a</i>, represented by a probability map over the board. The value network similarly uses many convolutional layers with parameters <i>θ</i>, but outputs a scalar value <i>v</i><sub><i>θ</i></sub>(<i>s</i>′) that predicts the expected outcome in position <i>s</i>′.</p></div></div><div class="text-right small-space-below hide-print"><a href="/articles/nature16961/figures/1" class="mb10 ml10 sans-serif pill-button inline-block" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="button" aria-label="Full size image of Neural network training pipeline and architecture." rel="nofollow"><span>Full size image</span></a></div><ul class="clean-list ma0 sans-serif text14 doctype-line-height hide-print"><li><a href="https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/slides/nature16961-pf1.ppt" class="icon icon-left icon-doctype-ppt-16x16-color pl20" data-track="click" data-track-category="article body" data-track-action="download resource" data-track-label="link">Download PowerPoint slide</a></li></ul></figure></div></div></div></section><section aria-labelledby="supervised-learning-of-policy-networks"><div class="serif article-section js-article-section cleared clear" id="supervised-learning-of-policy-networks-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="supervised-learning-of-policy-networks">Supervised learning of policy networks</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="supervised-learning-of-policy-networks-content"><p>For the first stage of the training pipeline, we build on prior work on predicting expert moves in the game of Go using supervised learning<sup><a id="ref-link-section-30" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a>,<a id="ref-link-section-31" title="Stern, D., Herbrich, R. &amp; Graepel, T. Bayesian pattern ranking for move prediction in the game of Go. In International Conference of Machine Learning, 873–880 (2006)" href="/articles/nature16961#ref21" aria-label="Reference 21" data-track="click" data-track-action="reference anchor" data-track-label="link">21</a>,<a id="ref-link-section-32" title="Sutskever, I. &amp; Nair, V. Mimicking Go experts with convolutional neural networks. In International Conference on Artificial Neural Networks, 101–110 (2008)" href="/articles/nature16961#ref22" aria-label="Reference 22" data-track="click" data-track-action="reference anchor" data-track-label="link">22</a>,<a id="ref-link-section-33" title="Maddison, C. J., Huang, A., Sutskever, I. &amp; Silver, D. Move evaluation in Go using deep convolutional neural networks. 3rd International Conference on Learning Representations (2015)" href="/articles/nature16961#ref23" aria-label="Reference 23" data-track="click" data-track-action="reference anchor" data-track-label="link">23</a>,<a id="ref-link-section-34" title="Clark, C. &amp; Storkey, A. J. Training deep convolutional neural networks to play go. In 32nd International Conference on Machine Learning, 1766–1774 (2015)" href="/articles/nature16961#ref24" aria-label="Reference 24" data-track="click" data-track-action="reference anchor" data-track-label="link">24</a></sup>. The SL policy network <i>p</i><sub><i>σ</i></sub>(<i>a|s</i>) alternates between convolutional layers with weights <i>σ</i>, and rectifier nonlinearities. A final softmax layer outputs a probability distribution over all legal moves <i>a</i>. The input <i>s</i> to the policy network is a simple representation of the board state (see <a href="#t2" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 2</a>). The policy network is trained on randomly sampled state-action pairs (<i>s</i>, <i>a</i>), using stochastic gradient ascent to maximize the likelihood of the human move <i>a</i> selected in state <i>s</i></p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m1.gif" alt="" class="block mt20 mb20"><p>We trained a 13-layer policy network, which we call the SL policy network, from 30 million positions from the KGS Go Server. The network predicted expert moves on a held out test set with an accuracy of 57.0% using all input features, and 55.7% using only raw board position and move history as inputs, compared to the state-of-the-art from other research groups of 44.4% at date of submission<sup><a id="ref-link-section-35" title="Clark, C. &amp; Storkey, A. J. Training deep convolutional neural networks to play go. In 32nd International Conference on Machine Learning, 1766–1774 (2015)" href="/articles/nature16961#ref24" aria-label="Reference 24" data-track="click" data-track-action="reference anchor" data-track-label="link">24</a></sup> (full results in <a href="#t3" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 3</a>). Small improvements in accuracy led to large improvements in playing strength (<a href="#f2" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 2a</a>); larger networks achieve better accuracy but are slower to evaluate during search. We also trained a faster but less accurate rollout policy <i>p</i><sub><i>π</i></sub>(<i>a</i>|<i>s</i>), using a linear softmax of small pattern features (see <a href="#t4" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 4</a>) with weights <i>π</i>; this achieved an accuracy of 24.2%, using just 2 μs to select an action, rather than 3 ms for the policy network.</p><div class="border-gray-medium border-all-5 standard-space-below pl10 pr10 pt20 pb20 clear" data-container-section="figure"><figure><figcaption><b id="f2" class="block tiny-space-below">Figure 2: Strength and accuracy of policy and value networks.</b></figcaption><div class="small-space-below"><a href="/articles/nature16961/figures/2" class="block small-space-below" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="image" rel="nofollow"><img src="https://media.nature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f2.jpg" alt="Figure 2" class="block max-width" aria-describedby="f2-desc"></a><div class="suppress-bottom-margin add-top-margin text14 sans-serif" id="f2-desc"><p><b>a</b>, Plot showing the playing strength of policy networks as a function of their training accuracy. Policy networks with 128, 192, 256 and 384 convolutional filters per layer were evaluated periodically during training; the plot shows the winning rate of AlphaGo using that policy network against the match version of AlphaGo. <b>b</b>, Comparison of evaluation accuracy between the value network and rollouts with different policies. Positions and outcomes were sampled from human expert games. Each position was evaluated by a single forward pass of the value network <i>v</i><sub><i>θ</i></sub>, or by the mean outcome of 100 rollouts, played out using either uniform random rollouts, the fast rollout policy <i>p</i><sub><i>π</i></sub>, the SL policy network <i>p</i><sub><i>σ</i></sub> or the RL policy network <i>p</i><sub><i>ρ</i></sub>. The mean squared error between the predicted value and the actual game outcome is plotted against the stage of the game (how many moves had been played in the given position).</p></div></div><div class="text-right small-space-below hide-print"><a href="/articles/nature16961/figures/2" class="mb10 ml10 sans-serif pill-button inline-block" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="button" aria-label="Full size image of Strength and accuracy of policy and value networks." rel="nofollow"><span>Full size image</span></a></div><ul class="clean-list ma0 sans-serif text14 doctype-line-height hide-print"><li><a href="https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/slides/nature16961-pf2.ppt" class="icon icon-left icon-doctype-ppt-16x16-color pl20" data-track="click" data-track-category="article body" data-track-action="download resource" data-track-label="link">Download PowerPoint slide</a></li></ul></figure></div></div></div></section><section aria-labelledby="reinforcement-learning-of-policy-networks"><div class="serif article-section js-article-section cleared clear" id="reinforcement-learning-of-policy-networks-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="reinforcement-learning-of-policy-networks">Reinforcement learning of policy networks</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="reinforcement-learning-of-policy-networks-content"><p>The second stage of the training pipeline aims at improving the policy network by policy gradient reinforcement learning (RL)<sup><a id="ref-link-section-36" title="Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992)" href="/articles/nature16961#ref25" aria-label="Reference 25" data-track="click" data-track-action="reference anchor" data-track-label="link">25</a>,<a id="ref-link-section-37" title="Sutton, R., McAllester, D., Singh, S. &amp; Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 1057–1063 (2000)" href="/articles/nature16961#ref26" aria-label="Reference 26" data-track="click" data-track-action="reference anchor" data-track-label="link">26</a></sup>. The RL policy network <i>p</i><sub><i>ρ</i></sub> is identical in structure to the SL policy network, and its weights <i>ρ</i> are initialized to the same values, <i>ρ</i> = <i>σ</i>. We play games between the current policy network <i>p</i><sub><i>ρ</i></sub> and a randomly selected previous iteration of the policy network. Randomizing from a pool of opponents in this way stabilizes training by preventing overfitting to the current policy. We use a reward function <i>r</i>(<i>s</i>) that is zero for all non-terminal time steps <i>t</i> &lt; <i>T</i>. The outcome <i>z</i><sub><i>t</i></sub> = ± <i>r</i>(<i>s</i><sub><i>T</i></sub>) is the terminal reward at the end of the game from the perspective of the current player at time step <i>t</i>: +1 for winning and −1 for losing. Weights are then updated at each time step <i>t</i> by stochastic gradient ascent in the direction that maximizes expected outcome<sup><a id="ref-link-section-38" title="Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992)" href="/articles/nature16961#ref25" aria-label="Reference 25" data-track="click" data-track-action="reference anchor" data-track-label="link">25</a></sup></p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m2.gif" alt="" class="block mt20 mb20"><p>We evaluated the performance of the RL policy network in game play, sampling each move <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m3.gif" alt="" class="inline"> from its output probability distribution over actions. When played head-to-head, the RL policy network won more than 80% of games against the SL policy network. We also tested against the strongest open-source Go program, Pachi<sup><a id="ref-link-section-39" title="Baudiš, P. &amp; Gailly, J.-L. Pachi: State of the art open source Go program. In Advances in Computer Games, 24–38 (Springer, 2012)" href="/articles/nature16961#ref14" aria-label="Reference 14" data-track="click" data-track-action="reference anchor" data-track-label="link">14</a></sup>, a sophisticated Monte Carlo search program, ranked at 2 amateur <i>dan</i> on KGS, that executes 100,000 simulations per move. Using no search at all, the RL policy network won 85% of games against Pachi. In comparison, the previous state-of-the-art, based only on supervised learning of convolutional networks, won 11% of games against Pachi<sup><a id="ref-link-section-40" title="Maddison, C. J., Huang, A., Sutskever, I. &amp; Silver, D. Move evaluation in Go using deep convolutional neural networks. 3rd International Conference on Learning Representations (2015)" href="/articles/nature16961#ref23" aria-label="Reference 23" data-track="click" data-track-action="reference anchor" data-track-label="link">23</a></sup> and 12% against a slightly weaker program, Fuego<sup><a id="ref-link-section-41" title="Clark, C. &amp; Storkey, A. J. Training deep convolutional neural networks to play go. In 32nd International Conference on Machine Learning, 1766–1774 (2015)" href="/articles/nature16961#ref24" aria-label="Reference 24" data-track="click" data-track-action="reference anchor" data-track-label="link">24</a></sup>.</p></div></div></section><section aria-labelledby="reinforcement-learning-of-value-networks"><div class="serif article-section js-article-section cleared clear" id="reinforcement-learning-of-value-networks-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="reinforcement-learning-of-value-networks">Reinforcement learning of value networks</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="reinforcement-learning-of-value-networks-content"><p>The final stage of the training pipeline focuses on position evaluation, estimating a value function <i>v</i><sup><i>p</i></sup>(<i>s</i>) that predicts the outcome from position <i>s</i> of games played by using policy <i>p</i> for both players<sup><a id="ref-link-section-42" title="Schraudolph, N. N., Dayan, P. &amp; Sejnowski, T. J. Temporal difference learning of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817–824 (1994)" href="/articles/nature16961#ref28" aria-label="Reference 28" data-track="click" data-track-action="reference anchor" data-track-label="link">28</a>,<a id="ref-link-section-43" title="Enzenberger, M. Evaluation in Go by a neural network using soft segmentation. In 10th Advances in Computer Games Conference, 97–108 (2003). 267" href="/articles/nature16961#ref29" aria-label="Reference 29" data-track="click" data-track-action="reference anchor" data-track-label="link">29</a>,<a id="ref-link-section-44" title="Silver, D., Sutton, R. &amp; Müller, M. Temporal-difference search in computer Go. Mach. Learn. 87, 183–219 (2012)" href="/articles/nature16961#ref30" aria-label="Reference 30" data-track="click" data-track-action="reference anchor" data-track-label="link">30</a></sup></p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m4.gif" alt="" class="block mt20 mb20"><p>Ideally, we would like to know the optimal value function under perfect play <i>v</i><sup>*</sup>(<i>s</i>); in practice, we instead estimate the value function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m5.gif" alt="" class="inline"> for our strongest policy, using the RL policy network <i>p</i><sub><i>ρ</i></sub>. We approximate the value function using a value network <i>v</i><sub><i>θ</i></sub>(<i>s</i>) with weights <i>θ</i>, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m6.gif" alt="" class="inline">. This neural network has a similar architecture to the policy network, but outputs a single prediction instead of a probability distribution. We train the weights of the value network by regression on state-outcome pairs (<i>s</i>, <i>z</i>), using stochastic gradient descent to minimize the mean squared error (MSE) between the predicted value <i>v</i><sub><i>θ</i></sub>(<i>s</i>), and the corresponding outcome <i>z</i></p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m7.gif" alt="" class="block mt20 mb20"><p>The naive approach of predicting game outcomes from data consisting of complete games leads to overfitting. The problem is that successive positions are strongly correlated, differing by just one stone, but the regression target is shared for the entire game. When trained on the KGS data set in this way, the value network memorized the game outcomes rather than generalizing to new positions, achieving a minimum MSE of 0.37 on the test set, compared to 0.19 on the training set. To mitigate this problem, we generated a new self-play data set consisting of 30 million distinct positions, each sampled from a separate game. Each game was played between the RL policy network and itself until the game terminated. Training on this data set led to MSEs of 0.226 and 0.234 on the training and test set respectively, indicating minimal overfitting. <a href="#f2" data-track="click" data-track-action="figure anchor" data-track-label="link">Figure 2b</a> shows the position evaluation accuracy of the value network, compared to Monte Carlo rollouts using the fast rollout policy <i>p</i><sub><i>π</i></sub>; the value function was consistently more accurate. A single evaluation of <i>v</i><sub><i>θ</i></sub>(<i>s</i>) also approached the accuracy of Monte Carlo rollouts using the RL policy network <i>p</i><sub><i>ρ</i></sub>, but using 15,000 times less computation.</p></div></div></section><section aria-labelledby="searching-with-policy-and-value-networks"><div class="serif article-section js-article-section cleared clear" id="searching-with-policy-and-value-networks-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="searching-with-policy-and-value-networks">Searching with policy and value networks</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="searching-with-policy-and-value-networks-content"><p>AlphaGo combines the policy and value networks in an MCTS algorithm (<a href="#f3" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 3</a>) that selects actions by lookahead search. Each edge (<i>s</i>, <i>a</i>) of the search tree stores an action value <i>Q</i>(<i>s</i>, <i>a</i>), visit count <i>N</i>(<i>s</i>, <i>a</i>), and prior probability <i>P</i>(<i>s</i>, <i>a</i>). The tree is traversed by simulation (that is, descending the tree in complete games without backup), starting from the root state. At each time step <i>t</i> of each simulation, an action <i>a</i><sub><i>t</i></sub> is selected from state <i>s</i><sub><i>t</i></sub></p><div class="border-gray-medium border-all-5 standard-space-below pl10 pr10 pt20 pb20 clear" data-container-section="figure"><figure><figcaption><b id="f3" class="block tiny-space-below">Figure 3: Monte Carlo tree search in AlphaGo.</b></figcaption><div class="small-space-below"><a href="/articles/nature16961/figures/3" class="block small-space-below" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="image" rel="nofollow"><img src="https://media.nature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f3.jpg" alt="Figure 3" class="block max-width" aria-describedby="f3-desc"></a><div class="suppress-bottom-margin add-top-margin text14 sans-serif" id="f3-desc"><p><b>a</b>, Each simulation traverses the tree by selecting the edge with maximum action value <i>Q</i>, plus a bonus <i>u</i>(<i>P</i>) that depends on a stored prior probability <i>P</i> for that edge. <b>b</b>, The leaf node may be expanded; the new node is processed once by the policy network <i>p</i><sub><i>σ</i></sub> and the output probabilities are stored as prior probabilities <i>P</i> for each action. <b>c</b>, At the end of a simulation, the leaf node is evaluated in two ways: using the value network <i>v</i><sub><i>θ</i></sub>; and by running a rollout to the end of the game with the fast rollout policy <i>p</i><sub><i>π</i></sub>, then computing the winner with function <i>r</i>. <b>d</b>, Action values <i>Q</i> are updated to track the mean value of all evaluations <i>r</i>(·) and <i>v</i><sub><i>θ</i></sub>(·) in the subtree below that action.</p></div></div><div class="text-right small-space-below hide-print"><a href="/articles/nature16961/figures/3" class="mb10 ml10 sans-serif pill-button inline-block" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="button" aria-label="Full size image of Monte Carlo tree search in AlphaGo." rel="nofollow"><span>Full size image</span></a></div><ul class="clean-list ma0 sans-serif text14 doctype-line-height hide-print"><li><a href="https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/slides/nature16961-pf3.ppt" class="icon icon-left icon-doctype-ppt-16x16-color pl20" data-track="click" data-track-category="article body" data-track-action="download resource" data-track-label="link">Download PowerPoint slide</a></li></ul></figure></div><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m8.gif" alt="" class="block mt20 mb20"><p>so as to maximize action value plus a bonus</p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m9.gif" alt="" class="block mt20 mb20"><p>that is proportional to the prior probability but decays with repeated visits to encourage exploration. When the traversal reaches a leaf node <i>s</i><sub><i>L</i></sub> at step <i>L</i>, the leaf node may be expanded. The leaf position <i>s</i><sub><i>L</i></sub> is processed just once by the SL policy network <i>p</i><sub><i>σ</i></sub>. The output probabilities are stored as prior probabilities <i>P</i> for each legal action <i>a</i>, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m10.gif" alt="" class="inline">. The leaf node is evaluated in two very different ways: first, by the value network <i>v</i><sub><i>θ</i></sub>(<i>s</i><sub><i>L</i></sub>); and second, by the outcome <i>z</i><sub><i>L</i></sub> of a random rollout played out until terminal step <i>T</i> using the fast rollout policy <i>p</i><sub><i>π</i></sub>; these evaluations are combined, using a mixing parameter <i>λ</i>, into a leaf evaluation <i>V</i>(<i>s</i><sub><i>L</i></sub>)</p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m11.gif" alt="" class="block mt20 mb20"><p>At the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge</p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m12.gif" alt="" class="block mt20 mb20"><p>where <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m13.gif" alt="" class="inline"> is the leaf node from the <i>i</i>th simulation, and 1(<i>s</i>, <i>a</i>, <i>i</i>) indicates whether an edge (<i>s</i>, <i>a</i>) was traversed during the <i>i</i>th simulation. Once the search is complete, the algorithm chooses the most visited move from the root position.</p><p>It is worth noting that the SL policy network <i>p</i><sub><i>σ</i></sub> performed better in AlphaGo than the stronger RL policy network <i>p</i><sub><i>ρ</i></sub>, presumably because humans select a diverse beam of promising moves, whereas RL optimizes for the single best move. However, the value function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m14.gif" alt="" class="inline"> derived from the stronger RL policy network performed better in AlphaGo than a value function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m15.gif" alt="" class="inline"> derived from the SL policy network.</p><p>Evaluating policy and value networks requires several orders of magnitude more computation than traditional search heuristics. To efficiently combine MCTS with deep neural networks, AlphaGo uses an asynchronous multi-threaded search that executes simulations on CPUs, and computes policy and value networks in parallel on GPUs. The final version of AlphaGo used 40 search threads, 48 CPUs, and 8 GPUs. We also implemented a distributed version of AlphaGo that exploited multiple machines, 40 search threads, 1,202 CPUs and 176 GPUs. The Methods section provides full details of asynchronous and distributed MCTS.</p></div></div></section><section aria-labelledby="evaluating-the-playing-strength-of-alphago"><div class="serif article-section js-article-section cleared clear" id="evaluating-the-playing-strength-of-alphago-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="evaluating-the-playing-strength-of-alphago">Evaluating the playing strength of AlphaGo</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="evaluating-the-playing-strength-of-alphago-content"><p>To evaluate AlphaGo, we ran an internal tournament among variants of AlphaGo and several other Go programs, including the strongest commercial programs Crazy Stone<sup><a id="ref-link-section-45" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a></sup> and Zen, and the strongest open source programs Pachi<sup><a id="ref-link-section-46" title="Baudiš, P. &amp; Gailly, J.-L. Pachi: State of the art open source Go program. In Advances in Computer Games, 24–38 (Springer, 2012)" href="/articles/nature16961#ref14" aria-label="Reference 14" data-track="click" data-track-action="reference anchor" data-track-label="link">14</a></sup> and Fuego<sup><a id="ref-link-section-47" title="Müller, M., Enzenberger, M., Arneson, B. &amp; Segal, R. Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259–270 (2010)" href="/articles/nature16961#ref15" aria-label="Reference 15" data-track="click" data-track-action="reference anchor" data-track-label="link">15</a></sup>. All of these programs are based on high-performance MCTS algorithms. In addition, we included the open source program GnuGo, a Go program using state-of-the-art search methods that preceded MCTS. All programs were allowed 5 s of computation time per move.</p><p>The results of the tournament (see <a href="#f4" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 4a</a>) suggest that single-machine AlphaGo is many <i>dan</i> ranks stronger than any previous Go program, winning 494 out of 495 games (99.8%) against other Go programs. To provide a greater challenge to AlphaGo, we also played games with four handicap stones (that is, free moves for the opponent); AlphaGo won 77%, 86%, and 99% of handicap games against Crazy Stone, Zen and Pachi, respectively. The distributed version of AlphaGo was significantly stronger, winning 77% of games against single-machine AlphaGo and 100% of its games against other programs.</p><div class="border-gray-medium border-all-5 standard-space-below pl10 pr10 pt20 pb20 clear" data-container-section="figure"><figure><figcaption><b id="f4" class="block tiny-space-below">Figure 4: Tournament evaluation of AlphaGo.</b></figcaption><div class="small-space-below"><a href="/articles/nature16961/figures/4" class="block small-space-below" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="image" rel="nofollow"><img src="https://media.nature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f4.jpg" alt="Figure 4" class="block max-width" aria-describedby="f4-desc"></a><div class="suppress-bottom-margin add-top-margin text14 sans-serif" id="f4-desc"><p><b>a</b>, Results of a tournament between different Go programs (see <a href="/articles/nature16961/tables/6" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Tables 6</a>,<a href="/articles/nature16961/tables/7" data-track="click" data-track-action="table anchor" data-track-label="link">7</a>,<a href="/articles/nature16961/tables/8" data-track="click" data-track-action="table anchor" data-track-label="link">8</a>,<a href="/articles/nature16961/tables/9" data-track="click" data-track-action="table anchor" data-track-label="link">9</a>,<a href="/articles/nature16961/tables/10" data-track="click" data-track-action="table anchor" data-track-label="link">10</a>,<a href="/articles/nature16961/tables/11" data-track="click" data-track-action="table anchor" data-track-label="link">11</a>). Each program used approximately 5 s computation time per move. To provide a greater challenge to AlphaGo, some programs (pale upper bars) were given four handicap stones (that is, free moves at the start of every game) against all opponents. Programs were evaluated on an Elo scale<sup><a id="ref-link-figure-1" title="Coulom, R. Whole-history rating: A Bayesian rating system for players of time-varying strength. In International Conference on Computers and Games, 113–124 (2008)" href="/articles/nature16961#ref37" aria-label="Reference 37" data-track="click" data-track-action="reference anchor" data-track-label="link">37</a></sup>: a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur <i>dan</i> rank advantage on KGS<sup><a id="ref-link-figure-2" title="KGS. Rating system math. http://www.gokgs.com/help/rmath.html" href="/articles/nature16961#ref38" aria-label="Reference 38" data-track="click" data-track-action="reference anchor" data-track-label="link">38</a></sup>; an approximate correspondence to human ranks is also shown, horizontal lines show KGS ranks achieved online by that program. Games against the human European champion Fan Hui were also included; these games used longer time controls. 95% confidence intervals are shown. <b>b</b>, Performance of AlphaGo, on a single machine, for different combinations of components. The version solely using the policy network does not perform any search. <b>c</b>, Scalability study of MCTS in AlphaGo with search threads and GPUs, using asynchronous search (light blue) or distributed search (dark blue), for 2 s per move.</p></div></div><div class="text-right small-space-below hide-print"><a href="/articles/nature16961/figures/4" class="mb10 ml10 sans-serif pill-button inline-block" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="button" aria-label="Full size image of Tournament evaluation of AlphaGo." rel="nofollow"><span>Full size image</span></a></div><ul class="clean-list ma0 sans-serif text14 doctype-line-height hide-print"><li><a href="https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/slides/nature16961-pf4.ppt" class="icon icon-left icon-doctype-ppt-16x16-color pl20" data-track="click" data-track-category="article body" data-track-action="download resource" data-track-label="link">Download PowerPoint slide</a></li></ul></figure></div><p>We also assessed variants of AlphaGo that evaluated positions using just the value network (<i>λ</i> = 0) or just rollouts (<i>λ</i> = 1) (see <a href="#f4" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 4b</a>). Even without rollouts AlphaGo exceeded the performance of all other Go programs, demonstrating that value networks provide a viable alternative to Monte Carlo evaluation in Go. However, the mixed evaluation (<i>λ</i> = 0.5) performed best, winning ≥95% of games against other variants. This suggests that the two position-evaluation mechanisms are complementary: the value network approximates the outcome of games played by the strong but impractically slow <i>p</i><sub><i>ρ</i></sub>, while the rollouts can precisely score and evaluate the outcome of games played by the weaker but faster rollout policy <i>p</i><sub><i>π</i></sub>. <a href="#f5" data-track="click" data-track-action="figure anchor" data-track-label="link">Figure 5</a> visualizes the evaluation of a real game position by AlphaGo.</p><div class="border-gray-medium border-all-5 standard-space-below pl10 pr10 pt20 pb20 clear" data-container-section="figure"><figure><figcaption><b id="f5" class="block tiny-space-below">Figure 5: How AlphaGo (black, to play) selected its move in an informal game against Fan Hui.</b></figcaption><div class="small-space-below"><a href="/articles/nature16961/figures/5" class="block small-space-below" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="image" rel="nofollow"><img src="https://media.nature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f5.jpg" alt="Figure 5" class="block max-width" aria-describedby="f5-desc"></a><div class="suppress-bottom-margin add-top-margin text14 sans-serif" id="f5-desc"><p>For each of the following statistics, the location of the maximum value is indicated by an orange circle. <b>a</b>, Evaluation of all successors <i>s</i>′ of the root position <i>s</i>, using the value network <i>v</i><sub><i>θ</i></sub>(<i>s</i>′); estimated winning percentages are shown for the top evaluations. <b>b</b>, Action values <i>Q</i>(<i>s</i>, <i>a</i>) for each edge (<i>s</i>, <i>a</i>) in the tree from root position <i>s</i>; averaged over value network evaluations only (<i>λ</i> = 0). <b>c</b>, Action values <i>Q</i>(<i>s</i>, <i>a</i>), averaged over rollout evaluations only (<i>λ</i> = 1). <b>d</b>, Move probabilities directly from the SL policy network, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m54.gif" alt="" class="inline">; reported as a percentage (if above 0.1%). <b>e</b>, Percentage frequency with which actions were selected from the root during simulations. <b>f</b>, The principal variation (path with maximum visit count) from AlphaGo’s search tree. The moves are presented in a numbered sequence. AlphaGo selected the move indicated by the red circle; Fan Hui responded with the move indicated by the white square; in his post-game commentary he preferred the move (labelled 1) predicted by AlphaGo.</p></div></div><div class="text-right small-space-below hide-print"><a href="/articles/nature16961/figures/5" class="mb10 ml10 sans-serif pill-button inline-block" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="button" aria-label="Full size image of How AlphaGo (black, to play) selected its move in an informal game against Fan Hui." rel="nofollow"><span>Full size image</span></a></div><ul class="clean-list ma0 sans-serif text14 doctype-line-height hide-print"><li><a href="https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/slides/nature16961-pf5.ppt" class="icon icon-left icon-doctype-ppt-16x16-color pl20" data-track="click" data-track-category="article body" data-track-action="download resource" data-track-label="link">Download PowerPoint slide</a></li></ul></figure></div><p>Finally, we evaluated the distributed version of AlphaGo against Fan Hui, a professional 2 <i>dan</i>, and the winner of the 2013, 2014 and 2015 European Go championships. Over 5–9 October 2015 AlphaGo and Fan Hui competed in a formal five-game match. AlphaGo won the match 5 games to 0 (<a href="#f6" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 6</a> and <a href="#t1" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 1</a>). This is the first time that a computer Go program has defeated a human professional player, without handicap, in the full game of Go—a feat that was previously believed to be at least a decade away<sup><a id="ref-link-section-48" title="Schaeffer, J. The games computers (and people) play. Advances in Computers 52, 189–266 (2000)" href="/articles/nature16961#ref3" aria-label="Reference 3" data-track="click" data-track-action="reference anchor" data-track-label="link">3</a>,<a id="ref-link-section-49" title="Müller, M. Computer Go. Artif. Intell. 134, 145–179 (2002)" href="/articles/nature16961#ref7" aria-label="Reference 7" data-track="click" data-track-action="reference anchor" data-track-label="link">7</a>,<a id="ref-link-section-50" title="Levinovitz, A. The mystery of Go, the ancient game that computers still can’t win. Wired Magazine (2014)" href="/articles/nature16961#ref31" aria-label="Reference 31" data-track="click" data-track-action="reference anchor" data-track-label="link">31</a></sup>.</p><div class="border-gray-medium border-all-5 standard-space-below pl10 pr10 pt20 pb20 clear" data-container-section="figure"><figure><figcaption><b id="f6" class="block tiny-space-below">Figure 6: Games from the match between AlphaGo and the European champion, Fan Hui.</b></figcaption><div class="small-space-below"><a href="/articles/nature16961/figures/6" class="block small-space-below" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="image" rel="nofollow"><img src="https://media.nature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f6.jpg" alt="Figure 6" class="block max-width" aria-describedby="f6-desc"></a><div class="suppress-bottom-margin add-top-margin text14 sans-serif" id="f6-desc"><p>Moves are shown in a numbered sequence corresponding to the order in which they were played. Repeated moves on the same intersection are shown in pairs below the board. The first move number in each pair indicates when the repeat move was played, at an intersection identified by the second move number (see <a href="/articles/nature16961#s1" data-track="click" data-track-action="supplementary material anchor" data-track-label="link">Supplementary Information</a>).</p></div></div><div class="text-right small-space-below hide-print"><a href="/articles/nature16961/figures/6" class="mb10 ml10 sans-serif pill-button inline-block" data-track="click" data-track-category="article body" data-track-action="view figure" data-track-label="button" aria-label="Full size image of Games from the match between AlphaGo and the European champion, Fan Hui." rel="nofollow"><span>Full size image</span></a></div><ul class="clean-list ma0 sans-serif text14 doctype-line-height hide-print"><li><a href="https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/slides/nature16961-pf6.ppt" class="icon icon-left icon-doctype-ppt-16x16-color pl20" data-track="click" data-track-category="article body" data-track-action="download resource" data-track-label="link">Download PowerPoint slide</a></li></ul></figure></div></div></div></section><section aria-labelledby="discussion"><div class="serif article-section js-article-section cleared clear" id="discussion-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="discussion">Discussion</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="discussion-content"><p>In this work we have developed a Go program, based on a combination of deep neural networks and tree search, that plays at the level of the strongest human players, thereby achieving one of artificial intelligence’s “grand challenges”<sup><a id="ref-link-section-51" title="Levinovitz, A. The mystery of Go, the ancient game that computers still can’t win. Wired Magazine (2014)" href="/articles/nature16961#ref31" aria-label="Reference 31" data-track="click" data-track-action="reference anchor" data-track-label="link">31</a>,<a id="ref-link-section-52" title="Mechner, D. All Systems Go. The Sciences 38, 32–37 (1998)" href="/articles/nature16961#ref32" aria-label="Reference 32" data-track="click" data-track-action="reference anchor" data-track-label="link">32</a>,<a id="ref-link-section-53" title="Mandziuk, J. Computational intelligence in mind games. In Challenges for Computational Intelligence, 407–442 (2007)" href="/articles/nature16961#ref33" aria-label="Reference 33" data-track="click" data-track-action="reference anchor" data-track-label="link">33</a></sup>. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.</p><p>During the match against Fan Hui, AlphaGo evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov<sup><a id="ref-link-section-54" title="Campbell, M., Hoane, A. &amp; Hsu, F. Deep Blue. Artif. Intell. 134, 57–83 (2002)" href="/articles/nature16961#ref4" aria-label="Reference 4" data-track="click" data-track-action="reference anchor" data-track-label="link">4</a></sup>; compensating by selecting those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network—an approach that is perhaps closer to how humans play. Furthermore, while Deep Blue relied on a handcrafted evaluation function, the neural networks of AlphaGo are trained directly from gameplay purely through general-purpose supervised and reinforcement learning methods.</p><p>Go is exemplary in many ways of the difficulties faced by artificial intelligence<sup><a id="ref-link-section-55" title="Mandziuk, J. Computational intelligence in mind games. In Challenges for Computational Intelligence, 407–442 (2007)" href="/articles/nature16961#ref33" aria-label="Reference 33" data-track="click" data-track-action="reference anchor" data-track-label="link">33</a>,<a id="ref-link-section-56" title="Berliner, H. A chronology of computer chess and its literature. Artif. Intell. 10, 201–214 (1978)" href="/articles/nature16961#ref34" aria-label="Reference 34" data-track="click" data-track-action="reference anchor" data-track-label="link">34</a></sup>: a challenging decision-making task, an intractable search space, and an optimal solution so complex it appears infeasible to directly approximate using a policy or value function. The previous major breakthrough in computer Go, the introduction of MCTS, led to corresponding advances in many other domains; for example, general game-playing, classical planning, partially observed planning, scheduling, and constraint satisfaction<sup><a id="ref-link-section-57" title="Browne, C. et al. A survey of Monte-Carlo tree search methods. IEEE Trans. Comput. Intell. AI in Games 4, 1–43 (2012)" href="/articles/nature16961#ref35" aria-label="Reference 35" data-track="click" data-track-action="reference anchor" data-track-label="link">35</a>,<a id="ref-link-section-58" title="Gelly, S. et al. The grand challenge of computer Go: Monte Carlo tree search and extensions. Commun. ACM 55, 106–113 (2012)" href="/articles/nature16961#ref36" aria-label="Reference 36" data-track="click" data-track-action="reference anchor" data-track-label="link">36</a></sup>. By combining tree search with policy and value networks, AlphaGo has finally reached a professional level in Go, providing hope that human-level performance can now be achieved in other seemingly intractable artificial intelligence domains.</p></div></div></section><section aria-labelledby="methods"><div class="serif article-section js-article-section cleared clear" id="methods-section"><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id="methods">Methods</h2><div class="pl20 mq875-pl0 js-collapsible-section" id="methods-content"><h3 class="h3 strong mb4">Problem setting</h3><p>Many games of perfect information, such as chess, checkers, othello, backgammon and Go, may be defined as alternating Markov games<sup><a id="ref-link-section-59" title="Littman, M. L. Markov games as a framework for multi-agent reinforcement learning. In 11th International Conference on Machine Learning, 157–163 (1994)" href="/articles/nature16961#ref39" aria-label="Reference 39" data-track="click" data-track-action="reference anchor" data-track-label="link">39</a></sup>. In these games, there is a state space <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m16.gif" alt="" class="inline"> (where state includes an indication of the current player to play); an action space <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m17.gif" alt="" class="inline"> defining the legal actions in any given state <i>s</i> <span class="stix"><span class="stix">∈</span></span> <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m18.gif" alt="" class="inline">; a state transition function <i>f</i>(<i>s</i>, <i>a</i>, <i>ξ</i>) defining the successor state after selecting action <i>a</i> in state <i>s</i> and random input <i>ξ</i> (for example, dice); and finally a reward function <i>r</i><sup><i>i</i></sup>(<i>s</i>) describing the reward received by player <i>i</i> in state <i>s</i>. We restrict our attention to two-player zero-sum games, <i>r</i><sup>1</sup>(<i>s</i>) = −<i>r</i><sup>2</sup>(<i>s</i>) = <i>r</i>(<i>s</i>), with deterministic state transitions, <i>f</i>(<i>s</i>, <i>a</i>, <i>ξ</i>) = <i>f</i>(<i>s</i>, <i>a</i>), and zero rewards except at a terminal time step <i>T</i>. The outcome of the game <i>z</i><sub><i>t</i></sub> = ±<i>r</i>(<i>s</i><sub><i>T</i></sub>) is the terminal reward at the end of the game from the perspective of the current player at time step <i>t</i>. A policy <i>p</i>(<i>a</i>|<i>s</i>) is a probability distribution over legal actions <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m19.gif" alt="" class="inline">. A value function is the expected outcome if all actions for both players are selected according to policy <i>p</i>, that is, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m20.gif" alt="" class="inline"> . Zero-sum games have a unique optimal value function <i>v</i>*(<i>s</i>) that determines the outcome from state <i>s</i> following perfect play by both players,</p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m21.gif" alt="" class="block mt20 mb20"><h3 class="h3 strong mb4">Prior work</h3><p>The optimal value function can be computed recursively by minimax (or equivalently negamax) search<sup><a id="ref-link-section-60" title="Knuth, D. E. &amp; Moore, R. W. An analysis of alpha-beta pruning. Artif. Intell. 6, 293–326 (1975)" href="/articles/nature16961#ref40" aria-label="Reference 40" data-track="click" data-track-action="reference anchor" data-track-label="link">40</a></sup>. Most games are too large for exhaustive minimax tree search; instead, the game is truncated by using an approximate value function <i>v</i>(<i>s</i>) ≈ <i>v</i><sup>*</sup>(<i>s</i>) in place of terminal rewards. Depth-first minimax search with alpha–beta pruning<sup><a id="ref-link-section-61" title="Knuth, D. E. &amp; Moore, R. W. An analysis of alpha-beta pruning. Artif. Intell. 6, 293–326 (1975)" href="/articles/nature16961#ref40" aria-label="Reference 40" data-track="click" data-track-action="reference anchor" data-track-label="link">40</a></sup> has achieved superhuman performance in chess<sup><a id="ref-link-section-62" title="Campbell, M., Hoane, A. &amp; Hsu, F. Deep Blue. Artif. Intell. 134, 57–83 (2002)" href="/articles/nature16961#ref4" aria-label="Reference 4" data-track="click" data-track-action="reference anchor" data-track-label="link">4</a></sup>, checkers<sup><a id="ref-link-section-63" title="Schaeffer, J. et al. A world championship caliber checkers program. Artif. Intell. 53, 273–289 (1992)" href="/articles/nature16961#ref5" aria-label="Reference 5" data-track="click" data-track-action="reference anchor" data-track-label="link">5</a></sup> and othello<sup><a id="ref-link-section-64" title="Buro, M. From simple features to sophisticated evaluation functions. In 1st International Conference on Computers and Games, 126–145 (1999)" href="/articles/nature16961#ref6" aria-label="Reference 6" data-track="click" data-track-action="reference anchor" data-track-label="link">6</a></sup>, but it has not been effective in Go<sup><a id="ref-link-section-65" title="Müller, M. Computer Go. Artif. Intell. 134, 145–179 (2002)" href="/articles/nature16961#ref7" aria-label="Reference 7" data-track="click" data-track-action="reference anchor" data-track-label="link">7</a></sup>.</p><p>Reinforcement learning can learn to approximate the optimal value function directly from games of self-play<sup><a id="ref-link-section-66" title="Littman, M. L. Markov games as a framework for multi-agent reinforcement learning. In 11th International Conference on Machine Learning, 157–163 (1994)" href="/articles/nature16961#ref39" aria-label="Reference 39" data-track="click" data-track-action="reference anchor" data-track-label="link">39</a></sup>. The majority of prior work has focused on a linear combination <i>v</i><sub><i>θ</i></sub>(<i>s</i>) = <i>φ</i>(<i>s</i>) · <i>θ</i> of features <i>φ</i>(<i>s</i>) with weights <i>θ</i>. Weights were trained using temporal-difference learning<sup><a id="ref-link-section-67" title="Sutton, R. Learning to predict by the method of temporal differences. Mach. Learn. 3, 9–44 (1988)" href="/articles/nature16961#ref41" aria-label="Reference 41" data-track="click" data-track-action="reference anchor" data-track-label="link">41</a></sup> in chess<sup><a id="ref-link-section-68" title="Baxter, J., Tridgell, A. &amp; Weaver, L. Learning to play chess using temporal differences. Mach. Learn. 40, 243–263 (2000)" href="/articles/nature16961#ref42" aria-label="Reference 42" data-track="click" data-track-action="reference anchor" data-track-label="link">42</a>,<a id="ref-link-section-69" title="Veness, J., Silver, D., Blair, A. &amp; Uther, W. Bootstrapping from game tree search. In Advances in Neural Information Processing Systems (2009)" href="/articles/nature16961#ref43" aria-label="Reference 43" data-track="click" data-track-action="reference anchor" data-track-label="link">43</a></sup>, checkers<sup><a id="ref-link-section-70" title="Samuel, A. L. Some studies in machine learning using the game of checkers II - recent progress. IBM J. Res. Develop. 11, 601–617 (1967)" href="/articles/nature16961#ref44" aria-label="Reference 44" data-track="click" data-track-action="reference anchor" data-track-label="link">44</a>,<a id="ref-link-section-71" title="Schaeffer, J., Hlynka, M. &amp; Jussila, V. Temporal difference learning applied to a high-performance game-playing program. In 17th International Joint Conference on Artificial Intelligence, 529–534 (2001)" href="/articles/nature16961#ref45" aria-label="Reference 45" data-track="click" data-track-action="reference anchor" data-track-label="link">45</a></sup> and Go<sup><a id="ref-link-section-72" title="Silver, D., Sutton, R. &amp; Müller, M. Temporal-difference search in computer Go. Mach. Learn. 87, 183–219 (2012)" href="/articles/nature16961#ref30" aria-label="Reference 30" data-track="click" data-track-action="reference anchor" data-track-label="link">30</a></sup>; or using linear regression in othello<sup><a id="ref-link-section-73" title="Buro, M. From simple features to sophisticated evaluation functions. In 1st International Conference on Computers and Games, 126–145 (1999)" href="/articles/nature16961#ref6" aria-label="Reference 6" data-track="click" data-track-action="reference anchor" data-track-label="link">6</a></sup> and Scrabble<sup><a id="ref-link-section-74" title="Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241–275 (2002)" href="/articles/nature16961#ref9" aria-label="Reference 9" data-track="click" data-track-action="reference anchor" data-track-label="link">9</a></sup>. Temporal-difference learning has also been used to train a neural network to approximate the optimal value function, achieving superhuman performance in backgammon<sup><a id="ref-link-section-75" title="Tesauro, G. TD-gammon, a self-teaching backgammon program, achieves master-level play. Neural Comput. 6, 215–219 (1994)" href="/articles/nature16961#ref46" aria-label="Reference 46" data-track="click" data-track-action="reference anchor" data-track-label="link">46</a></sup>; and achieving weak <i>kyu</i>-level performance in small-board Go<sup><a id="ref-link-section-76" title="Schraudolph, N. N., Dayan, P. &amp; Sejnowski, T. J. Temporal difference learning of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817–824 (1994)" href="/articles/nature16961#ref28" aria-label="Reference 28" data-track="click" data-track-action="reference anchor" data-track-label="link">28</a>,<a id="ref-link-section-77" title="Enzenberger, M. Evaluation in Go by a neural network using soft segmentation. In 10th Advances in Computer Games Conference, 97–108 (2003). 267" href="/articles/nature16961#ref29" aria-label="Reference 29" data-track="click" data-track-action="reference anchor" data-track-label="link">29</a>,<a id="ref-link-section-78" title="Dahl, F. Honte, a Go-playing program using neural nets. In Machines that learn to play games, 205–223 (Nova Science, 1999)" href="/articles/nature16961#ref47" aria-label="Reference 47" data-track="click" data-track-action="reference anchor" data-track-label="link">47</a></sup> using convolutional networks.</p><p>An alternative approach to minimax search is Monte Carlo tree search (MCTS)<sup><a id="ref-link-section-79" title="Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In 5th International Conference on Computers and Games, 72–83 (2006)" href="/articles/nature16961#ref11" aria-label="Reference 11" data-track="click" data-track-action="reference anchor" data-track-label="link">11</a>,<a id="ref-link-section-80" title="Kocsis, L. &amp; Szepesvári, C. Bandit based Monte-Carlo planning. In 15th European Conference on Machine Learning, 282–293 (2006)" href="/articles/nature16961#ref12" aria-label="Reference 12" data-track="click" data-track-action="reference anchor" data-track-label="link">12</a></sup>, which estimates the optimal value of interior nodes by a double approximation, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m22.gif" alt="" class="inline">. The first approximation, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m23.gif" alt="" class="inline">, uses <i>n</i> Monte Carlo simulations to estimate the value function of a simulation policy <i>P</i><sup><i>n</i></sup>. The second approximation, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m24.gif" alt="" class="inline">, uses a simulation policy <i>P</i><sup><i>n</i></sup> in place of minimax optimal actions. The simulation policy selects actions according to a search control function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m25.gif" alt="" class="inline">, such as UCT<sup><a id="ref-link-section-81" title="Kocsis, L. &amp; Szepesvári, C. Bandit based Monte-Carlo planning. In 15th European Conference on Machine Learning, 282–293 (2006)" href="/articles/nature16961#ref12" aria-label="Reference 12" data-track="click" data-track-action="reference anchor" data-track-label="link">12</a></sup>, that selects children with higher action values, <i>Q</i><sup><i>n</i></sup>(<i>s</i>, <i>a</i>) = −<i>V</i><sup><i>n</i></sup>(<i>f</i>(<i>s</i>, <i>a</i>)), plus a bonus <i>u</i>(<i>s</i>, <i>a</i>) that encourages exploration; or in the absence of a search tree at state <i>s</i>, it samples actions from a fast rollout policy <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m26.gif" alt="" class="inline"> . As more simulations are executed and the search tree grows deeper, the simulation policy becomes informed by increasingly accurate statistics. In the limit, both approximations become exact and MCTS (for example, with UCT) converges<sup><a id="ref-link-section-82" title="Kocsis, L. &amp; Szepesvári, C. Bandit based Monte-Carlo planning. In 15th European Conference on Machine Learning, 282–293 (2006)" href="/articles/nature16961#ref12" aria-label="Reference 12" data-track="click" data-track-action="reference anchor" data-track-label="link">12</a></sup> to the optimal value function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m27.gif" alt="" class="inline">. The strongest current Go programs are based on MCTS<sup><a id="ref-link-section-83" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a>,<a id="ref-link-section-84" title="Baudiš, P. &amp; Gailly, J.-L. Pachi: State of the art open source Go program. In Advances in Computer Games, 24–38 (Springer, 2012)" href="/articles/nature16961#ref14" aria-label="Reference 14" data-track="click" data-track-action="reference anchor" data-track-label="link">14</a>,<a id="ref-link-section-85" title="Müller, M., Enzenberger, M., Arneson, B. &amp; Segal, R. Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259–270 (2010)" href="/articles/nature16961#ref15" aria-label="Reference 15" data-track="click" data-track-action="reference anchor" data-track-label="link">15</a>,<a id="ref-link-section-86" title="Gelly, S. et al. The grand challenge of computer Go: Monte Carlo tree search and extensions. Commun. ACM 55, 106–113 (2012)" href="/articles/nature16961#ref36" aria-label="Reference 36" data-track="click" data-track-action="reference anchor" data-track-label="link">36</a></sup>.</p><p>MCTS has previously been combined with a policy that is used to narrow the beam of the search tree to high-probability moves<sup><a id="ref-link-section-87" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a></sup>; or to bias the bonus term towards high-probability moves<sup><a id="ref-link-section-88" title="Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203–230 (2011)" href="/articles/nature16961#ref48" aria-label="Reference 48" data-track="click" data-track-action="reference anchor" data-track-label="link">48</a></sup>. MCTS has also been combined with a value function that is used to initialize action values in newly expanded nodes<sup><a id="ref-link-section-89" title="Gelly, S. &amp; Silver, D. Combining online and offline learning in UCT. In 17th International Conference on Machine Learning, 273–280 (2007)" href="/articles/nature16961#ref16" aria-label="Reference 16" data-track="click" data-track-action="reference anchor" data-track-label="link">16</a></sup>, or to mix Monte Carlo evaluation with minimax evaluation<sup><a id="ref-link-section-90" title="Lanctot, M., Winands, M. H. M., Pepels, T. &amp; Sturtevant, N. R. Monte Carlo tree search with heuristic evaluations using implicit minimax backups. In IEEE Conference on Computational Intelligence and Games, 1–8 (2014)" href="/articles/nature16961#ref49" aria-label="Reference 49" data-track="click" data-track-action="reference anchor" data-track-label="link">49</a></sup>. By contrast, AlphaGo’s use of value functions is based on truncated Monte Carlo search algorithms<sup><a id="ref-link-section-91" title="Tesauro, G. &amp; Galperin, G. On-line policy improvement using Monte-Carlo search. In Advances in Neural Information Processing, 1068–1074 (1996)" href="/articles/nature16961#ref8" aria-label="Reference 8" data-track="click" data-track-action="reference anchor" data-track-label="link">8</a>,<a id="ref-link-section-92" title="Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241–275 (2002)" href="/articles/nature16961#ref9" aria-label="Reference 9" data-track="click" data-track-action="reference anchor" data-track-label="link">9</a></sup>, which terminate rollouts before the end of the game and use a value function in place of the terminal reward. AlphaGo’s position evaluation mixes full rollouts with truncated rollouts, resembling in some respects the well-known temporal-difference learning algorithm TD(<i>λ</i>). AlphaGo also differs from prior work by using slower but more powerful representations of the policy and value function; evaluating deep neural networks is several orders of magnitude slower than linear representations and must therefore occur asynchronously.</p><p>The performance of MCTS is to a large degree determined by the quality of the rollout policy. Prior work has focused on handcrafted patterns<sup><a id="ref-link-section-93" title="Gelly, S., Wang, Y., Munos, R. &amp; Teytaud, O. Modification of UCT with patterns in Monte-Carlo Go. Tech. Rep. 6062, INRIA (2006)" href="/articles/nature16961#ref50" aria-label="Reference 50" data-track="click" data-track-action="reference anchor" data-track-label="link">50</a></sup> or learning rollout policies by supervised learning<sup><a id="ref-link-section-94" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a></sup>, reinforcement learning<sup><a id="ref-link-section-95" title="Gelly, S. &amp; Silver, D. Combining online and offline learning in UCT. In 17th International Conference on Machine Learning, 273–280 (2007)" href="/articles/nature16961#ref16" aria-label="Reference 16" data-track="click" data-track-action="reference anchor" data-track-label="link">16</a></sup>, simulation balancing<sup><a id="ref-link-section-96" title="Silver, D. &amp; Tesauro, G. Monte-Carlo simulation balancing. In 26th International Conference on Machine Learning, 119 (2009)" href="/articles/nature16961#ref51" aria-label="Reference 51" data-track="click" data-track-action="reference anchor" data-track-label="link">51</a>,<a id="ref-link-section-97" title="Huang, S.-C., Coulom, R. &amp; Lin, S.-S. Monte-Carlo simulation balancing in practice. In 7th International Conference on Computers and Games, 81–92 (Springer-Verlag, 2011)" href="/articles/nature16961#ref52" aria-label="Reference 52" data-track="click" data-track-action="reference anchor" data-track-label="link">52</a></sup> or online adaptation<sup><a id="ref-link-section-98" title="Silver, D., Sutton, R. &amp; Müller, M. Temporal-difference search in computer Go. Mach. Learn. 87, 183–219 (2012)" href="/articles/nature16961#ref30" aria-label="Reference 30" data-track="click" data-track-action="reference anchor" data-track-label="link">30</a>,<a id="ref-link-section-99" title="Baier, H. &amp; Drake, P. D. The power of forgetting: improving the last-good-reply policy in Monte Carlo Go. IEEE Trans. Comput. Intell. AI in Games 2, 303–309 (2010)" href="/articles/nature16961#ref53" aria-label="Reference 53" data-track="click" data-track-action="reference anchor" data-track-label="link">53</a></sup>; however, it is known that rollout-based position evaluation is frequently inaccurate<sup><a id="ref-link-section-100" title="Huang, S. &amp; Müller, M. Investigating the limits of Monte-Carlo tree search methods in computer Go. In 8th International Conference on Computers and Games, 39–48 (2013)" href="/articles/nature16961#ref54" aria-label="Reference 54" data-track="click" data-track-action="reference anchor" data-track-label="link">54</a></sup>. AlphaGo uses relatively simple rollouts, and instead addresses the challenging problem of position evaluation more directly using value networks.</p><h3 class="h3 strong mb4">Search algorithm</h3><p>To efficiently integrate large neural networks into AlphaGo, we implemented an asynchronous policy and value MCTS algorithm (APV-MCTS). Each node <i>s</i> in the search tree contains edges (<i>s</i>, <i>a</i>) for all legal actions <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m28.gif" alt="" class="inline">. Each edge stores a set of statistics,</p><img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m29.gif" alt="" class="block mt20 mb20"><p>where <i>P</i>(<i>s</i>, <i>a</i>) is the prior probability, <i>W</i><sub><i>v</i></sub>(<i>s</i>, <i>a</i>) and <i>W</i><sub><i>r</i></sub>(<i>s</i>, <i>a</i>) are Monte Carlo estimates of total action value, accumulated over <i>N</i><sub><i>v</i></sub>(<i>s</i>, <i>a</i>) and <i>N</i><sub><i>r</i></sub>(<i>s</i>, <i>a</i>) leaf evaluations and rollout rewards, respectively, and <i>Q</i>(<i>s</i>, <i>a</i>) is the combined mean action value for that edge. Multiple simulations are executed in parallel on separate search threads. The APV-MCTS algorithm proceeds in the four stages outlined in <a href="#f3" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 3</a>.</p><p><i>Selection (<a href="#f3" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 3a</a>)</i>. The first in-tree phase of each simulation begins at the root of the search tree and finishes when the simulation reaches a leaf node at time step <i>L</i>. At each of these time steps, <i>t</i> &lt; <i>L</i>, an action is selected according to the statistics in the search tree, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m30.gif" alt="" class="inline"> using a variant of the PUCT algorithm<sup><a id="ref-link-section-101" title="Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203–230 (2011)" href="/articles/nature16961#ref48" aria-label="Reference 48" data-track="click" data-track-action="reference anchor" data-track-label="link">48</a></sup>, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m31.gif" alt="" class="inline">, where <i>c</i><sub>puct</sub> is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action value.</p><p><i>Evaluation (<a href="#f3" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 3c</a>)</i>. The leaf position <i>s</i><sub><i>L</i></sub> is added to a queue for evaluation <i>v</i><sub><i>θ</i></sub>(<i>s</i><sub><i>L</i></sub>) by the value network, unless it has previously been evaluated. The second rollout phase of each simulation begins at leaf node <i>s</i><sub><i>L</i></sub> and continues until the end of the game. At each of these time-steps, <i>t</i> ≥ <i>L</i>, actions are selected by both players according to the rollout policy, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m32.gif" alt="" class="inline">. When the game reaches a terminal state, the outcome <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m33.gif" alt="" class="inline"> is computed from the final score.</p><p><i>Backup (<a href="#f3" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 3d</a>)</i>. At each in-tree step <i>t</i> ≤ <i>L</i> of the simulation, the rollout statistics are updated as if it has lost <i>n</i><sub>vl</sub> games, <i>N</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) ← <i>N</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) + <i>n</i><sub>vl</sub>; <i>W</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) ← <i>W</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) −<i>n</i><sub>vl</sub>; this virtual loss<sup><a id="ref-link-section-102" title="Segal, R. B. On the scalability of parallel UCT. Computers and Games 6515, 36–47 (2011)" href="/articles/nature16961#ref55" aria-label="Reference 55" data-track="click" data-track-action="reference anchor" data-track-label="link">55</a></sup> discourages other threads from simultaneously exploring the identical variation. At the end of the simulation, t he rollout statistics are updated in a backward pass through each step <i>t</i> ≤ <i>L</i>, replacing the virtual losses by the outcome, <i>N</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) ← <i>N</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) −<i>n</i><sub>vl</sub> + 1; <i>W</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) ← <i>W</i><sub><i>r</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) + <i>n</i><sub>vl</sub> + <i>z</i><sub><i>t</i></sub>. Asynchronously, a separate backward pass is initiated when the evaluation of the leaf position <i>s</i><sub><i>L</i></sub> completes. The output of the value network <i>v</i><sub><i>θ</i></sub>(<i>s</i><sub><i>L</i></sub>) is used to update value statistics in a second backward pass through each step <i>t</i> ≤ <i>L</i>, <i>N</i><sub><i>v</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) ← <i>N</i><sub><i>v</i></sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) + 1, <i>W</i><sub>v</sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) ← <i>W</i><sub>v</sub>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) + <i>v</i><sub><i>θ</i></sub>(<i>s</i><sub><i>L</i></sub>). The overall evaluation of each state action is a weighted average of the Monte Carlo estimates, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m34.gif" alt="" class="inline">, that mixes together the value network and rollout evaluations with weighting parameter <i>λ</i>. All updates are performed lock-free<sup><a id="ref-link-section-103" title="Enzenberger, M. &amp; Müller, M. A lock-free multithreaded Monte-Carlo tree search algorithm. In 12th Advances in Computer Games Conference, 14–20 (2009)" href="/articles/nature16961#ref56" aria-label="Reference 56" data-track="click" data-track-action="reference anchor" data-track-label="link">56</a></sup>.</p><p><i>Expansion (<a href="#f3" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 3b</a>)</i>. When the visit count exceeds a threshold, <i>N</i><sub><i>r</i></sub>(<i>s</i>, <i>a</i>) &gt; <i>n</i><sub>thr</sub>, the successor state <i>s</i>′ = <i>f</i>(<i>s</i>, <i>a</i>) is added to the search tree. The new node is initialized to {<i>N</i><sub><i></i></sub>(<i>s</i>′, <i>a</i>) =<sub><i> </i></sub><i>N</i><sub><i>r</i></sub>(<i>s</i>′, <i>a</i>) = 0, <i>W</i><sub><i></i></sub>(<i>s</i>′, <i>a</i>) =<sub><i> </i></sub><i>W</i><sub><i>r</i></sub>(<i>s</i>′, <i>a</i>) = 0, <i>P</i>(<i>s</i>′,<i>a</i>) = <i>p</i><sub><i>σ</i></sub>(<i>a</i>|<i>s</i>′)}, using a tree policy <i>p</i><sub><i></i></sub><sub>τ</sub>(<i>a</i>|<i>s</i>′) (similar to the rollout policy but with more features, see <a href="#t4" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 4</a>) to provide placeholder prior probabilities for action selection. The position <i>s</i>′ is also inserted into a queue for asynchronous GPU evaluation by the policy network. Prior probabilities are computed by the SL policy network <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m35.gif" alt="" class="inline"> with a softmax temperature set to <i>β</i>; these replace the placeholder prior probabilities, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m36.gif" alt="" class="inline">, using an atomic update. The threshold <i>n</i><sub>thr</sub> is adjusted dynamically to ensure that the rate at which positions are added to the policy queue matches the rate at which the GPUs evaluate the policy network. Positions are evaluated by both the policy network and the value network using a mini-batch size of 1 to minimize end-to-end evaluation time.</p><p>We also implemented a distributed APV-MCTS algorithm. This architecture consists of a single master machine that executes the main search, many remote worker CPUs that execute asynchronous rollouts, and many remote worker GPUs that execute asynchronous policy and value network evaluations. The entire search tree is stored on the master, which only executes the in-tree phase of each simulation. The leaf positions are communicated to the worker CPUs, which execute the rollout phase of simulation, and to the worker GPUs, which compute network features and evaluate the policy and value networks. The prior probabilities of the policy network are returned to the master, where they replace placeholder prior probabilities at the newly expanded node. The rewards from rollouts and the value network outputs are each returned to the master, and backed up the originating search path.</p><p>At the end of search AlphaGo selects the action with maximum visit count; this is less sensitive to outliers than maximizing action value<sup><a id="ref-link-section-104" title="Müller, M., Enzenberger, M., Arneson, B. &amp; Segal, R. Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259–270 (2010)" href="/articles/nature16961#ref15" aria-label="Reference 15" data-track="click" data-track-action="reference anchor" data-track-label="link">15</a></sup>. The search tree is reused at subsequent time steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded. The match version of AlphaGo continues searching during the opponent’s move. It extends the search if the action maximizing visit count and the action maximizing action value disagree. Time controls were otherwise shaped to use most time in the middle-game<sup><a id="ref-link-section-105" title="Huang, S.-C., Coulom, R. &amp; Lin, S.-S. Time management for Monte-Carlo tree search applied to the game of Go. In International Conference on Technologies and Applications of Artificial Intelligence, 462–466 (2010)" href="/articles/nature16961#ref57" aria-label="Reference 57" data-track="click" data-track-action="reference anchor" data-track-label="link">57</a></sup>. AlphaGo resigns when its overall evaluation drops below an estimated 10% probability of winning the game, that is, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m37.gif" alt="" class="inline">.</p><p>AlphaGo does not employ the all-moves-as-first<sup><a id="ref-link-section-106" title="Bouzy, B. &amp; Helmstetter, B. Monte-Carlo Go developments. In 10th International Conference on Advances in Computer Games, 159–174 (2003)" href="/articles/nature16961#ref10" aria-label="Reference 10" data-track="click" data-track-action="reference anchor" data-track-label="link">10</a></sup> or rapid action value estimation<sup><a id="ref-link-section-107" title="Gelly, S. &amp; Silver, D. Monte-Carlo tree search and rapid action value estimation in computer Go. Artif. Intell. 175, 1856–1875 (2011)" href="/articles/nature16961#ref58" aria-label="Reference 58" data-track="click" data-track-action="reference anchor" data-track-label="link">58</a></sup> heuristics used in the majority of Monte Carlo Go programs; when using policy networks as prior knowledge, these biased heuristics do not appear to give any additional benefit. In addition AlphaGo does not use progressive widening<sup><a id="ref-link-section-108" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a></sup>, dynamic <i>komi</i><sup><a id="ref-link-section-109" title="Baudiš, P. Balancing MCTS by dynamically adjusting the komi value. ICGA J. 34, 131 (2011)" href="/articles/nature16961#ref59" aria-label="Reference 59" data-track="click" data-track-action="reference anchor" data-track-label="link">59</a></sup> or an opening book<sup><a id="ref-link-section-110" title="Baier, H. &amp; Winands, M. H. Active opening book application for Monte-Carlo tree search in 19×19 Go. In Benelux Conference on Artificial Intelligence, 3–10 (2011)" href="/articles/nature16961#ref60" aria-label="Reference 60" data-track="click" data-track-action="reference anchor" data-track-label="link">60</a></sup>. The parameters used by AlphaGo in the Fan Hui match are listed in <a href="#t5" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 5</a>.</p><h3 class="h3 strong mb4">Rollout policy</h3><p>The rollout policy <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m38.gif" alt="" class="inline"> is a linear softmax policy based on fast, incrementally computed, local pattern-based features consisting of both ‘response’ patterns around the previous move that led to state <i>s</i>, and ‘non-response’ patterns around the candidate move <i>a</i> in state <i>s</i>. Each non-response pattern is a binary feature matching a specific 3 × 3 pattern centred on <i>a</i>, defined by the colour (black, white, empty) and liberty count (1, 2, ≥3) for each adjacent intersection. Each response pattern is a binary feature matching the colour and liberty count in a 12-point diamond-shaped pattern<sup><a id="ref-link-section-111" title="Stern, D., Herbrich, R. &amp; Graepel, T. Bayesian pattern ranking for move prediction in the game of Go. In International Conference of Machine Learning, 873–880 (2006)" href="/articles/nature16961#ref21" aria-label="Reference 21" data-track="click" data-track-action="reference anchor" data-track-label="link">21</a></sup> centred around the previous move. Additionally, a small number of handcrafted local features encode common-sense Go rules (see <a href="#t4" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 4</a>). Similar to the policy network, the weights <i>π</i> of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board.</p><p>Our rollout policy <i>p</i><sub><i>π</i></sub>(<i>a</i>|<i>s</i>) contains less handcrafted knowledge than state-of-the-art Go programs<sup><a id="ref-link-section-112" title="Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198–208 (2007)" href="/articles/nature16961#ref13" aria-label="Reference 13" data-track="click" data-track-action="reference anchor" data-track-label="link">13</a></sup>. Instead, we exploit the higher-quality action selection within MCTS, which is informed both by the search tree and the policy network. We introduce a new technique that caches all moves from the search tree and then plays similar moves during rollouts; a generalization of the ‘last good reply’ heuristic<sup><a id="ref-link-section-113" title="Baier, H. &amp; Drake, P. D. The power of forgetting: improving the last-good-reply policy in Monte Carlo Go. IEEE Trans. Comput. Intell. AI in Games 2, 303–309 (2010)" href="/articles/nature16961#ref53" aria-label="Reference 53" data-track="click" data-track-action="reference anchor" data-track-label="link">53</a></sup>. At every step of the tree traversal, the most probable action is inserted into a hash table, along with the 3 × 3 pattern context (colour, liberty and stone counts) around both the previous move and the current move. At each step of the rollout, the pattern context is matched against the hash table; if a match is found then the stored move is played with high probability.</p><h3 class="h3 strong mb4">Symmetries</h3><p>In previous work, the symmetries of Go have been exploited by using rotationally and reflectionally invariant filters in the convolutional layers<sup><a id="ref-link-section-114" title="Clark, C. &amp; Storkey, A. J. Training deep convolutional neural networks to play go. In 32nd International Conference on Machine Learning, 1766–1774 (2015)" href="/articles/nature16961#ref24" aria-label="Reference 24" data-track="click" data-track-action="reference anchor" data-track-label="link">24</a>,<a id="ref-link-section-115" title="Schraudolph, N. N., Dayan, P. &amp; Sejnowski, T. J. Temporal difference learning of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817–824 (1994)" href="/articles/nature16961#ref28" aria-label="Reference 28" data-track="click" data-track-action="reference anchor" data-track-label="link">28</a>,<a id="ref-link-section-116" title="Enzenberger, M. Evaluation in Go by a neural network using soft segmentation. In 10th Advances in Computer Games Conference, 97–108 (2003). 267" href="/articles/nature16961#ref29" aria-label="Reference 29" data-track="click" data-track-action="reference anchor" data-track-label="link">29</a></sup>. Although this may be effective in small neural networks, it actually hurts performance in larger networks, as it prevents the intermediate filters from identifying specific asymmetric patterns<sup><a id="ref-link-section-117" title="Maddison, C. J., Huang, A., Sutskever, I. &amp; Silver, D. Move evaluation in Go using deep convolutional neural networks. 3rd International Conference on Learning Representations (2015)" href="/articles/nature16961#ref23" aria-label="Reference 23" data-track="click" data-track-action="reference anchor" data-track-label="link">23</a></sup>. Instead, we exploit symmetries at run-time by dynamically transforming each position <i>s</i> using the dihedral group of eight reflections and rotations, <i>d</i><sub>1</sub>(<i>s</i>), …, <i>d</i><sub>8</sub>(<i>s</i>). In an explicit symmetry ensemble, a mini-batch of all 8 positions is passed into the policy network or value network and computed in parallel. For the value network, the output values are simply averaged, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m39.gif" alt="" class="inline">. For the policy network, the planes of output probabilities are rotated/reflected back into the original orientation, and averaged together to provide an ensemble prediction, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m40.gif" alt="" class="inline">; this approach was used in our raw network evaluation (see <a href="#t3" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 3</a>). Instead, APV-MCTS makes use of an implicit symmetry ensemble that randomly selects a single rotation/reflection <i>j</i> <span class="stix"><span class="stix">∈</span></span> [1, 8] for each evaluation. We compute exactly one evaluation for that orientation only; in each simulation we compute the value of leaf node <i>s</i><sub><i>L</i></sub> by <i>v</i><sub><i>θ</i></sub>(<i>d</i><sub><i>j</i></sub>(<i>s</i><sub><i>L</i></sub>)), and allow the search procedure to average over these evaluations. Similarly, we compute the policy network for a single, randomly selected rotation/reflection, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m41.gif" alt="" class="inline">.</p><h3 class="h3 strong mb4">Policy network: classification</h3><p>We trained the policy network <i>p</i><sub><i>σ</i></sub> to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 <i>dan</i> human players; 35.4% of the games are handicap games. The data set was split into a test set (the first million positions) and a training set (the remaining 28.4 million positions). Pass moves were excluded from the data set. Each position consisted of a raw board description <i>s</i> and the move <i>a</i> selected by the human. We augmented the data set to include all eight reflections and rotations of each position. Symmetry augmentation and input features were pre-computed for each position. For each training step, we sampled a randomly selected mini-batch of <i>m</i> samples from the augmented KGS data set, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m42.gif" alt="" class="inline"> and applied an asynchronous stochastic gradient descent update to maximize the log likelihood of the action, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m43.gif" alt="" class="inline">. The step size <i>α</i> was initialized to 0.003 and was halved every 80 million training steps, without momentum terms, and a mini-batch size of <i>m</i> = 16. Updates were applied asynchronously on 50 GPUs using DistBelief <sup><a id="ref-link-section-118" title="Dean, J. et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, 1223–1231 (2012)" href="/articles/nature16961#ref61" aria-label="Reference 61" data-track="click" data-track-action="reference anchor" data-track-label="link">61</a></sup>; gradients older than 100 steps were discarded. Training took around 3 weeks for 340 million training steps.</p><h3 class="h3 strong mb4">Policy network: reinforcement learning</h3><p>We further trained the policy network by policy gradient reinforcement learning<sup><a id="ref-link-section-119" title="Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992)" href="/articles/nature16961#ref25" aria-label="Reference 25" data-track="click" data-track-action="reference anchor" data-track-label="link">25</a>,<a id="ref-link-section-120" title="Sutton, R., McAllester, D., Singh, S. &amp; Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 1057–1063 (2000)" href="/articles/nature16961#ref26" aria-label="Reference 26" data-track="click" data-track-action="reference anchor" data-track-label="link">26</a></sup>. Each iteration consisted of a mini-batch of <i>n</i> games played in parallel, between the current policy network <i>p</i><sub><i>ρ</i></sub> that is being trained, and an opponent <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m44.gif" alt="" class="inline"> that uses parameters <i>ρ</i><sup>−</sup> from a previous iteration, randomly sampled from a pool of opponents, so as to increase the stability of training. Weights were initialized to <i>ρ</i> = <i>ρ</i><sup>−</sup> = <i>σ</i>. Every 500 iterations, we added the current parameters <i>ρ</i> to the opponent pool. Each game <i>i</i> in the mini-batch was played out until termination at step <i>T</i><sup><i>i</i></sup>, and then scored to determine the outcome <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m45.gif" alt="" class="inline"> from each player’s perspective. The games were then replayed to determine the policy gradient update, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m46.gif" alt="" class="inline">, using the REINFORCE algorithm<sup><a id="ref-link-section-121" title="Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992)" href="/articles/nature16961#ref25" aria-label="Reference 25" data-track="click" data-track-action="reference anchor" data-track-label="link">25</a></sup> with baseline <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m47.gif" alt="" class="inline"> for variance reduction. On the first pass through the training pipeline, the baseline was set to zero; on the second pass we used the value network <i>v</i><sub><i>θ</i></sub>(<i>s</i>) as a baseline; this provided a small performance boost. The policy network was trained in this way for 10,000 mini-batches of 128 games, using 50 GPUs, for one day.</p><h3 class="h3 strong mb4">Value network: regression</h3><p>We trained a value network <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m48.gif" alt="" class="inline"> to approximate the value function of the RL policy network <i>p</i><sub><i>ρ</i></sub>. To avoid overfitting to the strongly correlated positions within games, we constructed a new data set of uncorrelated self-play positions. This data set consisted of over 30 million positions, each drawn from a unique game of self-play. Each game was generated in three phases by randomly sampling a time step <i>U</i> ~ unif{1, 450}, and sampling the first <i>t</i> = 1,… <i>U</i> − 1 moves from the SL policy network, <i>a</i><sub><i>t</i></sub> ~ <i>p</i><sub><i>σ</i></sub>(·|<i>s</i><sub><i>t</i></sub>); then sampling one move uniformly at random from available moves, <i>a</i><sub><i>U</i></sub> ~ unif{1, 361} (repeatedly until <i>a</i><sub><i>U</i></sub> is legal); then sampling the remaining sequence of moves until the game terminates, <i>t</i> = <i>U</i> + 1, … <i>T</i>, from the RL policy network, <i>a</i><sub><i>t</i></sub> ~ <i>p</i><sub><i>ρ</i></sub>(·|<i>s</i><sub><i>t</i></sub>). Finally, the game is scored to determine the outcome <i>z</i><sub><i>t</i></sub> = ±<i>r</i>(<i>s</i><sub><i>T</i></sub>). Only a single training example (<i>s</i><sub><i>U</i>+1</sub>, <i>z</i><sub><i>U</i>+1</sub>) is added to the data set from each game. This data provides unbiased samples of the value function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m49.gif" alt="" class="inline">. During the first two phases of generation we sample from noisier distributions so as to increase the diversity of the data set. The training method was identical to SL policy network training, except that the parameter update was based on mean squared error between the predicted values and the observed rewards, <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m50.gif" alt="" class="inline">. The value network was trained for 50 million mini-batches of 32 positions, using 50 GPUs, for one week.</p><h3 class="h3 strong mb4">Features for policy/value network</h3><p>Each position <i>s</i> was pre-processed into a set of 19 × 19 feature planes. The features that we use come directly from the raw representation of the game rules, indicating the status of each intersection of the Go board: stone colour, liberties (adjacent empty points of stone’s chain), captures, legality, turns since stone was played, and (for the value network only) the current colour to play. In addition, we use one simple tactical feature that computes the outcome of a ladder search<sup><a id="ref-link-section-122" title="Müller, M. Computer Go. Artif. Intell. 134, 145–179 (2002)" href="/articles/nature16961#ref7" aria-label="Reference 7" data-track="click" data-track-action="reference anchor" data-track-label="link">7</a></sup>. All features were computed relative to the current colour to play; for example, the stone colour at each intersection was represented as either player or opponent rather than black or white. Each integer feature value is split into multiple 19 × 19 planes of binary values (one-hot encoding). For example, separate binary feature planes are used to represent whether an intersection has 1 liberty, 2 liberties,…, ≥8 liberties. The full set of feature planes are listed in <a href="#t2" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 2</a>.</p><h3 class="h3 strong mb4">Neural network architecture</h3><p>The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves <i>k</i> filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves <i>k</i> filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used <i>k</i> = 192 filters; <a href="#f2" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 2b</a> and <a href="#t3" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 3</a> additionally show the results of training with <i>k</i> = 128, 256 and 384 filters.</p><p>The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.</p><h3 class="h3 strong mb4">Evaluation</h3><p>We evaluated the relative strength of computer Go programs by running an internal tournament and measuring the Elo rating of each program. We estimate the probability that program <i>a</i> will beat program <i>b</i> by a logistic function <img src="https://media.nature.com/full/nature-assets/nature/journal/v529/n7587/images/nature16961-m51.gif" alt="" class="inline">, and estimate the ratings <i>e</i>(·) by Bayesian logistic regression, computed by the <i>BayesElo</i> program<sup><a id="ref-link-section-123" title="Coulom, R. Whole-history rating: A Bayesian rating system for players of time-varying strength. In International Conference on Computers and Games, 113–124 (2008)" href="/articles/nature16961#ref37" aria-label="Reference 37" data-track="click" data-track-action="reference anchor" data-track-label="link">37</a></sup> using the standard constant <i>c</i><sub>elo</sub> = 1/400. The scale was anchored to the <i>BayesElo</i> rating of professional Go player Fan Hui (2,908 at date of submission)<sup><a id="ref-link-section-124" title="Go ratings. http://www.goratings.org" href="/articles/nature16961#ref62" aria-label="Reference 62" data-track="click" data-track-action="reference anchor" data-track-label="link">62</a></sup>. All programs received a maximum of 5 s computation time per move; games were scored using Chinese rules with a <i>komi</i> of 7.5 points (extra points to compensate white for playing second). We also played handicap games where AlphaGo played white against existing Go programs; for these games we used a non-standard handicap system in which <i>komi</i> was retained but black was given additional stones on the usual handicap points. Using these rules, a handicap of <i>K</i> stones is equivalent to giving <i>K</i> − 1 free moves to black, rather than <i>K</i> − 1/2 free moves using standard no-<i>komi</i> handicap rules. We used these handicap rules because AlphaGo’s value network was trained specifically to use a <i>komi</i> of 7.5.</p><p>With the exception of distributed AlphaGo, each computer Go program was executed on its own single machine, with identical specifications, using the latest available version and the best hardware configuration supported by that program (see <a href="#t6" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 6</a>). In <a href="#f4" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 4</a>, approximate ranks of computer programs are based on the highest KGS rank achieved by that program; however, the KGS version may differ from the publicly available version.</p><p>The match against Fan Hui was arbitrated by an impartial referee. Five formal games and five informal games were played with 7.5 <i>komi</i>, no handicap, and Chinese rules. AlphaGo won these games 5–0 and 3–2 respectively (<a href="#f6" data-track="click" data-track-action="figure anchor" data-track-label="link">Fig. 6</a> and <a href="#t1" data-track="click" data-track-action="table anchor" data-track-label="link">Extended Data Table 1</a>). Time controls for formal games were 1 h main time plus three periods of 30 s <i>byoyomi</i>. Time controls for informal games were three periods of 30 s <i>byoyomi</i>. Time controls and playing conditions were chosen by Fan Hui in advance of the match; it was also agreed that the overall match outcome would be determined solely by the formal games. To approximately assess the relative rating of Fan Hui to computer Go programs, we appended the results of all ten games to our internal tournament results, ignoring differences in time controls.</p></div></div></section>
                        

                        <section aria-labelledby=references><div class="serif article-section js-article-section cleared clear" id=references-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=references>References</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=references-content><div data-container-section=references><ol class="clean-list ma0 standard-space-below indented-list" data-component=reading-companion-references><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">1.</span><p id=ref1 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Allis, L. V.</span></span> <i>Searching for Solutions in Games and Artificial Intelligence.</i> PhD thesis, Univ. Limburg, Maastricht, The Netherlands (1994)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;author=Allis%2CL.%20V.&amp;journal=Searching%20for%20Solutions%20in%20Games%20and%20Artificial%20Intelligence.&amp;publication_year=1994" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 1 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">2.</span><p id=ref2 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>van den Herik, H.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Uiterwijk, J. W.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>van Rijswijck, J.</span></span> <cite itemprop="name headline">Games solved: now and in the future</cite>. <i>Artif. Intell.</i> <b>134</b>, 277–311 (2002)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/S0004-3702(01)00152-7 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 2">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Games%20solved%3A%20now%20and%20in%20the%20future&amp;author=van%20den%20Herik%2CH.&amp;author=Uiterwijk%2CJ.%20W.&amp;author=van%20Rijswijck%2CJ.&amp;journal=Artif.%20Intell.&amp;volume=134&amp;pages=277-311&amp;publication_year=2002" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 2 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">3.</span><p id=ref3 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Schaeffer, J.</span></span> <cite itemprop="name headline">The games computers (and people) play</cite>. <i>Advances in Computers</i> <b>52</b>, 189–266 (2000)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20games%20computers%20(and%20people)%20play&amp;author=Schaeffer%2CJ.&amp;journal=Advances%20in%20Computers&amp;volume=52&amp;pages=189-266&amp;publication_year=2000" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 3 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">4.</span><p id=ref4 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Campbell, M.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Hoane, A.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Hsu, F.</span></span> <cite itemprop="name headline">Deep Blue</cite>. <i>Artif. Intell.</i> <b>134</b>, 57–83 (2002)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/S0004-3702(01)00129-1 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 4">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20Blue&amp;author=Campbell%2CM.&amp;author=Hoane%2CA.&amp;author=Hsu%2CF.&amp;journal=Artif.%20Intell.&amp;volume=134&amp;pages=57-83&amp;publication_year=2002" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 4 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">5.</span><p id=ref5 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Schaeffer, J.</span></span> <i>et al.</i> <cite itemprop="name headline">A world championship caliber checkers program</cite>. <i>Artif. Intell.</i> <b>53</b>, 273–289 (1992)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/0004-3702(92)90074-8 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 5">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20world%20championship%20caliber%20checkers%20program&amp;author=Schaeffer%2CJ.&amp;journal=Artif.%20Intell.&amp;volume=53&amp;pages=273-289&amp;publication_year=1992" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 5 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">6.</span><p id=ref6 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Buro, M.</span></span> <cite itemprop="name headline">From simple features to sophisticated evaluation functions</cite>. In <i>1st International Conference on Computers and Games</i>, 126–145 (1999)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=From%20simple%20features%20to%20sophisticated%20evaluation%20functions&amp;author=Buro%2CM.&amp;journal=1st%20International%20Conference%20on%20Computers%20and%20Games&amp;pages=126-145&amp;publication_year=1999" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 6 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">7.</span><p id=ref7 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Müller, M.</span></span> <cite itemprop="name headline">Computer Go</cite>. <i>Artif. Intell.</i> <b>134</b>, 145–179 (2002)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/S0004-3702(01)00121-7 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 7">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20Go&amp;author=M%C3%BCller%2CM.&amp;journal=Artif.%20Intell.&amp;volume=134&amp;pages=145-179&amp;publication_year=2002" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 7 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">8.</span><p id=ref8 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Tesauro, G.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Galperin, G.</span></span> <cite itemprop="name headline">On-line policy improvement using Monte-Carlo search</cite>. In <i>Advances in Neural Information Processing</i>, 1068–1074 (1996)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=On-line%20policy%20improvement%20using%20Monte-Carlo%20search&amp;author=Tesauro%2CG.&amp;author=Galperin%2CG.&amp;journal=Advances%20in%20Neural%20Information%20Processing&amp;pages=1068-1074&amp;publication_year=1996" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 8 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">9.</span><p id=ref9 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sheppard, B.</span></span> <cite itemprop="name headline">World-championship-caliber Scrabble</cite>. <i>Artif. Intell.</i> <b>134</b>, 241–275 (2002)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/S0004-3702(01)00166-7 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 9">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=World-championship-caliber%20Scrabble&amp;author=Sheppard%2CB.&amp;journal=Artif.%20Intell.&amp;volume=134&amp;pages=241-275&amp;publication_year=2002" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 9 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">10.</span><p id=ref10 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Bouzy, B.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Helmstetter, B.</span></span> <cite itemprop="name headline">Monte-Carlo Go developments</cite>. In <i>10th International Conference on Advances in Computer Games</i>, 159–174 (2003)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Monte-Carlo%20Go%20developments&amp;author=Bouzy%2CB.&amp;author=Helmstetter%2CB.&amp;journal=10th%20International%20Conference%20on%20Advances%20in%20Computer%20Games&amp;pages=159-174&amp;publication_year=2003" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 10 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">11.</span><p id=ref11 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Coulom, R.</span></span> <cite itemprop="name headline">Efficient selectivity and backup operators in Monte-Carlo tree search</cite>. In <i>5th International Conference on Computers and Games</i>, 72–83 (2006)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20selectivity%20and%20backup%20operators%20in%20Monte-Carlo%20tree%20search&amp;author=Coulom%2CR.&amp;journal=5th%20International%20Conference%20on%20Computers%20and%20Games&amp;pages=72-83&amp;publication_year=2006" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 11 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">12.</span><p id=ref12 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Kocsis, L.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Szepesvári, C.</span></span> <cite itemprop="name headline">Bandit based Monte-Carlo planning</cite>. In <i>15th European Conference on Machine Learning</i>, 282–293 (2006)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Bandit%20based%20Monte-Carlo%20planning&amp;author=Kocsis%2CL.&amp;author=Szepesv%C3%A1ri%2CC.&amp;journal=15th%20European%20Conference%20on%20Machine%20Learning&amp;pages=282-293&amp;publication_year=2006" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 12 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">13.</span><p id=ref13 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Coulom, R.</span></span> <cite itemprop="name headline">Computing Elo ratings of move patterns in the game of Go</cite>. <i>ICGA J.</i> <b>30</b>, 198–208 (2007)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Computing%20Elo%20ratings%20of%20move%20patterns%20in%20the%20game%20of%20Go&amp;author=Coulom%2CR.&amp;journal=ICGA%20J.&amp;volume=30&amp;pages=198-208&amp;publication_year=2007" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 13 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">14.</span><p id=ref14 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Baudiš, P.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Gailly, J.-L.</span></span> <cite itemprop="name headline">Pachi: State of the art open source Go program</cite>. In <i>Advances in Computer Games</i>, 24–38 (Springer, 2012)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Pachi%3A%20State%20of%20the%20art%20open%20source%20Go%20program&amp;author=Baudi%C5%A1%2CP.&amp;author=Gailly%2CJ.-L.&amp;journal=Advances%20in%20Computer%20Games&amp;pages=24-38&amp;publication_year=2012" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 14 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">15.</span><p id=ref15 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Müller, M.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Enzenberger, M.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Arneson, B.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Segal, R.</span></span> <cite itemprop="name headline">Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search</cite>. <i>IEEE Trans. Comput. Intell. AI in Games</i> <b>2</b>, 259–270 (2010)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1109/TCIAIG.2010.2083662 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 15">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Fuego%20%E2%80%93%20an%20open-source%20framework%20for%20board%20games%20and%20Go%20engine%20based%20on%20Monte-Carlo%20tree%20search&amp;author=M%C3%BCller%2CM.&amp;author=Enzenberger%2CM.&amp;author=Arneson%2CB.&amp;author=Segal%2CR.&amp;journal=IEEE%20Trans.%20Comput.%20Intell.%20AI%20in%20Games&amp;volume=2&amp;pages=259-270&amp;publication_year=2010" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 15 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">16.</span><p id=ref16 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Gelly, S.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Silver, D.</span></span> <cite itemprop="name headline">Combining online and offline learning in UCT</cite>. In <i>17th International Conference on Machine Learning</i>, 273–280 (2007)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Combining%20online%20and%20offline%20learning%20in%20UCT&amp;author=Gelly%2CS.&amp;author=Silver%2CD.&amp;journal=17th%20International%20Conference%20on%20Machine%20Learning&amp;pages=273-280&amp;publication_year=2007" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 16 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">17.</span><p id=ref17 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Krizhevsky, A.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutskever, I.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Hinton, G.</span></span> <cite itemprop="name headline">ImageNet classification with deep convolutional neural networks</cite>. In <i>Advances in Neural Information Processing Systems</i>, 1097–1105 (2012)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks&amp;author=Krizhevsky%2CA.&amp;author=Sutskever%2CI.&amp;author=Hinton%2CG.&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;pages=1097-1105&amp;publication_year=2012" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 17 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">18.</span><p id=ref18 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Lawrence, S.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Giles, C. L.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Tsoi, A. C.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Back, A. D.</span></span> <cite itemprop="name headline">Face recognition: a convolutional neural-network approach</cite>. <i>IEEE Trans. Neural Netw.</i> <b>8</b>, 98–113 (1997)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=18255614&amp;dopt=Abstract" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="View reference 18 on PubMed">PubMed</a><li class="pin-left pl20"><a href=https://doi.org/10.1109/72.554195 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 18">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Face%20recognition%3A%20a%20convolutional%20neural-network%20approach&amp;author=Lawrence%2CS.&amp;author=Giles%2CC.%20L.&amp;author=Tsoi%2CA.%20C.&amp;author=Back%2CA.%20D.&amp;journal=IEEE%20Trans.%20Neural%20Netw.&amp;volume=8&amp;pages=98-113&amp;publication_year=1997" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 18 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">19.</span><p id=ref19 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Mnih, V.</span></span> <i>et al.</i> <cite itemprop="name headline">Human-level control through deep reinforcement learning</cite>. <i>Nature</i> <b>518</b>, 529–533 (2015)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&amp;action=reflink&amp;origin=npg&amp;version=1.0&amp;coi=1:CAS:528:DC%2BC2MXjsVagur0%3D&amp;md5=7c9d3a50f72bb12c7816fd0a4447d594" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="View reference 19 on CAS">CAS</a><li class="pin-left pl20"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=25719670&amp;dopt=Abstract" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="View reference 19 on PubMed">PubMed</a><li class="pin-left pl20"><a href=https://doi.org/10.1038/nature14236 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 19">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Human-level%20control%20through%20deep%20reinforcement%20learning&amp;author=Mnih%2CV.&amp;journal=Nature&amp;volume=518&amp;pages=529-533&amp;publication_year=2015" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 19 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">20.</span><p id=ref20 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>LeCun, Y.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Bengio, Y.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Hinton, G.</span></span> <cite itemprop="name headline">Deep learning</cite>. <i>Nature</i> <b>521</b>, 436–444 (2015)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://chemport.cas.org/cgi-bin/sdcgi?APP=ftslink&amp;action=reflink&amp;origin=npg&amp;version=1.0&amp;coi=1:CAS:528:DC%2BC2MXht1WlurzP&amp;md5=b22da620b3752bec1b7261020a008129" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="View reference 20 on CAS">CAS</a><li class="pin-left pl20"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?holding=npg&amp;cmd=Retrieve&amp;db=PubMed&amp;list_uids=26017442&amp;dopt=Abstract" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="View reference 20 on PubMed">PubMed</a><li class="pin-left pl20"><a href=https://doi.org/10.1038/nature14539 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 20">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning&amp;author=LeCun%2CY.&amp;author=Bengio%2CY.&amp;author=Hinton%2CG.&amp;journal=Nature&amp;volume=521&amp;pages=436-444&amp;publication_year=2015" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 20 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">21.</span><p id=ref21 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Stern, D.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Herbrich, R.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Graepel, T.</span></span> <cite itemprop="name headline">Bayesian pattern ranking for move prediction in the game of Go</cite>. In <i>International Conference of Machine Learning</i>, 873–880 (2006)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20pattern%20ranking%20for%20move%20prediction%20in%20the%20game%20of%20Go&amp;author=Stern%2CD.&amp;author=Herbrich%2CR.&amp;author=Graepel%2CT.&amp;journal=International%20Conference%20of%20Machine%20Learning&amp;pages=873-880&amp;publication_year=2006" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 21 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">22.</span><p id=ref22 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutskever, I.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Nair, V.</span></span> <cite itemprop="name headline">Mimicking Go experts with convolutional neural networks</cite>. In <i>International Conference on Artificial Neural Networks</i>, 101–110 (2008)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Mimicking%20Go%20experts%20with%20convolutional%20neural%20networks&amp;author=Sutskever%2CI.&amp;author=Nair%2CV.&amp;journal=International%20Conference%20on%20Artificial%20Neural%20Networks&amp;pages=101-110&amp;publication_year=2008" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 22 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">23.</span><p id=ref23 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Maddison, C. J.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Huang, A.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutskever, I.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Silver, D.</span></span> <cite itemprop="name headline">Move evaluation in Go using deep convolutional neural networks</cite>. <i>3rd International Conference on Learning Representations</i> (2015)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Move%20evaluation%20in%20Go%20using%20deep%20convolutional%20neural%20networks&amp;author=Maddison%2CC.%20J.&amp;author=Huang%2CA.&amp;author=Sutskever%2CI.&amp;author=Silver%2CD.&amp;journal=3rd%20International%20Conference%20on%20Learning%20Representations&amp;publication_year=2015" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 23 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">24.</span><p id=ref24 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Clark, C.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Storkey, A. J.</span></span> <cite itemprop="name headline">Training deep convolutional neural networks to play go</cite>. In <i>32nd International Conference on Machine Learning</i>, 1766–1774 (2015)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Training%20deep%20convolutional%20neural%20networks%20to%20play%20go&amp;author=Clark%2CC.&amp;author=Storkey%2CA.%20J.&amp;journal=32nd%20International%20Conference%20on%20Machine%20Learning&amp;pages=1766-1774&amp;publication_year=2015" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 24 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">25.</span><p id=ref25 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Williams, R. J.</span></span> <cite itemprop="name headline">Simple statistical gradient-following algorithms for connectionist reinforcement learning</cite>. <i>Mach. Learn.</i> <b>8</b>, 229–256 (1992)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1023/A:1022672621406 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 25">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning&amp;author=Williams%2CR.%20J.&amp;journal=Mach.%20Learn.&amp;volume=8&amp;pages=229-256&amp;publication_year=1992" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 25 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">26.</span><p id=ref26 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutton, R.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>McAllester, D.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Singh, S.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Mansour, Y.</span></span> <cite itemprop="name headline">Policy gradient methods for reinforcement learning with function approximation</cite>. In <i>Advances in Neural Information Processing Systems</i>, 1057–1063 (2000)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation&amp;author=Sutton%2CR.&amp;author=McAllester%2CD.&amp;author=Singh%2CS.&amp;author=Mansour%2CY.&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;pages=1057-1063&amp;publication_year=2000" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 26 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">27.</span><p id=ref27 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutton, R.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Barto, A.</span></span> <i>Reinforcement Learning: an Introduction</i> (MIT Press, 1998)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;author=Sutton%2CR.&amp;author=Barto%2CA.&amp;journal=Reinforcement%20Learning%3A%20an%20Introduction&amp;publication_year=1998" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 27 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">28.</span><p id=ref28 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Schraudolph, N. N.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Dayan, P.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sejnowski, T. J.</span></span> <cite itemprop="name headline">Temporal difference learning of position evaluation in the game of Go</cite>. <i>Adv. Neural Inf. Process. Syst.</i> <b>6</b>, 817–824 (1994)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal%20difference%20learning%20of%20position%20evaluation%20in%20the%20game%20of%20Go&amp;author=Schraudolph%2CN.%20N.&amp;author=Dayan%2CP.&amp;author=Sejnowski%2CT.%20J.&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=6&amp;pages=817-824&amp;publication_year=1994" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 28 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">29.</span><p id=ref29 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Enzenberger, M.</span></span> <cite itemprop="name headline">Evaluation in Go by a neural network using soft segmentation</cite>. In <i>10th Advances in Computer Games Conference</i>, 97–108 (2003). 267</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20in%20Go%20by%20a%20neural%20network%20using%20soft%20segmentation&amp;author=Enzenberger%2CM.&amp;journal=10th%20Advances%20in%20Computer%20Games%20Conference&amp;pages=97-108&amp;publication_year=2003" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 29 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">30.</span><p id=ref30 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Silver, D.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutton, R.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Müller, M.</span></span> <cite itemprop="name headline">Temporal-difference search in computer Go</cite>. <i>Mach. Learn.</i> <b>87</b>, 183–219 (2012)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1007/s10994-012-5280-0 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 30">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal-difference%20search%20in%20computer%20Go&amp;author=Silver%2CD.&amp;author=Sutton%2CR.&amp;author=M%C3%BCller%2CM.&amp;journal=Mach.%20Learn.&amp;volume=87&amp;pages=183-219&amp;publication_year=2012" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 30 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">31.</span><p id=ref31 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Levinovitz, A.</span></span> <cite itemprop="name headline">The mystery of Go, the ancient game that computers still can’t win</cite>. <i>Wired Magazine</i> (2014)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20mystery%20of%20Go%2C%20the%20ancient%20game%20that%20computers%20still%20can%E2%80%99t%20win&amp;author=Levinovitz%2CA.&amp;journal=Wired%20Magazine&amp;publication_year=2014" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 31 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">32.</span><p id=ref32 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Mechner, D.</span></span> <cite itemprop="name headline">All Systems Go</cite>. <i>The Sciences</i> <b>38</b>, 32–37 (1998)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1002/j.2326-1951.1998.tb03356.x data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 32">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=All%20Systems%20Go&amp;author=Mechner%2CD.&amp;journal=The%20Sciences&amp;volume=38&amp;pages=32-37&amp;publication_year=1998" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 32 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">33.</span><p id=ref33 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Mandziuk, J.</span></span> <cite itemprop="name headline">Computational intelligence in mind games</cite>. In <i>Challenges for Computational Intelligence</i>, 407–442 (2007)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20intelligence%20in%20mind%20games&amp;author=Mandziuk%2CJ.&amp;journal=Challenges%20for%20Computational%20Intelligence&amp;pages=407-442&amp;publication_year=2007" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 33 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">34.</span><p id=ref34 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Berliner, H.</span></span> <cite itemprop="name headline">A chronology of computer chess and its literature</cite>. <i>Artif. Intell.</i> <b>10</b>, 201–214 (1978)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/S0004-3702(78)80012-5 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 34">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20chronology%20of%20computer%20chess%20and%20its%20literature&amp;author=Berliner%2CH.&amp;journal=Artif.%20Intell.&amp;volume=10&amp;pages=201-214&amp;publication_year=1978" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 34 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">35.</span><p id=ref35 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Browne, C.</span></span> <i>et al.</i> <cite itemprop="name headline">A survey of Monte-Carlo tree search methods</cite>. <i>IEEE Trans. Comput. Intell. AI in Games</i> <b>4</b>, 1–43 (2012)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1109/TCIAIG.2012.2186810 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 35">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20Monte-Carlo%20tree%20search%20methods&amp;author=Browne%2CC.&amp;journal=IEEE%20Trans.%20Comput.%20Intell.%20AI%20in%20Games&amp;volume=4&amp;pages=1-43&amp;publication_year=2012" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 35 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">36.</span><p id=ref36 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Gelly, S.</span></span> <i>et al.</i> <cite itemprop="name headline">The grand challenge of computer Go: Monte Carlo tree search and extensions</cite>. <i>Commun. ACM</i> <b>55</b>, 106–113 (2012)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1145/2093548.2093574 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 36">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20grand%20challenge%20of%20computer%20Go%3A%20Monte%20Carlo%20tree%20search%20and%20extensions&amp;author=Gelly%2CS.&amp;journal=Commun.%20ACM&amp;volume=55&amp;pages=106-113&amp;publication_year=2012" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 36 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">37.</span><p id=ref37 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Coulom, R.</span></span> <cite itemprop="name headline">Whole-history rating: A Bayesian rating system for players of time-varying strength</cite>. In <i>International Conference on Computers and Games</i>, 113–124 (2008)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Whole-history%20rating%3A%20A%20Bayesian%20rating%20system%20for%20players%20of%20time-varying%20strength&amp;author=Coulom%2CR.&amp;journal=International%20Conference%20on%20Computers%20and%20Games&amp;pages=113-124&amp;publication_year=2008" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 37 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">38.</span><p id=ref38 class=tiny-space-below>KGS. <cite itemprop="name headline">Rating system math</cite>. <a href=http://www.gokgs.com/help/rmath.html itemprop=url>http://www.gokgs.com/help/rmath.html</a></p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Rating%20system%20math" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 38 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">39.</span><p id=ref39 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Littman, M. L.</span></span> <cite itemprop="name headline">Markov games as a framework for multi-agent reinforcement learning</cite>. In <i>11th International Conference on Machine Learning</i>, 157–163 (1994)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning&amp;author=Littman%2CM.%20L.&amp;journal=11th%20International%20Conference%20on%20Machine%20Learning&amp;pages=157-163&amp;publication_year=1994" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 39 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">40.</span><p id=ref40 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Knuth, D. E.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Moore, R. W.</span></span> <cite itemprop="name headline">An analysis of alpha-beta pruning</cite>. <i>Artif. Intell.</i> <b>6</b>, 293–326 (1975)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/0004-3702(75)90019-3 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 40">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=An%20analysis%20of%20alpha-beta%20pruning&amp;author=Knuth%2CD.%20E.&amp;author=Moore%2CR.%20W.&amp;journal=Artif.%20Intell.&amp;volume=6&amp;pages=293-326&amp;publication_year=1975" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 40 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">41.</span><p id=ref41 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sutton, R.</span></span> <cite itemprop="name headline">Learning to predict by the method of temporal differences</cite>. <i>Mach. Learn.</i> <b>3</b>, 9–44 (1988)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1023/A:1022633531479 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 41">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20predict%20by%20the%20method%20of%20temporal%20differences&amp;author=Sutton%2CR.&amp;journal=Mach.%20Learn.&amp;volume=3&amp;pages=9-44&amp;publication_year=1988" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 41 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">42.</span><p id=ref42 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Baxter, J.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Tridgell, A.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Weaver, L.</span></span> <cite itemprop="name headline">Learning to play chess using temporal differences</cite>. <i>Mach. Learn.</i> <b>40</b>, 243–263 (2000)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1023/A:1007634325138 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 42">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20play%20chess%20using%20temporal%20differences&amp;author=Baxter%2CJ.&amp;author=Tridgell%2CA.&amp;author=Weaver%2CL.&amp;journal=Mach.%20Learn.&amp;volume=40&amp;pages=243-263&amp;publication_year=2000" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 42 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">43.</span><p id=ref43 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Veness, J.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Silver, D.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Blair, A.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Uther, W.</span></span> <cite itemprop="name headline">Bootstrapping from game tree search</cite>. In <i>Advances in Neural Information Processing Systems</i> (2009)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Bootstrapping%20from%20game%20tree%20search&amp;author=Veness%2CJ.&amp;author=Silver%2CD.&amp;author=Blair%2CA.&amp;author=Uther%2CW.&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;publication_year=2009" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 43 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">44.</span><p id=ref44 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Samuel, A. L.</span></span> <cite itemprop="name headline">Some studies in machine learning using the game of checkers II - recent progress</cite>. <i>IBM J. Res. Develop.</i> <b>11</b>, 601–617 (1967)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1147/rd.116.0601 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 44">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Some%20studies%20in%20machine%20learning%20using%20the%20game%20of%20checkers%20II%20-%20recent%20progress&amp;author=Samuel%2CA.%20L.&amp;journal=IBM%20J.%20Res.%20Develop.&amp;volume=11&amp;pages=601-617&amp;publication_year=1967" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 44 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">45.</span><p id=ref45 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Schaeffer, J.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Hlynka, M.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Jussila, V.</span></span> <cite itemprop="name headline">Temporal difference learning applied to a high-performance game-playing program</cite>. In <i>17th International Joint Conference on Artificial Intelligence</i>, 529–534 (2001)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal%20difference%20learning%20applied%20to%20a%20high-performance%20game-playing%20program&amp;author=Schaeffer%2CJ.&amp;author=Hlynka%2CM.&amp;author=Jussila%2CV.&amp;journal=17th%20International%20Joint%20Conference%20on%20Artificial%20Intelligence&amp;pages=529-534&amp;publication_year=2001" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 45 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">46.</span><p id=ref46 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Tesauro, G.</span></span> <cite itemprop="name headline">TD-gammon, a self-teaching backgammon program, achieves master-level play</cite>. <i>Neural Comput.</i> <b>6</b>, 215–219 (1994)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1162/neco.1994.6.2.215 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 46">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=TD-gammon%2C%20a%20self-teaching%20backgammon%20program%2C%20achieves%20master-level%20play&amp;author=Tesauro%2CG.&amp;journal=Neural%20Comput.&amp;volume=6&amp;pages=215-219&amp;publication_year=1994" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 46 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">47.</span><p id=ref47 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Dahl, F.</span></span> <cite itemprop="name headline">Honte, a Go-playing program using neural nets</cite>. In <i>Machines that learn to play games</i>, 205–223 (Nova Science, 1999)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Honte%2C%20a%20Go-playing%20program%20using%20neural%20nets&amp;author=Dahl%2CF.&amp;journal=Machines%20that%20learn%20to%20play%20games&amp;pages=205-223&amp;publication_year=1999" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 47 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">48.</span><p id=ref48 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Rosin, C. D.</span></span> <cite itemprop="name headline">Multi-armed bandits with episode context</cite>. <i>Ann. Math. Artif. Intell.</i> <b>61</b>, 203–230 (2011)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1007/s10472-011-9258-6 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 48">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-armed%20bandits%20with%20episode%20context&amp;author=Rosin%2CC.%20D.&amp;journal=Ann.%20Math.%20Artif.%20Intell.&amp;volume=61&amp;pages=203-230&amp;publication_year=2011" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 48 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">49.</span><p id=ref49 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Lanctot, M.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Winands, M. H. M.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Pepels, T.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Sturtevant, N. R.</span></span> <cite itemprop="name headline">Monte Carlo tree search with heuristic evaluations using implicit minimax backups</cite>. In <i>IEEE Conference on Computational Intelligence and Games</i>, 1–8 (2014)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Monte%20Carlo%20tree%20search%20with%20heuristic%20evaluations%20using%20implicit%20minimax%20backups&amp;author=Lanctot%2CM.&amp;author=Winands%2CM.%20H.%20M.&amp;author=Pepels%2CT.&amp;author=Sturtevant%2CN.%20R.&amp;journal=IEEE%20Conference%20on%20Computational%20Intelligence%20and%20Games&amp;pages=1-8&amp;publication_year=2014" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 49 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">50.</span><p id=ref50 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Gelly, S.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Wang, Y.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Munos, R.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Teytaud, O.</span></span> <cite itemprop="name headline">Modification of UCT with patterns in Monte-Carlo Go</cite>. <i>Tech. Rep.</i> <b>6062</b>, INRIA (2006)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Modification%20of%20UCT%20with%20patterns%20in%20Monte-Carlo%20Go&amp;author=Gelly%2CS.&amp;author=Wang%2CY.&amp;author=Munos%2CR.&amp;author=Teytaud%2CO.&amp;journal=Tech.%20Rep.&amp;volume=6062&amp;publication_year=2006" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 50 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">51.</span><p id=ref51 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Silver, D.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Tesauro, G.</span></span> <cite itemprop="name headline">Monte-Carlo simulation balancing</cite>. In <i>26th International Conference on Machine Learning</i>, <b>119</b> (2009)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Monte-Carlo%20simulation%20balancing&amp;author=Silver%2CD.&amp;author=Tesauro%2CG.&amp;journal=26th%20International%20Conference%20on%20Machine%20Learning&amp;volume=119&amp;publication_year=2009" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 51 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">52.</span><p id=ref52 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Huang, S.-C.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Coulom, R.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Lin, S.-S.</span></span> <cite itemprop="name headline">Monte-Carlo simulation balancing in practice</cite>. In <i>7th International Conference on Computers and Games</i>, 81–92 (Springer-Verlag, 2011)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Monte-Carlo%20simulation%20balancing%20in%20practice&amp;author=Huang%2CS.-C.&amp;author=Coulom%2CR.&amp;author=Lin%2CS.-S.&amp;journal=7th%20International%20Conference%20on%20Computers%20and%20Games&amp;pages=81-92&amp;publication_year=2011" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 52 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">53.</span><p id=ref53 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Baier, H.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Drake, P. D.</span></span> <cite itemprop="name headline">The power of forgetting: improving the last-good-reply policy in Monte Carlo Go</cite>. <i>IEEE Trans. Comput. Intell. AI in Games</i> <b>2</b>, 303–309 (2010)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1109/TCIAIG.2010.2100396 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 53">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20power%20of%20forgetting%3A%20improving%20the%20last-good-reply%20policy%20in%20Monte%20Carlo%20Go&amp;author=Baier%2CH.&amp;author=Drake%2CP.%20D.&amp;journal=IEEE%20Trans.%20Comput.%20Intell.%20AI%20in%20Games&amp;volume=2&amp;pages=303-309&amp;publication_year=2010" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 53 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">54.</span><p id=ref54 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Huang, S.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Müller, M.</span></span> <cite itemprop="name headline">Investigating the limits of Monte-Carlo tree search methods in computer Go</cite>. In <i>8th International Conference on Computers and Games</i>, 39–48 (2013)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Investigating%20the%20limits%20of%20Monte-Carlo%20tree%20search%20methods%20in%20computer%20Go&amp;author=Huang%2CS.&amp;author=M%C3%BCller%2CM.&amp;journal=8th%20International%20Conference%20on%20Computers%20and%20Games&amp;pages=39-48&amp;publication_year=2013" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 54 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">55.</span><p id=ref55 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Segal, R. B.</span></span> <cite itemprop="name headline">On the scalability of parallel UCT</cite>. <i>Computers and Games</i> <b>6515</b>, 36–47 (2011)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20scalability%20of%20parallel%20UCT&amp;author=Segal%2CR.%20B.&amp;journal=Computers%20and%20Games&amp;volume=6515&amp;pages=36-47&amp;publication_year=2011" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 55 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">56.</span><p id=ref56 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Enzenberger, M.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Müller, M.</span></span> <cite itemprop="name headline">A lock-free multithreaded Monte-Carlo tree search algorithm</cite>. In <i>12th Advances in Computer Games Conference</i>, 14–20 (2009)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20lock-free%20multithreaded%20Monte-Carlo%20tree%20search%20algorithm&amp;author=Enzenberger%2CM.&amp;author=M%C3%BCller%2CM.&amp;journal=12th%20Advances%20in%20Computer%20Games%20Conference&amp;pages=14-20&amp;publication_year=2009" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 56 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">57.</span><p id=ref57 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Huang, S.-C.</span></span>, <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Coulom, R.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Lin, S.-S.</span></span> <cite itemprop="name headline">Time management for Monte-Carlo tree search applied to the game of Go</cite>. In <i>International Conference on Technologies and Applications of Artificial Intelligence</i>, 462–466 (2010)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Time%20management%20for%20Monte-Carlo%20tree%20search%20applied%20to%20the%20game%20of%20Go&amp;author=Huang%2CS.-C.&amp;author=Coulom%2CR.&amp;author=Lin%2CS.-S.&amp;journal=International%20Conference%20on%20Technologies%20and%20Applications%20of%20Artificial%20Intelligence&amp;pages=462-466&amp;publication_year=2010" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 57 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">58.</span><p id=ref58 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Gelly, S.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Silver, D.</span></span> <cite itemprop="name headline">Monte-Carlo tree search and rapid action value estimation in computer Go</cite>. <i>Artif. Intell.</i> <b>175</b>, 1856–1875 (2011)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=https://doi.org/10.1016/j.artint.2011.03.007 data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link itemprop=url aria-label="View reference 58">Article</a><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Monte-Carlo%20tree%20search%20and%20rapid%20action%20value%20estimation%20in%20computer%20Go&amp;author=Gelly%2CS.&amp;author=Silver%2CD.&amp;journal=Artif.%20Intell.&amp;volume=175&amp;pages=1856-1875&amp;publication_year=2011" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 58 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">59.</span><p id=ref59 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Baudiš, P.</span></span> <cite itemprop="name headline">Balancing MCTS by dynamically adjusting the komi value</cite>. <i>ICGA J.</i> <b>34</b>, 131 (2011)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Balancing%20MCTS%20by%20dynamically%20adjusting%20the%20komi%20value&amp;author=Baudi%C5%A1%2CP.&amp;journal=ICGA%20J.&amp;volume=34&amp;publication_year=2011" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 59 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">60.</span><p id=ref60 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Baier, H.</span></span> &amp; <span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Winands, M. H.</span></span> <cite itemprop="name headline">Active opening book application for Monte-Carlo tree search in 19×19 Go</cite>. In <i>Benelux Conference on Artificial Intelligence</i>, 3–10 (2011)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Active%20opening%20book%20application%20for%20Monte-Carlo%20tree%20search%20in%2019%C3%9719%20Go&amp;author=Baier%2CH.&amp;author=Winands%2CM.%20H.&amp;journal=Benelux%20Conference%20on%20Artificial%20Intelligence&amp;pages=3-10&amp;publication_year=2011" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 60 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/Article><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">61.</span><p id=ref61 class=tiny-space-below><span itemprop=author itemscope itemtype=http://schema.org/Person><span itemprop=name>Dean, J.</span></span> <i>et al.</i> <cite itemprop="name headline">Large scale distributed deep networks</cite>. In <i>Advances in Neural Information Processing Systems</i>, 1223–1231 (2012)</p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Large%20scale%20distributed%20deep%20networks&amp;author=Dean%2CJ.&amp;journal=Advances%20in%20Neural%20Information%20Processing%20Systems&amp;pages=1223-1231&amp;publication_year=2012" data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 61 on Google Scholar">Google Scholar</a></ul></ul><li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" itemprop=citation itemscope itemtype=http://schema.org/CreativeWork><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">62.</span><p id=ref62 class=tiny-space-below>Go ratings. <a href=http://www.goratings.org itemprop=url>http://www.goratings.org</a></p><ul class="js-ref-links clean-list cleared strong sans-serif text13 hide-print small-space-below"><li class=pin-right><ul class="clean-list ma0"><li class="pin-left pl20"><a href=http://scholar.google.com/scholar_lookup? data-track=click data-track-category="article body" data-track-action="outbound reference" data-track-label=link aria-label="Search for reference 62 on Google Scholar">Google Scholar</a></ul></ul></ol><p class="hide-print text-right"><a href=/articles/nature16961-references.ris class="text14 sans-serif strong" data-track=click data-track-category="article body" data-track-action="download citation references" data-track-label=link>Download references</a></p></div></div></div></section><section aria-labelledby=acknowledgements><div class="serif article-section js-article-section cleared clear" id=acknowledgements-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=acknowledgements>Acknowledgements</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=acknowledgements-content><p>We thank Fan Hui for agreeing to play against AlphaGo; T. Manning for refereeing the match; R. Munos and T. Schaul for helpful discussions and advice; A. Cain and M. Cant for work on the visuals; P. Dayan, G. Wayne, D. Kumaran, D. Purves, H. van Hasselt, A. Barreto and G. Ostrovski for reviewing the paper; and the rest of the DeepMind team for their support, ideas and encouragement.</p></div></div></section><section aria-labelledby=author-information><div class="serif article-section js-article-section cleared clear" id=author-information-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=author-information>Author information</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=author-information-content><h2 class="h3 strong mb4 visually-hidden" id=author-notes>Author notes</h2><ol class=clean-list><li class="suppress-bottom-margin small-space-below" id=n1><ul class="clean-list inline-list h3 strong ma0"><li><span class=js-separator></span><span>David Silver</span><li><span class=js-separator>&nbsp;&amp; </span><span>Aja Huang</span></ul> <p>These authors contributed equally to this work.</p></ol><h2 class="h3 strong mb4" id=affiliations>Affiliations</h2><ol class=clean-list><li id=a1><h3 class=emphasis>Google DeepMind, 5 New Street Square, London EC4A 3TW, UK</h3><ul class="small-space-below clean-list inline-list"><li><span class=js-separator></span><span>David Silver</span><li><span class=js-separator>, </span><span>Aja Huang</span><li><span class=js-separator>, </span><span>Chris J. Maddison</span><li><span class=js-separator>, </span><span>Arthur Guez</span><li><span class=js-separator>, </span><span>Laurent Sifre</span><li><span class=js-separator>, </span><span>George van den Driessche</span><li><span class=js-separator>, </span><span>Julian Schrittwieser</span><li><span class=js-separator>, </span><span>Ioannis Antonoglou</span><li><span class=js-separator>, </span><span>Veda Panneershelvam</span><li><span class=js-separator>, </span><span>Marc Lanctot</span><li><span class=js-separator>, </span><span>Sander Dieleman</span><li><span class=js-separator>, </span><span>Dominik Grewe</span><li><span class=js-separator>, </span><span>Nal Kalchbrenner</span><li><span class=js-separator>, </span><span>Timothy Lillicrap</span><li><span class=js-separator>, </span><span>Madeleine Leach</span><li><span class=js-separator>, </span><span>Koray Kavukcuoglu</span><li><span class=js-separator>, </span><span>Thore Graepel</span><li><span class=js-separator>&nbsp;&amp; </span><span>Demis Hassabis</span></ul><li id=a2><h3 class=emphasis>Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA</h3><ul class="small-space-below clean-list inline-list"><li><span class=js-separator></span><span>John Nham</span><li><span class=js-separator>&nbsp;&amp; </span><span>Ilya Sutskever</span></ul></ol><div class=js-hide><h2 class="h3 strong">Authors</h2><ol class=clean-list><li id=auth-1><h3 class="sans-serif strong text-gray-light js-search-name">Search for David Silver in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22David+Silver%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=David+Silver">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22David+Silver%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-2><h3 class="sans-serif strong text-gray-light js-search-name">Search for Aja Huang in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Aja+Huang%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Aja+Huang">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Aja+Huang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-3><h3 class="sans-serif strong text-gray-light js-search-name">Search for Chris J. Maddison in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Chris%20J.+Maddison%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chris%20J.+Maddison">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chris%20J.+Maddison%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-4><h3 class="sans-serif strong text-gray-light js-search-name">Search for Arthur Guez in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Arthur+Guez%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Arthur+Guez">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Arthur+Guez%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-5><h3 class="sans-serif strong text-gray-light js-search-name">Search for Laurent Sifre in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Laurent+Sifre%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Laurent+Sifre">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Laurent+Sifre%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-6><h3 class="sans-serif strong text-gray-light js-search-name">Search for George van den Driessche in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22George+van%20den%20Driessche%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=George+van%20den%20Driessche">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22George+van%20den%20Driessche%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-7><h3 class="sans-serif strong text-gray-light js-search-name">Search for Julian Schrittwieser in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Julian+Schrittwieser%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Julian+Schrittwieser">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Julian+Schrittwieser%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-8><h3 class="sans-serif strong text-gray-light js-search-name">Search for Ioannis Antonoglou in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Ioannis+Antonoglou%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ioannis+Antonoglou">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ioannis+Antonoglou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-9><h3 class="sans-serif strong text-gray-light js-search-name">Search for Veda Panneershelvam in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Veda+Panneershelvam%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Veda+Panneershelvam">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Veda+Panneershelvam%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-10><h3 class="sans-serif strong text-gray-light js-search-name">Search for Marc Lanctot in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Marc+Lanctot%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marc+Lanctot">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marc+Lanctot%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-11><h3 class="sans-serif strong text-gray-light js-search-name">Search for Sander Dieleman in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Sander+Dieleman%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sander+Dieleman">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sander+Dieleman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-12><h3 class="sans-serif strong text-gray-light js-search-name">Search for Dominik Grewe in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Dominik+Grewe%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dominik+Grewe">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dominik+Grewe%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-13><h3 class="sans-serif strong text-gray-light js-search-name">Search for John Nham in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22John+Nham%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John+Nham">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John+Nham%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-14><h3 class="sans-serif strong text-gray-light js-search-name">Search for Nal Kalchbrenner in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Nal+Kalchbrenner%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nal+Kalchbrenner">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nal+Kalchbrenner%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-15><h3 class="sans-serif strong text-gray-light js-search-name">Search for Ilya Sutskever in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Ilya+Sutskever%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ilya+Sutskever">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ilya+Sutskever%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-16><h3 class="sans-serif strong text-gray-light js-search-name">Search for Timothy Lillicrap in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Timothy+Lillicrap%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Timothy+Lillicrap">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Timothy+Lillicrap%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-17><h3 class="sans-serif strong text-gray-light js-search-name">Search for Madeleine Leach in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Madeleine+Leach%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Madeleine+Leach">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Madeleine+Leach%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-18><h3 class="sans-serif strong text-gray-light js-search-name">Search for Koray Kavukcuoglu in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Koray+Kavukcuoglu%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Koray+Kavukcuoglu">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Koray+Kavukcuoglu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-19><h3 class="sans-serif strong text-gray-light js-search-name">Search for Thore Graepel in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Thore+Graepel%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thore+Graepel">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thore+Graepel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul><li id=auth-20><h3 class="sans-serif strong text-gray-light js-search-name">Search for Demis Hassabis in:</h3><ul class="text13 clean-list inline-list strong small-space-below"><li><a href="/search/executeSearch?sp-q=%22Demis+Hassabis%22&amp;sp-p=all&amp;pag-start=1&amp;sp-c=25&amp;sp-m=1&amp;sp-s=date_descending">Nature Research journals</a> <span class=bullet>&bull;</span> <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Demis+Hassabis">PubMed</a> <span class=bullet>&bull;</span> <li><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Demis+Hassabis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">Google Scholar</a></ul></ol></div><h3 class="h3 strong mb4" id=contributions>Contributions</h3><p>A.H., G.v.d.D., J.S., I.A., M.La., A.G., T.G. and D.S. designed and implemented the search in AlphaGo. C.J.M., A.G., L.S., A.H., I.A., V.P., S.D., D.G., N.K., I.S., K.K. and D.S. designed and trained the neural networks in AlphaGo. J.S., J.N., A.H. and D.S. designed and implemented the evaluation framework for AlphaGo. D.S., M.Le., T.L., T.G., K.K. and D.H. managed and advised on the project. D.S., T.G., A.G. and D.H. wrote the paper.</p><h3 class="h3 strong mb4" id=competing-interests>Competing interests</h3><p>The authors declare no competing financial interests.</p><h2 class="h3 strong mb4" id=corresponding-author>Corresponding authors</h2><p>Correspondence to <a href=/articles/nature16961/email/correspondent/c1/new id=corresp-c1 rel=nofollow>David Silver</a> or <a href=/articles/nature16961/email/correspondent/c2/new id=corresp-c2 rel=nofollow>Demis Hassabis</a>.</p></div></div></section><section aria-labelledby=extended-data><div class="serif article-section js-article-section cleared clear" id=extended-data-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=extended-data>Extended data</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=extended-data-content><div class="cleared standard-space-below" data-container-section=extended-data data-component=extended-data><div class="cleared standard-space-below"><h2 class="h3 strong tiny-space-below block icon icon-left pl30 ml20 icon-table-16x16">Extended data tables</h2><ol class="clean-list indented-list ma0"><li class="cleared position-relative standard-space-below small-space-below" id=t1><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">1.</span><h3 class=text17><a href=/articles/nature16961/tables/1 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Details of match between AlphaGo and Fan Hui</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t2><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">2.</span><h3 class=text17><a href=/articles/nature16961/tables/2 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Input features for neural networks</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t3><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">3.</span><h3 class=text17><a href=/articles/nature16961/tables/3 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Supervised learning results for the policy network</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t4><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">4.</span><h3 class=text17><a href=/articles/nature16961/tables/4 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Input features for rollout and tree policy</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t5><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">5.</span><h3 class=text17><a href=/articles/nature16961/tables/5 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Parameters used by AlphaGo</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t6><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">6.</span><h3 class=text17><a href=/articles/nature16961/tables/6 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Results of a tournament between different Go programs</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t7><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">7.</span><h3 class=text17><a href=/articles/nature16961/tables/7 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Results of a tournament between different variants of AlphaGo</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t8><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">8.</span><h3 class=text17><a href=/articles/nature16961/tables/8 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Results of a tournament between AlphaGo and distributed AlphaGo, testing scalability with hardware</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t9><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">9.</span><h3 class=text17><a href=/articles/nature16961/tables/9 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Cross-table of win rates in per cent between programs</a></h3><li class="cleared position-relative standard-space-below small-space-below" id=t10><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">10.</span><h3 class=text17><a href=/articles/nature16961/tables/10 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Cross-table of win rates in per cent between programs in the single-machine scalability study</a></h3><li class="cleared position-relative standard-space-below" id=t11><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">11.</span><h3 class=text17><a href=/articles/nature16961/tables/11 data-track=click data-track-category="article body" data-track-action="download extended data table" data-track-label=link>Cross-table of win rates in per cent between programs in the distributed scalability study</a></h3></ol></div></div></div></div></section><section aria-labelledby=supplementary-information><div class="serif article-section js-article-section cleared clear" id=supplementary-information-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=supplementary-information>Supplementary information</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=supplementary-information-content><div class="cleared standard-space-below" data-container-section=supplementary-information data-component=supplementary-information><h2 class="h3 strong tiny-space-below block icon icon-left pl30 ml20 icon-doctype-zip-16x16-color">Zip files</h2><ol class="clean-list indented-list ma0"><li class="cleared position-relative standard-space-below" id=s1><span class="indented-counter serif h2 tighten-line-height text-right position-absolute">1.</span><h3 class=text17><a href=https://media.nature.com/original/nature-assets/nature/journal/v529/n7587/extref/nature16961-s1.zip data-track=click data-track-category="article body" data-track-action="view supplementary info" data-track-label=link>Supplementary Information</a></h3><div class=cleared data-component=thumbnail-container><p class="text14 ma0">This zipped file contains game records for the 5 formal match games played between AlphaGo and Fan Hui.</p></div></ol></div></div></div></section><section aria-labelledby=rightslink><div class="serif article-section js-article-section cleared clear" id=rightslink-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=rightslink>Rights and permissions</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=rightslink-content><p>To obtain permission to re-use content from this article visit <a href="https://s100.copyright.com/AppDispatchServlet?author=David%20Silver%2C%20Aja%20Huang%2C%20Chris%20J.%20Maddison%2C%20Arthur%20Guez%2C%20Laurent%20Sifre%20et%20al.&amp;contentID=10.1038%2Fnature16961&amp;imprint=Nature&amp;issueNum=7587&amp;orderBeanReset=true&amp;publication=Nature&amp;publicationDate=2016-01-27&amp;publisherName=SpringerNature&amp;title=Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search&amp;volumeNum=529" data-track=click data-track-category="article body" data-track-action="view rights and permissions" data-track-label=link>RightsLink</a>.</p></div></div></section><section aria-labelledby=article-info><div class="serif article-section js-article-section cleared clear" id=article-info-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=article-info>About this article</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=article-info-content><h3 class="h3 strong small-space-below">Publication history</h3><div class="grid grid-12"><div class="grid grid-3 mq875-grid-12 sans-serif text14"><h4>Received</h4><p class=standard-space-below><time datetime=2015-11-11>11 November 2015</time></p></div><div class="grid grid-3 mq875-grid-12 sans-serif text14"><h4>Accepted</h4><p class=standard-space-below><time datetime=2016-01-05>05 January 2016</time></p></div><div class="grid grid-3 mq875-grid-12 sans-serif text14 last"><h4>Published</h4><p class=standard-space-below><time datetime=2016-01-27>27 January 2016</time></p></div></div><h3 class="h3 strong mb4"><abbr title="Digital Object Identifier">DOI</abbr></h3><p class="standard-space-below text14"><a href=https://doi.org/10.1038/nature16961 data-track=click data-track-action="view doi" data-track-category="article body" data-track-label=link>https://doi.org/10.1038/nature16961</a></p><div data-component=share-box class=ma0></div><div data-component=article-info-list class="ma0 text14 sans-serif"></div></div></div></section>

                        
    <section aria-labelledby="further-reading">
      <div class="serif article-section js-article-section cleared clear" id="further-reading-section">
        <h2 class="js-section-title section-title strong position-relative tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below small-space-above mq640-pt10 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left"
            id="further-reading">
          <span>Further reading</span>
        </h2>
        <div class="pl20 mq875-pl0 js-collapsible-section" id="further-reading-content">
          <div data-container-section="further-reading">
            <ul class="clean-list ma0 standard-space-below" id="further-reading-list">
              
                <li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" id="further-reading-item-1">
                  <h3 class="text17">
                    <a class="print-link"
                       data-track="click"
                       data-track-action="view further reading article"
                       data-track-category="article body"
                       data-track-label="link:Novel fault diagnosis scheme utilizing deep learning networks"
                       href="https://doi.org/10.1016/j.pnucene.2019.103066"
                       id="title-link-1">
                      Novel fault diagnosis scheme utilizing deep learning networks
                    </a>
                  </h3>
                  
                    <ul data-test="author-list"
                        class="ma0 mt6 pa0 clean-list add-top-margin-small inline-list text13 text-gray-light tighten-line-height js-list-authors-3 js-etal-collapsed"
                        id="authors-1">
                      <li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator"></span><span
                            itemprop="name">Hanan A. Saeed</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Min-jun Peng</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Hang Wang</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">&nbsp;&amp;&nbsp;</span><span
                            itemprop="name">Bo-wen Zhang</span></li></ul>
                  
                  <p class="text13 text-gray-light standard-space-below" id="journal-name-1">
                    <i>Progress in Nuclear Energy</i>
                    (2020)
                  </p>
                </li>
              
                <li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" id="further-reading-item-2">
                  <h3 class="text17">
                    <a class="print-link"
                       data-track="click"
                       data-track-action="view further reading article"
                       data-track-category="article body"
                       data-track-label="link:Learning Robust LQ-Controllers Using Application Oriented Exploration"
                       href="https://doi.org/10.1109/LCSYS.2019.2921512"
                       id="title-link-2">
                      Learning Robust LQ-Controllers Using Application Oriented Exploration
                    </a>
                  </h3>
                  
                    <ul data-test="author-list"
                        class="ma0 mt6 pa0 clean-list add-top-margin-small inline-list text13 text-gray-light tighten-line-height js-list-authors-3 js-etal-collapsed"
                        id="authors-2">
                      <li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator"></span><span
                            itemprop="name">Mina Ferizbegovic</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Jack Umenberger</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Hakan Hjalmarsson</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">&nbsp;&amp;&nbsp;</span><span
                            itemprop="name">Thomas B. Schon</span></li></ul>
                  
                  <p class="text13 text-gray-light standard-space-below" id="journal-name-2">
                    <i>IEEE Control Systems Letters</i>
                    (2020)
                  </p>
                </li>
              
                <li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" id="further-reading-item-3">
                  <h3 class="text17">
                    <a class="print-link"
                       data-track="click"
                       data-track-action="view further reading article"
                       data-track-category="article body"
                       data-track-label="link:Multifidelity Information Fusion with Machine Learning: A Case Study of Dopant Formation Energies in Hafnia"
                       href="https://doi.org/10.1021/acsami.9b02174"
                       id="title-link-3">
                      Multifidelity Information Fusion with Machine Learning: A Case Study of Dopant Formation Energies in Hafnia
                    </a>
                  </h3>
                  
                    <ul data-test="author-list"
                        class="ma0 mt6 pa0 clean-list add-top-margin-small inline-list text13 text-gray-light tighten-line-height js-list-authors-3 js-etal-collapsed"
                        id="authors-3">
                      <li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator"></span><span
                            itemprop="name">Rohit Batra</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Ghanshyam Pilania</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Blas P. Uberuaga</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">&nbsp;&amp;&nbsp;</span><span
                            itemprop="name">Rampi Ramprasad</span></li></ul>
                  
                  <p class="text13 text-gray-light standard-space-below" id="journal-name-3">
                    <i>ACS Applied Materials &amp; Interfaces</i>
                    (2019)
                  </p>
                </li>
              
                <li class="small-space-below border-gray-medium border-bottom-1 position-relative js-ref-item" id="further-reading-item-4">
                  <h3 class="text17">
                    <a class="print-link"
                       data-track="click"
                       data-track-action="view further reading article"
                       data-track-category="article body"
                       data-track-label="link:Implementation of End-to-End Training of Deep Visuomotor Policies for Manipulation of a Robotic Arm of Baxter Research Robot"
                       href="https://doi.org/10.7746/jkros.2019.14.1.040"
                       id="title-link-4">
                      Implementation of End-to-End Training of Deep Visuomotor Policies for Manipulation of a Robotic Arm of Baxter Research Robot
                    </a>
                  </h3>
                  
                    <ul data-test="author-list"
                        class="ma0 mt6 pa0 clean-list add-top-margin-small inline-list text13 text-gray-light tighten-line-height js-list-authors-3 js-etal-collapsed"
                        id="authors-4">
                      <li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator"></span><span
                            itemprop="name">Seongun Kim</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Sol A Kim</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Rafael de Lima</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">&nbsp;&amp;&nbsp;</span><span
                            itemprop="name">Jaesik Choi</span></li></ul>
                  
                  <p class="text13 text-gray-light standard-space-below" id="journal-name-4">
                    <i>Journal of Korea Robotics Society</i>
                    (2019)
                  </p>
                </li>
              
                <li class="small-space-below position-relative js-ref-item" id="further-reading-item-5">
                  <h3 class="text17">
                    <a class="print-link"
                       data-track="click"
                       data-track-action="view further reading article"
                       data-track-category="article body"
                       data-track-label="link:Collaboration and Delegation between Humans and AI: An Experimental Investigation of the Future of Work"
                       href="https://doi.org/10.2139/ssrn.3368813"
                       id="title-link-5">
                      Collaboration and Delegation between Humans and AI: An Experimental Investigation of the Future of Work
                    </a>
                  </h3>
                  
                    <ul data-test="author-list"
                        class="ma0 mt6 pa0 clean-list add-top-margin-small inline-list text13 text-gray-light tighten-line-height js-list-authors-3 js-etal-collapsed"
                        id="authors-5">
                      <li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator"></span><span
                            itemprop="name">Andreas F&amp;uuml;gener</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">J&amp;ouml;rn Grahl</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">, </span><span
                            itemprop="name">Alok Gupta</span></li><li itemprop="creator" itemscope="" itemtype="http://schema.org/Person"><span
                            class="js-separator">&nbsp;&amp;&nbsp;</span><span
                            itemprop="name">Wolfgang Ketter</span></li></ul>
                  
                  <p class="text13 text-gray-light standard-space-below" id="journal-name-5">
                    <i>SSRN Electronic Journal </i>
                    (2019)
                  </p>
                </li>
              
            </ul>
          </div>
        </div>
      </div>
    </section>
  

                        
                            <section aria-labelledby=article-comments><div class="serif article-section js-article-section cleared clear" id=article-comments-section><h2 class="js-section-title section-title position-relative strong tighten-line-height background-gray-light pt20 pb6 pl0 pr20 standard-space-below mq640-pt20 mq640-pb10 mq640-pl20 mq640-mt0 mq640-ml-20 mq640-mr-20 extend-left small-space-above" id=article-comments>Comments</h2><div class="pl20 mq875-pl0 js-collapsible-section" id=article-comments-content><p>By submitting a comment you agree to abide by our <a href=/info/tandc.html>Terms</a> and <a href=/info/community-guidelines.html>Community Guidelines</a>. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.</p></div></div></section>
                            <div id="inject-comments">
                                <div class="placeholder" data-replace="true"
                                     data-placeholder="/platform/disqus?doi=10.1038/nature16961 #article-comments-container">
                                </div>
                            </div>
                        

                        <span data-recommended="jobs"></span>
                    </div>
                </div>

                <div class="hide-print reading-companion js-reading-companion position-relative cleared pin-left" role="complementary">
                    <div data-container-type="reading-companion" data-track-component="reading companion">
                        
                            
    
    <h2 class="h3 sans-serif visually-hidden strong">Article Tools</h2>
    <div class="tighten-line-height cleared text14 position-relative z-index-1 standard-space-below"
         data-article-tools="true" data-container-section="tools" data-test="components">
        
            <a id="pdf-download-button" class="block block-link pa15 pl0 icon-pdf-download nature-pdf-logo" href="/articles/nature16961.pdf" data-track-action="download pdf" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-category="reading companion" data-track-label="link">
                <span class="inline-block pl20 pt5 pb5 icon icon-right">Download PDF</span>
            </a>
        
    </div>
    

                        

                        
    
        <div class="flex-box grid-12 mq875-kill-flex cleared standard-space-below" data-container-section="info">
            
                <div class="flex-box grid grid-3 mq875-grid-12">
                    <div class="flex-box-item border-gray-medium extra-tight-line-height border-left-1 text13 mq875-pb10" data-component="article-history-list">
                        <p data-test="citation-count" class="pa10 pt0 pb0 mb0 pin-left"><strong>1862 </strong><span class="mq875-inline-block  text-gray-light">Citations</span></p>
                    </div>
                </div>
            

            
                
                    <div class="flex-box grid grid-3 mq875-grid-12">
                        <div class="flex-box-item border-gray-medium extra-tight-line-height border-left-1 text13 mq875-pb10" data-component="article-history-list">
                            <p data-test="altmetric-score" class="pa10 pt0 pb0 mb0 pin-left"><strong>3134 </strong><span class="mq875-inline-block text-gray-light">Altmetric</span></p>
                        </div>
                    </div>
                

                <div class="flex-box grid grid-6 mq875-grid-12 last">
                    <div class="flex-box-item border-gray-medium extra-tight-line-height border-left-1 text13" data-component="article-history-list">
                        
                            <p class="pa10 mb0 pt0 pb0 pin-left text13 mb0"><span class="mq875-hide"><br></span>
                                <span class="block"><a aria-label="More detail about citations and metrics for this article" data-test="metrics-link" href="/articles/nature16961/metrics"
                                                       class="icon icon-right icon-double-chevron-right-10x10-blue pr15"
                                                       data-track="click" data-track-action="view metrics" data-track-category="article body" data-track-label="link" rel="nofollow">Article metrics</a></span>
                            </p>
                        
                    </div>
                </div>
            
        </div>
    

                        
                        
                            <aside><div class="grid grid-12 last mb40 clear cleared"><div class="hide-overflow cleared serif"><section><h1 class="h3 tighten-line-height small-space-below">Editorial Summary</h1><h2 class="text14 strong tighten-line-height tiny-space-below">AlphaGo computer beats Go champion</h2><div class="suppress-bottom-margin add-top-margin-small hide-overflow text14" data-hellip data-show-more data-show-more-dest="editorial summary"><p>The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained — the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5–0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.</p></div></section></div></div></aside>
                        

                        
        
            <aside>
                <div class="grid grid-12 last mb10 clear cleared"><h1 class="h3 tighten-line-height small-space-below">
                    Associated Content</h1>
                    
                        <div class="standard-space-below">
                            <div class="hide-overflow cleared serif">

                                <section><p class="mb4 text13 tighten-line-height text-gray-light">Collection</p>
                                    <h3 class="mb10 extra-tight-line-height" itemprop="name headline"><a
                                        href="/collections/csgqqsrfxh"
                                        data-track="click"
                                        data-track-action="view collection"
                                        data-track-label="link">
                                         The multidisciplinary nature of machine intelligence
                                    </a></h3>
                                </section>
                            </div>
                        </div>
                    
                    
                    
                </div>
            </aside>
        

    

                        <div class="clear cleared" data-component="reading-companion-placeholder">
                            <div class="animate-width" data-component="reading-companion-sticky">
                                <div class="grid grid-12 last position-relative z-index-0 clear-float cleared clear" data-component="reading-companion-adverts">
                                    
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-gpt-unitpath="/285/nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=nature16961;doi=10.1038/nature16961;subjmeta=1042,117,1788,378,631,639,705;kwrd=Computational science,Computer science,Reward">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/nature.com/article&amp;sz=300x250&amp;c=935271709&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnature16961%26doi%3D10.1038/nature16961%26subjmeta%3D1042,117,1788,378,631,639,705%26kwrd%3DComputational science,Computer science,Reward">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/nature.com/article&amp;sz=300x250&amp;c=935271709&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnature16961%26doi%3D10.1038/nature16961%26subjmeta%3D1042,117,1788,378,631,639,705%26kwrd%3DComputational science,Computer science,Reward"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </article>
        </div>
    </div>
</div>


<div class="hide-print js-header-menu menu mb20 z-index-50 composite-layer background-white border-bottom-2 border-gray-medium tighten-line-height" id="menu" data-track-component="menu">
    <div class="menu-inner content js-hide">
        <nav>
            <div class="cleared border-bottom-1 border-gray-medium ml20 mr20">
                <div class="grid-ng grid-1of4 mq875-grid-12">
                    <h3 class="h3 pa20 mq875-pa0 mq875-mt20">
                        Nature<span class="visually-hidden"> menu</span>
                    </h3>
                </div>
                
    
    <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
        <ul class="clean-list mb0">
            
            <li class="mb4 pr10"><a href="/nature/research" data-track="click" data-track-action="research" data-track-label="link">Research</a></li>
            
            <li class="mb4 pr10"><a href="/news" data-track="click" data-track-action="news" data-track-label="link">News</a></li>
            
            <li class="mb4 pr10"><a href="/opinion" data-track="click" data-track-action="opinion" data-track-label="link">Opinion</a></li>
            
            <li class="mb4 pr10"><a href="/research-analysis" data-track="click" data-track-action="research analysis" data-track-label="link">Research Analysis</a></li>
            
            <li class="mb4 pr10"><a href="/careers" data-track="click" data-track-action="careers" data-track-label="link">Careers</a></li>
            
            <li class="mb4 pr10"><a href="/books-culture" data-track="click" data-track-action="books and culture" data-track-label="link">Books and Culture</a></li>
            
            <li class="mb4 pr10"><a href="/nature/podcast" data-track="click" data-track-action="podcasts" data-track-label="link">Podcasts</a></li>
            
            <li class="mb4 pr10"><a href="/nature/videoarchive" data-track="click" data-track-action="videos" data-track-label="link">Videos</a></li>
            
        </ul>
    </div>
    

    
    <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
        <ul class="clean-list mb0">
            
            <li class="mb4 pr10"><a href="/nature/current-issue" data-track="click" data-track-action="current issue" data-track-label="link">Current Issue</a></li>
            
            <li class="mb4 pr10"><a href="/nature/browse-issues" data-track="click" data-track-action="browse issues" data-track-label="link">Browse Issues</a></li>
            
            <li class="mb4 pr10"><a href="/nature/articles" data-track="click" data-track-action="browse articles" data-track-label="link">Browse Articles</a></li>
            
            <li class="mb4 pr10"><a href="/nature/collections" data-track="click" data-track-action="browse collections" data-track-label="link">Browse Collections</a></li>
            
            <li class="mb4 pr10"><a href="/nature/browse-subjects" data-track="click" data-track-action="browse subjects" data-track-label="link">Browse Subjects</a></li>
            
        </ul>
    </div>
    

    
    <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
        <ul class="clean-list mb0">
            
            <li class="mb4 pr10"><a href="/nature/about" data-track="click" data-track-action="about the journal" data-track-label="link">About the Journal</a></li>
            
            <li class="mb4 pr10"><a href="/nature/for-authors" data-track="click" data-track-action="for authors" data-track-label="link">For Authors</a></li>
            
            <li class="mb4 pr10"><a href="/nature/for-referees" data-track="click" data-track-action="for referees" data-track-label="link">For Referees</a></li>
            
            <li class="mb4 pr10"><a href="/nature/awards" data-track="click" data-track-action="awards" data-track-label="link">Awards</a></li>
            
            <li class="mb4 pr10"><a href="/nature/subscribe" data-track="click" data-track-action="subscribe" data-track-label="link">Subscribe</a></li>
            
            <li class="mb4 pr10"><a href="/nature/e-alert" data-track="click" data-track-action="e-alert" data-track-label="link">E-alert</a></li>
            
            <li class="mb4 pr10"><a href="/nature/submit-online" data-track="click" data-track-action="submit" data-track-label="link">Submit</a></li>
            
        </ul>
    </div>
    

            </div>
            <div class="cleared ml20 mr20">
                <div class="cleared pb10">
    <div class="grid-ng grid-1of4 mq875-grid-12">
        <h3 class="serif pa20 mq875-pa0 mq875-mt20">Nature Research<span class="visually-hidden"> menu</span></h3>
    </div>
    <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
        <h4 class="mb4">Our Journals</h4>
        <ul class="clean-list mb0">
            <li class="mb4 pr10"><a href="/nature" data-track="click" data-track-action="nature" data-track-label="link">Nature</a></li>
            <li class="mb4 pr10"><a href="/ncomms" data-track="click" data-track-action="nature communications" data-track-label="link">Nature Communications</a></li>
            <li class="mb4 pr10"><a href="/nprot" data-track="click" data-track-action="nature protocols" data-track-label="link">Nature Protocols</a></li>
            <li class="mb4 pr10"><a href="/srep" data-track="click" data-track-action="scientific reports" data-track-label="link">Scientific Reports</a></li>
            <li class="mb4 pr10"><a href="/siteindex" data-track="click" data-track-action="journal a-z view all" data-track-label="link">View all journals</a></li>
        </ul>
    </div>
    <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
        <h4 class="mb4">Subjects</h4>
        <ul class="clean-list mb0">
            <li class="mb4 pr10"><a href="/subjects/biological-sciences" data-track="click" data-track-action="biological sciences" data-track-label="link">Biological Sciences</a></li>
            <li class="mb4 pr10"><a href="/subjects/scientific-community-and-society" data-track="click" data-track-action="scientific community and society" data-track-label="link">Scientific Community &amp; Society</a></li>
            <li class="mb4 pr10"><a href="/subjects/earth-and-environmental-sciences" data-track="click" data-track-action="earth and environmental sciences" data-track-label="link">Earth &amp; Environmental Sciences</a></li>
            <li class="mb4 pr10"><a href="/subjects/health-sciences" data-track="click" data-track-action="health sciences" data-track-label="link">Health Sciences</a></li>
            <li class="mb4 pr10"><a href="/subjects/physical-sciences" data-track="click" data-track-action="physical sciences" data-track-label="link">Physical Sciences</a></li>
            <li class="mb4 pr10"><a href="/subjects" data-track="click" data-track-action="subjects view all" data-track-label="link">View all subjects</a></li>
        </ul>
    </div>
    <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
        <h4 class="mb4">More</h4>
        <ul class="clean-list mb0">
            <li class="mb4 pr10"><a href="https://support.nature.com/support/home" data-track="click" data-track-action="subscriptions" data-track-label="link">Contact us</a></li>
            <li class="mb4 pr10"><a href="/authors/index.html" data-track="click" data-track-action="authors and referees" data-track-label="link">Authors &amp; Referees</a></li>
            <li class="mb4 pr10"><a href="/libraries/index.html" data-track="click" data-track-action="librarians" data-track-label="link">Librarians</a></li>
            <li class="mb4 pr10"><a href="/advertising/index.html" data-track="click" data-track-action="advertisers" data-track-label="link">Advertisers</a></li>
            <li class="mb4 pr10"><a href="/npg_/press_room/index.html" data-track="click" data-track-action="press" data-track-label="link">Press</a></li>
            <li class="mb4 pr10"><a href="/npg_/index_npg.html" data-track="click" data-track-action="about" data-track-label="link">About Nature Research</a></li>
        </ul>
    </div>
</div>

            </div>
        </nav>
    </div>
</div>


<div id="search-menu" class="menu pt20 mb20 z-index-50 composite-layer background-white border-gray-medium border-bottom-2" data-component="tray" data-track-component="header">
    <div class="menu-inner js-hide">
        <section>
            <h2 class="visually-hidden">Search</h2>
            <div class="hide-print content mb40 mq1200-padded position-relative" data-test="inline-search">
                <div class="pt30 cleared background-white sans-serif">
                    <div class="grid grid-8 grid-left-2 mq875-grid-12 mq875-ml0 mq640-pb30">
                        <form action="/search" method="get" role="search" autocomplete="off" class="standard-space-below" data-track="submit" data-track-action="search" data-track-label="form">
                            <label for="keywords" class="block strong">Article search</label>
                            <div class="position-relative">
                                <input type="search" id="keywords" class="border-gray border-all-1 equalize-line-height pa10 pr40 box-sizing grid-12" name="q" value="" placeholder="Search by keywords or author" data-test="search-keywords">
                                <div class="position-absolute position-right position-top mt1 mr1">
                                    <button type="submit" class="icon icon-center search-btn kill-border" data-test="search-submit">
                                        Search
                                    </button>
                                </div>
                                <p class="mb0 mt4 text14"><a href="/search/advanced" data-track="click" data-track-action="advanced search" data-track-label="link">Advanced search</a></p>
                            </div>
                        </form>
                        <h3 class="h3 strong mb4 border-gray-medium border-bottom-1 sans-serif">Quick links</h3>
                        <ul class="clean-list mb0 text14">
                            <li class="grid-ng grid-1of2 mb6 mt6"><a href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
                            <li class="grid-ng grid-1of2 mb6 mt6"><a href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
                            <li class="grid-ng grid-1of2"><a href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
                            <li class="grid-ng grid-1of2"><a href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>
    </div>
</div>


<footer>

    <div class="content pl30 pr30 text14 text-gray-light">
    
    </div>
    <div class="container-footer mt40 cleared container-footer-full" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        
          <div class="content pt20 pr30 pl30">
            <div class="grid grid-12 last clear-float cleared mb15 text-gray-light text13 flex-box flex-wrap-reverse">
                <h2 aria-level="2" class="text13 strong emphasis ma0 sans-serif" itemprop="name">Nature</h2>
                <p class="pt0 pr10 pb0 pl10 ma0"><abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="issn">1476-4687</span> (online)</p>
            </div>
          </div>
        

        <div class="grid grid-12 pl30 pr30 last" id="footer">
        <h2 aria-level="2" class="hide">nature.com sitemap</h2>
    </div>

    <div class="pl30 pr30 content" data-track-component="footer">

        <div class="grid grid-12 last clear-float cleared mb5 pt20 text-gray-light flex-wrap">
            <div class="grid grid-3 mq875-grid-12 just-mq640-last just-mq875-last mb20"><img alt="Nature" src="/static/images/natureresearch-logo.2fe351a520.svg" class="js-svg" data-png="/static/images/natureresearch-logo.1f5acc607c.png"></div>
            <ul class="grid grid-7 mq875-grid-8 mq640-grid-12 just-mq640-last clean-list mb10 mt10 text15 u-hide-print">
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12"><a href="https://www.nature.com/npg_/company_info/index.html" data-track="click" data-track-action="about us" class="text-gray-light" data-track-label="link">About us</a></li>
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12 just-mq640-last just-mq480-last"><a href="https://www.nature.com/npg_/press_room/press_releases.html" data-track="click" data-track-action="press releases" data-track-label="link" class="text-gray-light">Press releases</a></li>
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12"><a href="https://press.nature.com/" data-track="click" data-track-action="press office" data-track-label="link" class="text-gray-light">Press office</a></li>
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12 last"><a href="https://support.nature.com/support/home" data-track="click" data-track-action="contact us" class="text-gray-light" data-track-label="link">Contact us</a></li>
            </ul>
            <ul class="grid grid-2 mq875-grid-4 mq640-grid-12 last clean-list mb10 mt4 u-hide-print">
                <li class="grid grid-4 mq875-grid-3"><a href="https://www.facebook.com/nature/" data-track="click" data-track-action="facebook" data-track-label="link" class="text-gray-light"><img src="/static/images/fb.49ba391f15.svg" class="cleared text-footer-img" alt="Facebook"/></a></li>
                <li class="grid grid-4 mq875-grid-3"><a href="https://twitter.com/nresearchnews?lang=en" data-track="click" data-track-action="twitter" data-track-label="link" class="text-gray-light"><img src="/static/images/twitter.a90dbeb03a.svg" class="cleared text-footer-img" alt="Twitter"/></a></li>
                <li class="grid grid-4 mq875-grid-3 last"><a href="https://www.youtube.com/channel/UCvCLdSgYdSTpWcOgEJgi-ng" data-track="click" data-track-action="youtube" data-track-label="link" class="text-gray-light"><img src="/static/images/youtube.6071f6012c.svg" class="cleared text-footer-img" alt="Youtube"/></a></li>
            </ul>
        </div>

        <div class="cleared z-index-1 pt30 pb20 border-top-1 border-gray text-footer-mobile-bottom hide-print u-hide-print">
            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12">
                <h3 class="text-footer-heading">Discover content</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.nature.com/siteindex/" data-track="click" data-track-action="journals a-z" data-track-label="link" class="text-gray-light">Journals A-Z</a></li>
                    <li class="pb4"><a href="https://www.nature.com/subjects/" data-track="click" data-track-action="article by subject" data-track-label="link" class="text-gray-light">Articles by subject</a></li>
                    <li class="pb4"><a href="https://nano.nature.com/" data-track="click" data-track-action="nano" data-track-label="link" class="text-gray-light">Nano</a></li>
                    <li class="pb4"><a href="https://www.nature.com/protocolexchange/" data-track="click" data-track-action="protocol exchange" data-track-label="link" class="text-gray-light">Protocol Exchange</a></li>
                    <li class="pb4"><a href="https://www.natureindex.com/" data-track="click" data-track-action="nature index" data-track-label="link" class="text-gray-light">Nature Index</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq640-last">
                <h3 class="text-footer-heading">Publish with us</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.nature.com/authors/author_resources/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link" class="text-gray-light" >Guide to Authors</a></li>
                    <li class="pb4"><a href="https://www.nature.com/authors/peer_review/" data-track="click" data-track-action="guide to referees" data-track-label="link" class="text-gray-light">Guide to Referees</a></li>
                    <li class="pb4"><a href="https://www.nature.com/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link" class="text-gray-light">Editorial policies</a></li>
                    <li class="pb4"><a href="http://www.nature.com/openresearch/publishing-with-npg/" data-track="click" data-track-action="open access" data-track-label="link" class="text-gray-light">Open access</a></li>
                    <li class="pb4" ><a href="https://www.nature.com/reprints/" data-track="click" data-track-action="reprints and permissions" data-track-label="link" class="text-gray-light">Reprints &amp; permissions</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq875-last">
                <h3 class="text-footer-heading">Researcher services</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.springernature.com/gp/authors/research-data" data-track="click" data-track-action="data research service" data-track-label="link" class="text-gray-light">Research data</a></li>
                    <li class="pb4"><a href="https://authorservices.springernature.com/go/nr" data-track="click" data-track-action="language editing" data-track-label="link" class="text-gray-light">Language editing</a></li>
                    <li class="pb4"><a href="https://authorservices.springernature.com/scientific-editing/" data-track="click" data-track-action="scientific editing" data-track-label="link" class="text-gray-light">Scientific editing</a></li>
                    <li class="pb4"><a href="https://masterclasses.nature.com/" data-track="click" data-track-action="nature masterclasses" data-track-label="link" class="text-gray-light">Nature Masterclasses</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/product/nature-research-academies/" data-track="click" data-track-action="nature research academies" data-track-label="link" class="text-gray-light">Nature Research Academies</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq875-clear just-mq640-last full-size-last just-mq1200-last just-mq875-last just-mq480-last">
                <h3 class="text-footer-heading">Libraries &amp; institutions</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.springernature.com/gp/librarians/tools-services" data-track="click" data-track-action="librarian service and tools" data-track-label="link" class="text-gray-light">Librarian service &amp; tools</a></li>
                    <li class="pb4"><a href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal" data-track="click" data-track-action="librarian portal" data-track-label="link" class="text-gray-light">Librarian portal</a></li>
                    <li class="pb4"><a href="http://www.nature.com/openresearch/about-open-access/information-for-institutions/" data-track="click" data-track-action="open research" data-track-label="link" class="text-gray-light">Open research</a></li>
                </ul>
            </div>
        </div>

        <div class="cleared z-index-1 pt30 pb20 border-top-1 border-gray text-footer-mobile-top hide-print u-hide-print">
            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12">
                <h3 class="text-footer-heading">Advertising &amp; partnerships</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://partnerships.nature.com/" data-track="click" data-track-action="advertising" data-track-label="link" class="text-gray-light">Advertising</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/home/services/" data-track="click" data-track-action="partnerships and services" data-track-label="link" class="text-gray-light">Partnerships &amp; Services</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/home/resources/" data-track="click" data-track-action="media kits" data-track-label="link" class="text-gray-light">Media kits</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/product/branded-content-native-advertising/" data-track-action="branded content" data-track-label="link" class="text-gray-light">Branded content</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq640-last just-mq875-last">
                <h3 class="text-footer-heading">Career development</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.nature.com/naturecareers" data-track="click" data-track-action="nature careers" data-track-label="link" class="text-gray-light">Nature Careers</a></li>
                    <li class="pb4"><a href="https://www.nature.com/natureconferences/" data-track="click" data-track-action="nature conferences" data-track-label="link" class="text-gray-light">Nature<span class="visually-hidden"> </span> Conferences</a></li>
                    <li class="pb4"><a href="https://www.nature.com/natureevents/" data-track="click" data-track-action="nature events" data-track-label="link" class="text-gray-light">Nature<span class="visually-hidden"> </span> events</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12">
                <h3 class="text-footer-heading">Regional websites</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="http://www.naturechina.com" data-track="click" data-track-action="nature china" data-track-label="link" class="text-gray-light">Nature China</a></li>
                    <li class="pb4"><a href="https://www.nature.com/nindia" data-track="click" data-track-action="nature india" data-track-label="link" class="text-gray-light">Nature India</a></li>
                    <li class="pb4"><a href="https://www.natureasia.com/ja-jp/" data-track="click" data-track-action="nature japan" data-track-label="link" class="text-gray-light">Nature Japan</a></li>
                    <li class="pb4"><a href="https://www.natureasia.com/ko-kr/" data-track="click" data-track-action="nature korea" data-track-label="link" class="text-gray-light">Nature Korea</a></li>
                    <li class="pb4"><a href="https://www.nature.com/nmiddleeast/" data-track="click" data-track-action="nature middle east" data-track-label="link" class="text-gray-light">Nature Middle East</a></li>
                </ul>
            </div>
        </div>
    </div>

    <div class="container-footer-box cleared text14 border-gray-medium border-top-1">
        <div class="content">
            <img src="/static/images/sn-logo.4368f3b177.svg" alt="Springer Nature" class="cleared"/>
            <p class="mb10 text-gray-light">© 2019 Springer Nature Publishing AG</p>
            <ul class="ma0 clean-list grid grid-12 hide-print u-hide-print">
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq480-last"><a href="https://www.nature.com/info/privacy.html" data-track="click" data-track-action="privacy policy" data-track-label="link" class="text-gray-light">Privacy Policy</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq640-last just-mq480-last"><a href="https://www.nature.com/info/cookies.html" data-track="click" data-track-action="use of cookies" data-track-label="link" class="text-gray-light">Use of cookies</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq875-last just-mq480-last"><a href="javascript:;" class="optanon-toggle-display text-gray-light" data-track="click" data-track-action="manage cookies" data-track-label="link">Manage cookies</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq640-last just-mq480-last"><a href="https://www.nature.com/info/legal_notice.html" data-track="click" data-track-action="legal notice" data-track-label="link" class="text-gray-light">Legal notice</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq480-last"><a href="https://www.nature.com/info/accessibility_statement.html" data-track="click" data-track-action="accessibility statement" data-track-label="link" class="text-gray-light">Accessibility statement</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 last"><a href="https://www.nature.com/info/tandc.html" data-track="click" data-track-action="terms and conditions" data-track-label="link" class="text-gray-light">Terms &amp; Conditions</a></li>
            </ul>
        </div>
    </div>


    </div>
</footer>



    






    <script>
        var readingCompanionData = {
  "sections" : [ {
    "id" : "abstract",
    "title" : "Abstract"
  }, {
    "id" : "main",
    "title" : "Main"
  }, {
    "id" : "supervised-learning-of-policy-networks",
    "title" : "Supervised learning of policy networks"
  }, {
    "id" : "reinforcement-learning-of-policy-networks",
    "title" : "Reinforcement learning of policy networks"
  }, {
    "id" : "reinforcement-learning-of-value-networks",
    "title" : "Reinforcement learning of value networks"
  }, {
    "id" : "searching-with-policy-and-value-networks",
    "title" : "Searching with policy and value networks"
  }, {
    "id" : "evaluating-the-playing-strength-of-alphago",
    "title" : "Evaluating the playing strength of AlphaGo"
  }, {
    "id" : "discussion",
    "title" : "Discussion"
  }, {
    "id" : "methods",
    "title" : "Methods"
  }, {
    "id" : "references",
    "title" : "References"
  }, {
    "id" : "acknowledgements",
    "title" : "Acknowledgements"
  }, {
    "id" : "author-information",
    "title" : "Author information"
  }, {
    "id" : "extended-data",
    "title" : "Extended data"
  }, {
    "id" : "supplementary-information",
    "title" : "Supplementary information"
  }, {
    "id" : "rightslink",
    "title" : "Rights and permissions"
  }, {
    "id" : "article-info",
    "title" : "About this article"
  }, {
    "id" : "article-comments",
    "title" : "Comments",
    "extraClass" : "hide"
  } ],
  "figures" : [ {
    "num" : "1",
    "index" : "1",
    "imagePaths" : [ "//media.springernature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f1.jpg" ],
    "title" : "Figure 1: Neural network training pipeline and architecture.",
    "link" : "/articles/nature16961/figures/1",
    "anchor" : "#f1",
    "extendedData" : false
  }, {
    "num" : "2",
    "index" : "2",
    "imagePaths" : [ "//media.springernature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f2.jpg" ],
    "title" : "Figure 2: Strength and accuracy of policy and value networks.",
    "link" : "/articles/nature16961/figures/2",
    "anchor" : "#f2",
    "extendedData" : false
  }, {
    "num" : "3",
    "index" : "3",
    "imagePaths" : [ "//media.springernature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f3.jpg" ],
    "title" : "Figure 3: Monte Carlo tree search in AlphaGo.",
    "link" : "/articles/nature16961/figures/3",
    "anchor" : "#f3",
    "extendedData" : false
  }, {
    "num" : "4",
    "index" : "4",
    "imagePaths" : [ "//media.springernature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f4.jpg" ],
    "title" : "Figure 4: Tournament evaluation of AlphaGo.",
    "link" : "/articles/nature16961/figures/4",
    "anchor" : "#f4",
    "extendedData" : false
  }, {
    "num" : "5",
    "index" : "5",
    "imagePaths" : [ "//media.springernature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f5.jpg" ],
    "title" : "Figure 5: How AlphaGo (black, to play) selected its move in an informal game against Fan Hui.",
    "link" : "/articles/nature16961/figures/5",
    "anchor" : "#f5",
    "extendedData" : false
  }, {
    "num" : "6",
    "index" : "6",
    "imagePaths" : [ "//media.springernature.com/m685/nature-assets/nature/journal/v529/n7587/images/nature16961-f6.jpg" ],
    "title" : "Figure 6: Games from the match between AlphaGo and the European champion, Fan Hui.",
    "link" : "/articles/nature16961/figures/6",
    "anchor" : "#f6",
    "extendedData" : false
  } ],
  "references" : [ {
    "id" : "1",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Allis, L. V.</span></span> <i>Searching for Solutions in Games and Artificial Intelligence.</i> PhD thesis, Univ. Limburg, Maastricht, The Netherlands (<span>1994</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "2",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">van den Herik, H.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Uiterwijk, J. W.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">van Rijswijck, J.</span></span> <cite>Games solved: now and in the future</cite>. <i>Artif. Intell.</i> <b>134</b>, <span>277</span>–<span>311</span> (<span>2002</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "3",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Schaeffer, J.</span></span> <cite>The games computers (and people) play</cite>. <i>Advances in Computers</i> <b>52</b>, <span>189</span>–<span>266</span> (<span>2000</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "4",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Campbell, M.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Hoane, A.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Hsu, F.</span></span> <cite>Deep Blue</cite>. <i>Artif. Intell.</i> <b>134</b>, <span>57</span>–<span>83</span> (<span>2002</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "5",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Schaeffer, J.</span></span> <i>et al.</i> <cite>A world championship caliber checkers program</cite>. <i>Artif. Intell.</i> <b>53</b>, <span>273</span>–<span>289</span> (<span>1992</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "6",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Buro, M.</span></span> <cite>From simple features to sophisticated evaluation functions</cite>. In <i>1st International Conference on Computers and Games</i>, <span>126</span>–<span>145</span> (<span>1999</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "7",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Müller, M.</span></span> <cite>Computer Go</cite>. <i>Artif. Intell.</i> <b>134</b>, <span>145</span>–<span>179</span> (<span>2002</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "8",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Tesauro, G.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Galperin, G.</span></span> <cite>On-line policy improvement using Monte-Carlo search</cite>. In <i>Advances in Neural Information Processing</i>, <span>1068</span>–<span>1074</span> (<span>1996</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "9",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sheppard, B.</span></span> <cite>World-championship-caliber Scrabble</cite>. <i>Artif. Intell.</i> <b>134</b>, <span>241</span>–<span>275</span> (<span>2002</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "10",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Bouzy, B.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Helmstetter, B.</span></span> <cite>Monte-Carlo Go developments</cite>. In <i>10th International Conference on Advances in Computer Games</i>, <span>159</span>–<span>174</span> (<span>2003</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "11",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Coulom, R.</span></span> <cite>Efficient selectivity and backup operators in Monte-Carlo tree search</cite>. In <i>5th International Conference on Computers and Games</i>, <span>72</span>–<span>83</span> (<span>2006</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "12",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Kocsis, L.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Szepesvári, C.</span></span> <cite>Bandit based Monte-Carlo planning</cite>. In <i>15th European Conference on Machine Learning</i>, <span>282</span>–<span>293</span> (<span>2006</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "13",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Coulom, R.</span></span> <cite>Computing Elo ratings of move patterns in the game of Go</cite>. <i>ICGA J.</i> <b>30</b>, <span>198</span>–<span>208</span> (<span>2007</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "14",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Baudiš, P.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Gailly, J.-L.</span></span> <cite>Pachi: State of the art open source Go program</cite>. In <i>Advances in Computer Games</i>, <span>24</span>–<span>38</span> (Springer, <span>2012</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "15",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Müller, M.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Enzenberger, M.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Arneson, B.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Segal, R.</span></span> <cite>Fuego – an open-source framework for board games and Go engine based on Monte-Carlo tree search</cite>. <i>IEEE Trans. Comput. Intell. AI in Games</i> <b>2</b>, <span>259</span>–<span>270</span> (<span>2010</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "16",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Gelly, S.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Silver, D.</span></span> <cite>Combining online and offline learning in UCT</cite>. In <i>17th International Conference on Machine Learning</i>, <span>273</span>–<span>280</span> (<span>2007</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "17",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Krizhevsky, A.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutskever, I.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Hinton, G.</span></span> <cite>ImageNet classification with deep convolutional neural networks</cite>. In <i>Advances in Neural Information Processing Systems</i>, <span>1097</span>–<span>1105</span> (<span>2012</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "18",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Lawrence, S.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Giles, C. L.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Tsoi, A. C.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Back, A. D.</span></span> <cite>Face recognition: a convolutional neural-network approach</cite>. <i>IEEE Trans. Neural Netw.</i> <b>8</b>, <span>98</span>–<span>113</span> (<span>1997</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "19",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Mnih, V.</span></span> <i>et al.</i> <cite>Human-level control through deep reinforcement learning</cite>. <i>Nature</i> <b>518</b>, <span>529</span>–<span>533</span> (<span>2015</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "20",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">LeCun, Y.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Bengio, Y.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Hinton, G.</span></span> <cite>Deep learning</cite>. <i>Nature</i> <b>521</b>, <span>436</span>–<span>444</span> (<span>2015</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "21",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Stern, D.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Herbrich, R.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Graepel, T.</span></span> <cite>Bayesian pattern ranking for move prediction in the game of Go</cite>. In <i>International Conference of Machine Learning</i>, <span>873</span>–<span>880</span> (<span>2006</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "22",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutskever, I.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Nair, V.</span></span> <cite>Mimicking Go experts with convolutional neural networks</cite>. In <i>International Conference on Artificial Neural Networks</i>, <span>101</span>–<span>110</span> (<span>2008</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "23",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Maddison, C. J.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Huang, A.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutskever, I.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Silver, D.</span></span> <cite>Move evaluation in Go using deep convolutional neural networks</cite>. <i>3rd International Conference on Learning Representations</i> (<span>2015</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "24",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Clark, C.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Storkey, A. J.</span></span> <cite>Training deep convolutional neural networks to play go</cite>. In <i>32nd International Conference on Machine Learning</i>, <span>1766</span>–<span>1774</span> (<span>2015</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "25",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Williams, R. J.</span></span> <cite>Simple statistical gradient-following algorithms for connectionist reinforcement learning</cite>. <i>Mach. Learn.</i> <b>8</b>, <span>229</span>–<span>256</span> (<span>1992</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "26",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutton, R.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">McAllester, D.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Singh, S.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Mansour, Y.</span></span> <cite>Policy gradient methods for reinforcement learning with function approximation</cite>. In <i>Advances in Neural Information Processing Systems</i>, <span>1057</span>–<span>1063</span> (<span>2000</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "27",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutton, R.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Barto, A.</span></span> <i>Reinforcement Learning: an Introduction</i> (MIT Press, <span>1998</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "28",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Schraudolph, N. N.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Dayan, P.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sejnowski, T. J.</span></span> <cite>Temporal difference learning of position evaluation in the game of Go</cite>. <i>Adv. Neural Inf. Process. Syst.</i> <b>6</b>, <span>817</span>–<span>824</span> (<span>1994</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "29",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Enzenberger, M.</span></span> <cite>Evaluation in Go by a neural network using soft segmentation</cite>. In <i>10th Advances in Computer Games Conference</i>, <span>97</span>–<span>108</span> (<span>2003</span>). 267</span>",
    "schemaType" : "Article"
  }, {
    "id" : "30",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Silver, D.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutton, R.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Müller, M.</span></span> <cite>Temporal-difference search in computer Go</cite>. <i>Mach. Learn.</i> <b>87</b>, <span>183</span>–<span>219</span> (<span>2012</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "31",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Levinovitz, A.</span></span> <cite>The mystery of Go, the ancient game that computers still can’t win</cite>. <i>Wired Magazine</i> (<span>2014</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "32",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Mechner, D.</span></span> <cite>All Systems Go</cite>. <i>The Sciences</i> <b>38</b>, <span>32</span>–<span>37</span> (<span>1998</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "33",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Mandziuk, J.</span></span> <cite>Computational intelligence in mind games</cite>. In <i>Challenges for Computational Intelligence</i>, <span>407</span>–<span>442</span> (<span>2007</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "34",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Berliner, H.</span></span> <cite>A chronology of computer chess and its literature</cite>. <i>Artif. Intell.</i> <b>10</b>, <span>201</span>–<span>214</span> (<span>1978</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "35",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Browne, C.</span></span> <i>et al.</i> <cite>A survey of Monte-Carlo tree search methods</cite>. <i>IEEE Trans. Comput. Intell. AI in Games</i> <b>4</b>, <span>1</span>–<span>43</span> (<span>2012</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "36",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Gelly, S.</span></span> <i>et al.</i> <cite>The grand challenge of computer Go: Monte Carlo tree search and extensions</cite>. <i>Commun. ACM</i> <b>55</b>, <span>106</span>–<span>113</span> (<span>2012</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "37",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Coulom, R.</span></span> <cite>Whole-history rating: A Bayesian rating system for players of time-varying strength</cite>. In <i>International Conference on Computers and Games</i>, <span>113</span>–<span>124</span> (<span>2008</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "38",
    "title" : "<span>KGS. <cite>Rating system math</cite>. <a href=\"http://www.gokgs.com/help/rmath.html\">http://www.gokgs.com/help/rmath.html</a></span>",
    "schemaType" : "Article"
  }, {
    "id" : "39",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Littman, M. L.</span></span> <cite>Markov games as a framework for multi-agent reinforcement learning</cite>. In <i>11th International Conference on Machine Learning</i>, <span>157</span>–<span>163</span> (<span>1994</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "40",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Knuth, D. E.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Moore, R. W.</span></span> <cite>An analysis of alpha-beta pruning</cite>. <i>Artif. Intell.</i> <b>6</b>, <span>293</span>–<span>326</span> (<span>1975</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "41",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sutton, R.</span></span> <cite>Learning to predict by the method of temporal differences</cite>. <i>Mach. Learn.</i> <b>3</b>, <span>9</span>–<span>44</span> (<span>1988</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "42",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Baxter, J.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Tridgell, A.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Weaver, L.</span></span> <cite>Learning to play chess using temporal differences</cite>. <i>Mach. Learn.</i> <b>40</b>, <span>243</span>–<span>263</span> (<span>2000</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "43",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Veness, J.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Silver, D.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Blair, A.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Uther, W.</span></span> <cite>Bootstrapping from game tree search</cite>. In <i>Advances in Neural Information Processing Systems</i> (<span>2009</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "44",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Samuel, A. L.</span></span> <cite>Some studies in machine learning using the game of checkers II - recent progress</cite>. <i>IBM J. Res. Develop.</i> <b>11</b>, <span>601</span>–<span>617</span> (<span>1967</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "45",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Schaeffer, J.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Hlynka, M.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Jussila, V.</span></span> <cite>Temporal difference learning applied to a high-performance game-playing program</cite>. In <i>17th International Joint Conference on Artificial Intelligence</i>, <span>529</span>–<span>534</span> (<span>2001</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "46",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Tesauro, G.</span></span> <cite>TD-gammon, a self-teaching backgammon program, achieves master-level play</cite>. <i>Neural Comput.</i> <b>6</b>, <span>215</span>–<span>219</span> (<span>1994</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "47",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Dahl, F.</span></span> <cite>Honte, a Go-playing program using neural nets</cite>. In <i>Machines that learn to play games</i>, <span>205</span>–<span>223</span> (Nova Science, <span>1999</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "48",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Rosin, C. D.</span></span> <cite>Multi-armed bandits with episode context</cite>. <i>Ann. Math. Artif. Intell.</i> <b>61</b>, <span>203</span>–<span>230</span> (<span>2011</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "49",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Lanctot, M.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Winands, M. H. M.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Pepels, T.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Sturtevant, N. R.</span></span> <cite>Monte Carlo tree search with heuristic evaluations using implicit minimax backups</cite>. In <i>IEEE Conference on Computational Intelligence and Games</i>, <span>1</span>–<span>8</span> (<span>2014</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "50",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Gelly, S.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Wang, Y.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Munos, R.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Teytaud, O.</span></span> <cite>Modification of UCT with patterns in Monte-Carlo Go</cite>. <i>Tech. Rep.</i> <b>6062</b>, <span>INRIA</span> (<span>2006</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "51",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Silver, D.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Tesauro, G.</span></span> <cite>Monte-Carlo simulation balancing</cite>. In <i>26th International Conference on Machine Learning</i>, <b>119</b> (<span>2009</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "52",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Huang, S.-C.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Coulom, R.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Lin, S.-S.</span></span> <cite>Monte-Carlo simulation balancing in practice</cite>. In <i>7th International Conference on Computers and Games</i>, <span>81</span>–<span>92</span> (Springer-Verlag, <span>2011</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "53",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Baier, H.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Drake, P. D.</span></span> <cite>The power of forgetting: improving the last-good-reply policy in Monte Carlo Go</cite>. <i>IEEE Trans. Comput. Intell. AI in Games</i> <b>2</b>, <span>303</span>–<span>309</span> (<span>2010</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "54",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Huang, S.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Müller, M.</span></span> <cite>Investigating the limits of Monte-Carlo tree search methods in computer Go</cite>. In <i>8th International Conference on Computers and Games</i>, <span>39</span>–<span>48</span> (<span>2013</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "55",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Segal, R. B.</span></span> <cite>On the scalability of parallel UCT</cite>. <i>Computers and Games</i> <b>6515</b>, <span>36</span>–<span>47</span> (<span>2011</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "56",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Enzenberger, M.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Müller, M.</span></span> <cite>A lock-free multithreaded Monte-Carlo tree search algorithm</cite>. In <i>12th Advances in Computer Games Conference</i>, <span>14</span>–<span>20</span> (<span>2009</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "57",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Huang, S.-C.</span></span>, <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Coulom, R.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Lin, S.-S.</span></span> <cite>Time management for Monte-Carlo tree search applied to the game of Go</cite>. In <i>International Conference on Technologies and Applications of Artificial Intelligence</i>, <span>462</span>–<span>466</span> (<span>2010</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "58",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Gelly, S.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Silver, D.</span></span> <cite>Monte-Carlo tree search and rapid action value estimation in computer Go</cite>. <i>Artif. Intell.</i> <b>175</b>, <span>1856</span>–<span>1875</span> (<span>2011</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "59",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Baudiš, P.</span></span> <cite>Balancing MCTS by dynamically adjusting the komi value</cite>. <i>ICGA J.</i> <b>34</b>, <span>131</span> (<span>2011</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "60",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Baier, H.</span></span> &amp; <span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Winands, M. H.</span></span> <cite>Active opening book application for Monte-Carlo tree search in 19×19 Go</cite>. In <i>Benelux Conference on Artificial Intelligence</i>, <span>3</span>–<span>10</span> (<span>2011</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "61",
    "title" : "<span><span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Dean, J.</span></span> <i>et al.</i> <cite>Large scale distributed deep networks</cite>. In <i>Advances in Neural Information Processing Systems</i>, <span>1223</span>–<span>1231</span> (<span>2012</span>)</span>",
    "schemaType" : "Article"
  }, {
    "id" : "62",
    "title" : "<span>Go ratings. <a href=\"http://www.goratings.org\">http://www.goratings.org</a></span>",
    "schemaType" : "CreativeWork"
  } ],
  "hasAccess" : true,
  "hasNumericReferences" : true,
  "refSuffix" : null,
  "video" : false
};
    </script>



    
        
            <script data-test="mosaic-js" src="/static/js/mosaic.a0c55d933e.js" id="mosaic"></script>
        
            <script>
        var backHalf = true;
        var idp = {
            institutionalLogin: function (names) {
                if (names.length && true) {
                    if (document.querySelector('.js-main-column')) {
                        var accessMessage = document.createElement('div');
                            if (backHalf) {
                                var prependAccessMessageTo = 'div.article-body div.distractionFree'
                                accessMessage.innerHTML = '<p style="transform: translateY(-5px)" class="block mb0 pb4 pt6 text-gray-light text13 text-center">Access provided by ' + names[0] + '</p>';
                                document.querySelector(prependAccessMessageTo).classList.remove("visually-hidden");
                                document.querySelector(prependAccessMessageTo).prepend(accessMessage);
                            } else {
                            accessMessage.innerHTML = '<p style="transform: translateY(-5px)" class="text14 block mb0 pb10 text-center">Access provided by ' + names[0] + '</p>';
                            document.querySelector('.js-main-column').prepend(accessMessage);
                        }
                    }
                }
            },


            hasNatureUserProof: function (hasProof) {
                if (!hasProof) {
                    document.getElementById("my-account").setAttribute("style", "display: none;");
                    document.getElementById("login-button").setAttribute("style", "");
                }
            }
        }
    </script>
    <script src="https://verify.nature.com/verify/nature.min.js"></script>
    




<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" border="0" width="0" height="0" style="display: none">
</noscript>



<script src="//content.readcube.com/ping?doi=10.1038/nature16961&amp;format=js&amp;last_modified=2018-07-12" async></script>
<img src="/platform/track/article/nature16961" width="1" height="1" alt="" class="visually-hidden"/>
</body>
</html>
